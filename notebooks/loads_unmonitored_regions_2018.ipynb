{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import nope\n",
    "import nivapy3 as nivapy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "sn.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating loads in unmonitored regions - 2018\n",
    "\n",
    "The new model can be used to estimate loads in unmonitored areas. We know the regine ID for each of the 155 stations where water chemistry is measured, and we also know which OSPAR region each monitoring site drains to. We want to use observed data to estimate loads upstream of each monitoring point, and modelled data elsewhere. This can be achieved using the output from the new model.\n",
    "\n",
    "This notebook is based on the one [here](http://nbviewer.jupyter.org/github/JamesSample/rid/blob/master/notebooks/loads_unmonitored_regions.ipynb). It first runs the NOPE model for 2018 and then extracts data for unmonitored regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to db\n",
    "engine = nivapy.da.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate model input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year of interest\n",
    "year = 2018\n",
    "\n",
    "# Parameters of interest\n",
    "par_list = ['Tot-N', 'Tot-P']\n",
    "\n",
    "# Folder containing NOPE data\n",
    "nope_fold = r'../../../NOPE/NOPE_Core_Input_Data'\n",
    "\n",
    "# Ouput path for model file\n",
    "out_csv = r'../../../NOPE/NOPE_Annual_Inputs/nope_input_data_%s.csv' % year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make input file\n",
    "df = nope.make_rid_input_file(year, engine, nope_fold, out_csv,\n",
    "                              par_list=par_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Input file\n",
    "in_csv = r'../../../NOPE/NOPE_Annual_Inputs/nope_input_data_%s.csv' % year\n",
    "\n",
    "# Run model\n",
    "g = nope.run_nope(in_csv, par_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results as csv\n",
    "out_csv = r'../../../NOPE/nope_results_%s_test.csv' % year\n",
    "df = nope.model_to_dataframe(g, out_path=out_csv)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save version with main catchments only\n",
    "main_list = [\"%03d.\" % i for i in range(1, 316)]\n",
    "df2 = df.query('regine in @main_list')\n",
    "df2.sort_values('regine', inplace=True)\n",
    "\n",
    "# Save\n",
    "out_csv = r'../../../NOPE/nope_results_%s_main_catchs.csv' % year\n",
    "df2.to_csv(out_csv, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore results\n",
    "\n",
    "### 4.1. Total N and P\n",
    "\n",
    "####  4.1.1. Identify areas with monitoring data\n",
    "\n",
    "Where observations are available, we want to use them in preference to the model output. This means identifying all the catchments with observed data and substracting the model results for these locations. This is more complicated than it appears, because a small number of observed catchments are upstream of others, so subtracting all the loads for the 155 monitored catchments involves \"double accounting\", which we want to avoid. The first step is therefore to identify the downstream-most nodes for the monitored areas i.e. for the cases where one catchment is upstream of another, we just want the downstream node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read station data\n",
    "in_xlsx = r'../../../Data/RID_Sites_List.xlsx'\n",
    "stn_df = pd.read_excel(in_xlsx, sheet_name='RID_All')\n",
    "\n",
    "# Get just cols of interest and drop duplicates \n",
    "# (some sites are in the same regine)\n",
    "stn_df = stn_df[['ospar_region', 'nve_vassdrag_nr']].drop_duplicates()\n",
    "\n",
    "# Get catch IDs with calib data\n",
    "calib_nds = set(stn_df['nve_vassdrag_nr'].values)\n",
    "\n",
    "# Build network\n",
    "in_path = r'../../../NOPE/NOPE_Annual_Inputs/nope_input_data_1990.csv'\n",
    "g, nd_list = nope.build_calib_network(in_path, calib_nds)\n",
    "\n",
    "# Get list of downstream nodes\n",
    "ds_nds = []\n",
    "for nd in g:\n",
    "    # If no downstream nodes\n",
    "    if g.out_degree(nd) == 0:\n",
    "        # Node is of interest\n",
    "        ds_nds.append(nd)\n",
    "\n",
    "# Get just the downstream catchments\n",
    "stn_df = stn_df[stn_df['nve_vassdrag_nr'].isin(ds_nds)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2. Sum model results for monitored locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read model output\n",
    "in_csv = r'../../../NOPE/nope_results_%s.csv' % year\n",
    "nope_df = pd.read_csv(in_csv)\n",
    "\n",
    "# Join accumulated outputs to stns of interest\n",
    "mon_df = pd.merge(stn_df, nope_df, how='left',\n",
    "                  left_on='nve_vassdrag_nr',\n",
    "                  right_on='regine')\n",
    "\n",
    "# Groupby OSPAR region\n",
    "mon_df = mon_df.groupby('ospar_region').sum()\n",
    "\n",
    "# Get just accum cols\n",
    "cols = [i for i in mon_df.columns if i.split('_')[0]=='accum']\n",
    "mon_df = mon_df[cols]\n",
    "\n",
    "mon_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table gives the **modelled** inputs to each OSPAR region from catchments for which we have observed data. We want to subtract these values from the overall modelled inputs to each region and substitute the observed data instead.\n",
    "\n",
    "The trickiest part of this is that the OSPAR regions in the TEOTIL catchment network (and therefore the network for my new model too) don't exactly match the new OSPAR definitions. The OSPAR boundaries were updated relatively recently, so instead of simply selecting the desired OSPAR region in the model output, I need to aggregate based on vassdragsnummers.\n",
    "\n",
    "**Note:** Eventually, it would be a good idea to update the network information in `regine.csv` to reflect the current OSPAR regions.\n",
    "\n",
    "#### 4.1.3. Group model output according to \"new\" OSPAR regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define \"new\" OSPAR regions\n",
    "os_dict = {'SKAGERAK':(1, 23),\n",
    "           'NORTH SEA':(24, 90),\n",
    "           'NORWEGIAN SEA2':(91, 170),\n",
    "           'LOFOTEN-BARENTS SEA':(171, 247)}\n",
    "\n",
    "# Container for results\n",
    "df_list = []\n",
    "\n",
    "# Loop over model output\n",
    "for reg in os_dict.keys():\n",
    "    min_id, max_id = os_dict[reg]\n",
    "    \n",
    "    regs = ['%03d.' % i for i in range(min_id, max_id+1)]\n",
    "    \n",
    "    # Get data for this region\n",
    "    df2 = nope_df[nope_df['regine'].isin(regs)]\n",
    "    \n",
    "    # Get just accum cols\n",
    "    cols = [i for i in df2.columns if i.split('_')[0]=='accum']\n",
    "    df2 = df2[cols]\n",
    "    \n",
    "    # Add region\n",
    "    df2['ospar_region'] = reg\n",
    "    \n",
    "    # Add sum to output\n",
    "    df_list.append(df2)\n",
    "\n",
    "# Build df\n",
    "os_df = pd.concat(df_list, axis=0)\n",
    "\n",
    "# Aggregate\n",
    "os_df = os_df.groupby('ospar_region').sum()\n",
    "\n",
    "os_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate the unmonitored component by simply subtracting the values modelled upstream of monitoring stations from the overall modelled inputs to each OSPAR region.\n",
    "\n",
    "#### 4.1.4. Estimate loads in unmonitored areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc unmonitored loads\n",
    "unmon_df = os_df - mon_df\n",
    "\n",
    "# Write output\n",
    "out_csv = r'../../../NOPE/unmonitored_loads_%s.csv' % year\n",
    "unmon_df.to_csv(out_csv, encoding='utf-8', index_label='ospar_region')\n",
    "\n",
    "unmon_df.round(0).astype(int).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.5. Aggregate values to required quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate to match report\n",
    "unmon_df['flow'] = unmon_df['accum_q_m3/s']*60*60*24/1000. # 1000s m3/day\n",
    "\n",
    "unmon_df['sew_n'] = unmon_df['accum_ren_tot-n_tonnes'] + unmon_df['accum_spr_tot-n_tonnes']\n",
    "unmon_df['sew_p'] = unmon_df['accum_ren_tot-p_tonnes'] + unmon_df['accum_spr_tot-p_tonnes']\n",
    "\n",
    "unmon_df['ind_n'] = unmon_df['accum_ind_tot-n_tonnes']\n",
    "unmon_df['ind_p'] = unmon_df['accum_ind_tot-p_tonnes']\n",
    "\n",
    "unmon_df['fish_n'] = unmon_df['accum_aqu_tot-n_tonnes']\n",
    "unmon_df['fish_p'] = unmon_df['accum_aqu_tot-p_tonnes']\n",
    "\n",
    "unmon_df['diff_n'] = unmon_df['accum_anth_diff_tot-n_tonnes'] + unmon_df['accum_nat_diff_tot-n_tonnes']\n",
    "unmon_df['diff_p'] = unmon_df['accum_anth_diff_tot-p_tonnes'] + unmon_df['accum_nat_diff_tot-p_tonnes']\n",
    "\n",
    "new_df = unmon_df[['flow', 'sew_n', 'sew_p', \n",
    "                   'ind_n', 'ind_p', 'fish_n', \n",
    "                   'fish_p', 'diff_n', 'diff_p']]\n",
    "\n",
    "# Total for Norway\n",
    "new_df.loc['NORWAY'] = new_df.sum(axis=0)\n",
    "\n",
    "# Reorder rows\n",
    "new_df = new_df.reindex(['NORWAY', 'LOFOTEN-BARENTS SEA', 'NORTH SEA', \n",
    "                         'NORWEGIAN SEA2', 'SKAGERAK'])\n",
    "\n",
    "new_df.round().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Other N and P species\n",
    "\n",
    "Tore's procedure `RESA2.FIXTEOTILPN` defines simple correction factors for estimating PO4, NO3 and NH4 from total P and N. The table below lists the factors used.\n",
    "\n",
    "|   Source    | Phosphate | Nitrate | Ammonium |\n",
    "|:-----------:|:---------:|:-------:|:--------:|\n",
    "|    Sewage   |     0.600 |   0.050 |    0.750 |\n",
    "|   Industry  |     0.600 |   0.050 |    0.750 |\n",
    "| Aquaculture |     0.690 |   0.110 |    0.800 |\n",
    "|   Diffuse   |     0.246 |   0.625 |    0.055 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict of conversion factors\n",
    "con_dict = {('sew', 'po4'):('p', 0.6),\n",
    "            ('ind', 'po4'):('p', 0.6),\n",
    "            ('fish', 'po4'):('p', 0.69),\n",
    "            ('diff', 'po4'):('p', 0.246),\n",
    "            ('sew', 'no3'):('n', 0.05),\n",
    "            ('ind', 'no3'):('n', 0.05),\n",
    "            ('fish', 'no3'):('n', 0.11),\n",
    "            ('diff', 'no3'):('n', 0.625),\n",
    "            ('sew', 'nh4'):('n', 0.75),\n",
    "            ('ind', 'nh4'):('n', 0.75),\n",
    "            ('fish', 'nh4'):('n', 0.8),\n",
    "            ('diff', 'nh4'):('n', 0.055)}\n",
    "\n",
    "# Apply factors\n",
    "for src in ['sew', 'ind', 'fish', 'diff']:\n",
    "    for spc in ['po4', 'no3', 'nh4']:\n",
    "        el, fac = con_dict[(src, spc)]\n",
    "        new_df[src+'_'+spc] = fac * new_df[src+'_'+el]\n",
    "        \n",
    "new_df.round().astype(int).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Other quantities\n",
    "\n",
    "The model currently only considers N and P, but the project focuses on a wider range of parameters. For now, we simply assume that all measured inputs (`renseanlegg`, `industri` and `akvakultur`) for regines outside of catchments with measured data make it to the sea.\n",
    "\n",
    "We only want data for catchments that are not monitored i.e. for regine IDs **not** in the graph created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sql below uses a horrible (and slow!) hack to get around Oracle's\n",
    "# 1000 item limit on IN clauses. See here for details:\n",
    "# https://stackoverflow.com/a/9084247/505698\n",
    "nd_list_hack = [(1, i) for i in nd_list]\n",
    "\n",
    "sql = (\"SELECT SUBSTR(a.regine, 1, 3) AS vassdrag, \"\n",
    "       \"  a.type, \"\n",
    "       \"  b.name, \"\n",
    "       \"  b.unit, \"\n",
    "       \"  SUM(c.value * d.factor) as value \"\n",
    "       \"FROM RESA2.RID_PUNKTKILDER a, \"\n",
    "       \"RESA2.RID_PUNKTKILDER_OUTPAR_DEF b, \"\n",
    "       \"RESA2.RID_PUNKTKILDER_INPAR_VALUES c, \"\n",
    "       \"RESA2.RID_PUNKTKILDER_INP_OUTP d \"\n",
    "       \"WHERE a.anlegg_nr = c.anlegg_nr \"\n",
    "       \"AND (1, a.regine) NOT IN %s \"\n",
    "       \"AND d.in_pid = c.inp_par_id \"\n",
    "       \"AND d.out_pid = b.out_pid \"\n",
    "       \"AND c.year = %s \"\n",
    "       \"GROUP BY SUBSTR(a.regine, 1, 3), a.type, b.name, b.unit \"\n",
    "       \"ORDER BY SUBSTR(a.regine, 1, 3), a.type\" % (tuple(nd_list_hack), year))\n",
    "\n",
    "df = pd.read_sql(sql, engine)\n",
    "\n",
    "# Tidy\n",
    "df['par'] = df['type'] + '_' + df['name'] + '_' + df['unit']\n",
    "del df['name'], df['unit'], df['type']\n",
    "\n",
    "# Pivot\n",
    "df = df.pivot(index='vassdrag', columns='par', values='value')\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    try:\n",
    "        a = int(x)\n",
    "        return a\n",
    "    except:\n",
    "        return -999\n",
    "\n",
    "# Convert vassdrag to numbers\n",
    "df['vass'] = df['vassdrag'].apply(f)\n",
    "\n",
    "# Get just the main catchments\n",
    "df = df.query('vass != -999')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(x):   \n",
    "    if x in range(1, 24):\n",
    "        return 'SKAGERAK'\n",
    "    elif x in range(24, 91):\n",
    "        return 'NORTH SEA'\n",
    "    elif x in range(91, 171):\n",
    "        return 'NORWEGIAN SEA2'\n",
    "    elif x in range(171, 248):\n",
    "        return 'LOFOTEN-BARENTS SEA'\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Assign main catchments to OSPAR regions\n",
    "df['osp_reg'] = df['vass'].apply(f2)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by OSPAR region\n",
    "df.fillna(0, inplace=True)\n",
    "df = df.groupby('osp_reg').sum()\n",
    "df.drop(0, inplace=True)\n",
    "\n",
    "# Total for Norway\n",
    "df.loc['NORWAY'] = df.sum(axis=0)\n",
    "\n",
    "# Join to model results \n",
    "df = new_df.join(df)\n",
    "\n",
    "# Get cols of interest\n",
    "umod_cols = ['S.P.M.', 'TOC', 'As', 'Pb', 'Cd', 'Cu', 'Zn', 'Ni', 'Cr', 'Hg']\n",
    "umod_cols = ['%s_%s_tonn' % (i, j) for i in ['INDUSTRI', 'RENSEANLEGG'] for j in umod_cols]\n",
    "cols = list(new_df.columns) + umod_cols\n",
    "cols.remove('RENSEANLEGG_TOC_tonn')\n",
    "df = df[cols]\n",
    "\n",
    "df.round(0).astype(int).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fish farm copper\n",
    "\n",
    "Finally, we need to add in the Cu totals from fish farms. The method is similar to that used above, but simpler because we're only interested in one parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sql below uses a horrible (and slow!) hack to get around Oracle's\n",
    "# 1000 item limit on IN clauses. See here for details:\n",
    "# https://stackoverflow.com/a/9084247/505698\n",
    "nd_list_hack = [(1, i) for i in nd_list]\n",
    "\n",
    "sql = (\"SELECT SUBSTR(a.regine, 1, 3) AS vassdrag, \"\n",
    "       \"  SUM(b.value) as value \"\n",
    "       \"FROM RESA2.RID_KILDER_AQUAKULTUR a, \"\n",
    "       \"RESA2.RID_KILDER_AQKULT_VALUES b \"\n",
    "       \"WHERE a.nr = b.anlegg_nr \"\n",
    "       \"AND (1, a.regine) NOT IN %s \"\n",
    "       \"AND b.inp_par_id = 41 \"\n",
    "       \"AND b.ar = %s \"\n",
    "       \"GROUP BY SUBSTR(a.regine, 1, 3), b.inp_par_id \"\n",
    "       \"ORDER BY SUBSTR(a.regine, 1, 3), b.inp_par_id\" % (tuple(nd_list_hack), year))\n",
    "\n",
    "aq_df = pd.read_sql(sql, engine)\n",
    "\n",
    "# Get vassdrag\n",
    "aq_df['vass'] = aq_df['vassdrag'].apply(f)\n",
    "aq_df = aq_df.query('vass != -999')\n",
    "\n",
    "# Calc OSPAR region and group\n",
    "aq_df['osp_reg'] = aq_df['vass'].apply(f2)\n",
    "aq_df.fillna(0, inplace=True)\n",
    "aq_df = aq_df.groupby('osp_reg').sum()\n",
    "del aq_df['vass']\n",
    "\n",
    "# Total for Norway\n",
    "aq_df.loc['NORWAY'] = aq_df.sum(axis=0)\n",
    "\n",
    "# Rename\n",
    "aq_df.columns = ['AQUAKULTUR_Cu_tonn',]\n",
    "\n",
    "# Join model results \n",
    "df = df.join(aq_df)\n",
    "\n",
    "df.round(0).astype(int).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write output\n",
    "out_csv = r'../../../Results/Unmon_loads/unmon_loads_%s.csv' % year\n",
    "df.to_csv(out_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data can then be used to create Table 3 in the report - see [this notebook](https://nbviewer.jupyter.org/github/JamesSample/rid/blob/master/notebooks/summary_table_2017.ipynb) for details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
