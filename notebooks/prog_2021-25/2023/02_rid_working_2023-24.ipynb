{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nivapy3 as nivapy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import useful_rid_code as rid\n",
    "from sqlalchemy import text\n",
    "\n",
    "sn.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Connect to db\n",
    "engine = nivapy.da.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Year of interest\n",
    "year = 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RID 2023-24: data processing notebook\n",
    "\n",
    "## 1. Add 2023 datasets\n",
    "\n",
    "### 1.1. Update flow datasets\n",
    "\n",
    "The notebook `update_flow_nve_hydapi.ipynb` can be used to update flow datasets in RESA2. Note that not all datasets are necessarily available via HydAPI, so it is still sometimes necessary to request datasets directly from Trine at NVE.\n",
    "\n",
    "### 1.2. Water chemistry quality control\n",
    "\n",
    "I Liv Bente has quality-checked the data in RESA and the necessary corrections have been made (see e-mail from Liv Bente received 23-08-2024).\n",
    "\n",
    "### 1.3. Sample selections\n",
    "\n",
    "Previous analysis for the RID report only used water samples collected as part of the \"core\" monitoring programme (i.e. not flood samples or those collected under Option 3). For 2021-5, the option 3 samples have been moved to a separate project called Bk-stations, so not sure whether this is still relevant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read new site groupings (for 2017 to 2020)\n",
    "in_xlsx = r\"../../../data/RID_Sites_List_2017-2020.xlsx\"\n",
    "rid_20_df = pd.read_excel(in_xlsx, sheet_name=\"RID_20\")\n",
    "rid_135_df = pd.read_excel(in_xlsx, sheet_name=\"RID_135\")\n",
    "rid_155_df = pd.read_excel(in_xlsx, sheet_name=\"RID_All\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1. Option 3/Bk-stations\n",
    "\n",
    "In the programme for 2017-20, additional samples were collected under \"Option 3\". In the programme for 2021-25, this sampling takes place as an entirely separate project, called the \"Bk-stasjoner\" (RESA project ID 4591). Samples from these stations will be treated the same as the Option 3 samples previously (i.e. added to `sample_selection 65`).\n",
    "\n",
    "**For 2023, there is no overlap between the RID_155 and the sampled BK_stations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get Bk-stations\n",
    "bk_df = nivapy.da.select_resa_project_stations([4591], engine)\n",
    "print(len(bk_df), \"stations in the Bk-project.\")\n",
    "\n",
    "# Find any Bk-stations also in the \"main\" project\n",
    "bk_in_155 = set(rid_155_df[\"station_id\"]).intersection(set(bk_df[\"station_id\"]))\n",
    "print(\"The following Bk-stations are also part of the RID 155.\")\n",
    "bk_in_155 = rid_155_df.query(\"station_id in @bk_in_155\")\n",
    "bk_in_155"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Get data from Bk-stations in RID 155 for year of interest\n",
    "# bk_par_df = nivapy.da.select_resa_station_parameters(\n",
    "#     bk_in_155, f\"{year}-01-01\", f\"{year}-12-31\", engine\n",
    "# )\n",
    "# bk_wc_df, bk_dup_df = nivapy.da.select_resa_water_chemistry(\n",
    "#     bk_in_155, bk_par_df, f\"{year}-01-01\", f\"{year}-12-31\", engine\n",
    "# )\n",
    "# print(len(bk_wc_df), \"samples to be linked to 'Option 3'.\")\n",
    "# bk_wc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Add samples from Bk-stations also in RID 155 to \"option 3\"\n",
    "# ws_ids = []\n",
    "# for idx, row in bk_wc_df.iterrows():\n",
    "#     sql = (\n",
    "#         \"SELECT water_sample_id FROM resa2.water_samples \"\n",
    "#         \"WHERE station_id = %s \"\n",
    "#         \"AND TRUNC(sample_date) = DATE '%s' \"\n",
    "#         \"AND depth1 = %s \"\n",
    "#         \"AND depth2 = %s\"\n",
    "#         % (\n",
    "#             row[\"station_id\"],\n",
    "#             row[\"sample_date\"].strftime(\"%Y-%m-%d\"),\n",
    "#             row[\"depth1\"],\n",
    "#             row[\"depth2\"],\n",
    "#         )\n",
    "#     )\n",
    "#     ws_id = engine.execute(sql).fetchall()[0]\n",
    "#     assert len(ws_id) == 1\n",
    "#     ws_id = ws_id[0]\n",
    "#     ws_ids.append(ws_id)\n",
    "\n",
    "# ws_df = pd.DataFrame(\n",
    "#     {\n",
    "#         \"water_sample_id\": ws_ids,\n",
    "#     }\n",
    "# )\n",
    "# ws_df[\"sample_selection_id\"] = 65\n",
    "\n",
    "# assert len(bk_wc_df) == len(ws_df)\n",
    "\n",
    "# # ws_df.to_sql(\n",
    "# #     \"sample_selections\", con=engine, schema=\"resa2\", if_exists=\"append\", index=False\n",
    "# # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2. Flood samples\n",
    "\n",
    "Flood samples were taken during Storm Hans in August on Glomma and Drammenselva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get WS IDs for flood samples\n",
    "fl_xlsx = f\"../../../data/flood_samples_{year}.xlsx\"\n",
    "fl_df = pd.read_excel(fl_xlsx, sheet_name=f\"flood_samples_{year}\")\n",
    "\n",
    "ws_ids = []\n",
    "with engine.connect() as connection:\n",
    "    for idx, row in fl_df.iterrows():\n",
    "        sql = text(\n",
    "            \"SELECT water_sample_id FROM resa2.water_samples \"\n",
    "            \"WHERE station_id = :station_id \"\n",
    "            \"AND TO_CHAR(sample_date, 'YYYY-MM-DD HH24:MI') = :sample_date \"\n",
    "            \"AND depth1 = :depth1 \"\n",
    "            \"AND depth2 = :depth2\"\n",
    "        )\n",
    "        params = {\n",
    "            \"station_id\": row[\"station_id\"],\n",
    "            \"sample_date\": row[\"sample_date\"].strftime(\"%Y-%m-%d %H:%M\"),\n",
    "            \"depth1\": row[\"depth1\"],\n",
    "            \"depth2\": row[\"depth2\"],\n",
    "        }\n",
    "        result = connection.execute(sql, params).fetchall()\n",
    "        assert len(result) == 1\n",
    "        ws_id = result[0][0]\n",
    "        ws_ids.append(ws_id)\n",
    "\n",
    "ws_df = pd.DataFrame(\n",
    "    {\n",
    "        \"water_sample_id\": ws_ids,\n",
    "    }\n",
    ")\n",
    "ws_df[\"sample_selection_id\"] = 64\n",
    "\n",
    "assert len(fl_df) == len(ws_df)\n",
    "\n",
    "# ws_df.to_sql(\n",
    "#     \"sample_selections\", con=engine, schema=\"resa2\", if_exists=\"append\", index=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3. Main programme\n",
    "\n",
    "Everything else (i.e. not Option 3 or flood) is assumed to be part of the main programme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get flood and Option 3 samples\n",
    "sql = (\n",
    "    \"SELECT water_sample_id \"\n",
    "    \"FROM resa2.sample_selections \"\n",
    "    \"WHERE sample_selection_id IN (64, 65)\"\n",
    ")\n",
    "oth_ws = pd.read_sql_query(sql, engine)\n",
    "assert oth_ws[\"water_sample_id\"].is_unique\n",
    "\n",
    "# Get all WS associated with core sites\n",
    "sql = (\n",
    "    \"SELECT water_sample_id FROM resa2.water_samples \"\n",
    "    \"WHERE station_id IN %s \"\n",
    "    \"AND sample_date >= DATE '%s-01-01' \"\n",
    "    \"AND sample_date < DATE '%s-01-01'\"\n",
    "    % (str(tuple(rid_155_df[\"station_id\"].astype(int))), year, year + 1)\n",
    ")\n",
    "all_ws = pd.read_sql_query(sql, engine)\n",
    "assert all_ws[\"water_sample_id\"].is_unique\n",
    "\n",
    "# Remove flood and option 3 samples from core\n",
    "core_ws = set(all_ws[\"water_sample_id\"]) - set(oth_ws[\"water_sample_id\"])\n",
    "\n",
    "# Add to sample selections\n",
    "core_df = pd.DataFrame({\"water_sample_id\": list(core_ws)})\n",
    "core_df[\"sample_selection_id\"] = 63\n",
    "\n",
    "print(len(core_df), \"samples in the main programme.\")\n",
    "\n",
    "# core_df.to_sql(\n",
    "#     \"sample_selections\", con=engine, schema=\"resa2\", if_exists=\"append\", index=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tabulate raw water chemistry and flow\n",
    "\n",
    "### 2.1. Data for year of interest\n",
    "\n",
    "From 2017 onwards, water chemistry samples have been collected at 20 sites (`RID_20`). In 2018, the station TROEMÅL2 was added to the RID_20 selection, making 21 stations in total. Both TROEMÅL and TROEMÅL2 were monitored in 2018, but from 2019 onwards TROEMÅL2 replaced TROEMÅL in the main programme (although TROEMÅL is sometimes still included as part of Option 3).\n",
    "\n",
    "The data are exported to CSV format below.\n",
    "\n",
    "**Added 19.05.2022:** For 2021-5, I have added an extra kwarg named `extract_flow` to the function below. This is because flow data are not ready for the new reporting deadlines in June, so we need to be able to process the concentration part without flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Whether to get flow data. Set to False for spring processing and True for autumn processing\n",
    "extract_flow = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Output CSV\n",
    "out_csv = f\"/home/jovyan/shared/common/elveovervakingsprogrammet/results/measured_loads/concs_and_flows_rid_20_{year}.csv\"\n",
    "df = rid.write_csv_water_chem(\n",
    "    rid_20_df, year, out_csv, engine, samp_sel=63, extract_flow=extract_flow\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Data for all years\n",
    "\n",
    "**Added 03.09.2018**. From 2017, we include estimates of trends in the \"main\" rivers for both loads and concentrations for the period from 1990 to present.\n",
    "\n",
    "**Added 19.05.2022**. Set `extract_flow` to `False` for the spring processing and `True` for the autumn processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Container for data\n",
    "df_list = []\n",
    "\n",
    "# Dummy path for intermediate output (which isn't needed here)\n",
    "out_csv = r\"/home/jovyan/shared/common/elveovervakingsprogrammet/results/measured_loads/cons_and_flows_intermed.csv\"\n",
    "\n",
    "# Loop over years\n",
    "for data_yr in range(1990, year + 1):\n",
    "    # Get data\n",
    "    df = rid.write_csv_water_chem(\n",
    "        rid_20_df, data_yr, out_csv, engine, samp_sel=63, extract_flow=extract_flow\n",
    "    )\n",
    "\n",
    "    # Add to output\n",
    "    df_list.append(df)\n",
    "\n",
    "# Delete intermediate\n",
    "os.remove(out_csv)\n",
    "\n",
    "# Combine\n",
    "df = pd.concat(df_list, axis=0)\n",
    "\n",
    "# Reorder cols and tidy\n",
    "st_cols = [\n",
    "    \"station_id\",\n",
    "    \"station_code\",\n",
    "    \"station_name\",\n",
    "    \"old_rid_group\",\n",
    "    \"new_rid_group\",\n",
    "    \"ospar_region\",\n",
    "    \"sample_date\",\n",
    "]\n",
    "if extract_flow:\n",
    "    st_cols.append(\"Qs_m3/s\")\n",
    "\n",
    "par_cols = [i for i in df.columns if i not in st_cols]\n",
    "par_cols.sort()\n",
    "df = df[st_cols + par_cols]\n",
    "\n",
    "# Output CSV\n",
    "out_csv = f\"/home/jovyan/shared/common/elveovervakingsprogrammet/results/measured_loads/concs_and_flows_rid_20_1990-{year}.csv\"\n",
    "df.to_csv(out_csv, encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Estimate observed loads\n",
    "\n",
    "### 3.1. Annual flows\n",
    "\n",
    "First get a dataframe of annual flow volumes to join to the summary output. **NB:** This dataframe isn't actually used in the loads calculations - they are handled separately - it's just for the output CSVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sites of interest: combine all site dfs into one\n",
    "rid_all_df = pd.concat([rid_20_df, rid_135_df], axis=0)\n",
    "\n",
    "# Get flow data\n",
    "q_df = rid.get_flow_volumes(rid_all_df, 1990, year, engine)\n",
    "\n",
    "q_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Loads for all rivers\n",
    "\n",
    "The code below is taken from Section 2 of [notebook 3](http://nbviewer.jupyter.org/github/JamesSample/rid/blob/master/notebooks/estimate_loads.ipynb). Loads are calculated directly from contemporary observations for the RID_20, and they are inferred from historic concentrations for the RID_135 sites.\n",
    "\n",
    "As above, note the use of the `'samp_sel'` argument in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sites of interest: combine all site dfs into one\n",
    "rid_all_df = pd.concat([rid_20_df, rid_135_df], axis=0)\n",
    "\n",
    "# Pars of interest\n",
    "par_list = [\n",
    "    \"SPM\",\n",
    "    \"TOC\",\n",
    "    \"PO4-P\",\n",
    "    \"TOTP\",\n",
    "    \"NO3-N\",\n",
    "    \"NH4-N\",\n",
    "    \"TOTN\",\n",
    "    \"SiO2\",\n",
    "    \"Ag\",\n",
    "    \"As\",\n",
    "    \"Pb\",\n",
    "    \"Cd\",\n",
    "    \"Cu\",\n",
    "    \"Zn\",\n",
    "    \"Ni\",\n",
    "    \"Cr\",\n",
    "    \"Hg\",\n",
    "]\n",
    "\n",
    "# Container for results from each site\n",
    "loads_list = []\n",
    "\n",
    "# Loop over sites\n",
    "for stn_id in rid_all_df[\"station_id\"].values:\n",
    "    # Estimate loads at this site\n",
    "    loads_list.append(\n",
    "        rid.estimate_loads(\n",
    "            stn_id, par_list, year, engine, infer_missing=True, samp_sel=63\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Concatenate to new df\n",
    "lds_all = pd.concat(loads_list, axis=0)\n",
    "lds_all.index.name = \"station_id\"\n",
    "lds_all.reset_index(inplace=True)\n",
    "\n",
    "# Get flow data for year\n",
    "q_yr = q_df.query(\"year == @year\")\n",
    "\n",
    "# Join\n",
    "lds_all = pd.merge(lds_all, rid_all_df, how=\"left\", on=\"station_id\")\n",
    "lds_all = pd.merge(lds_all, q_yr, how=\"left\", on=\"station_id\")\n",
    "\n",
    "# Reorder cols and tidy\n",
    "st_cols = [\n",
    "    \"station_id\",\n",
    "    \"station_code\",\n",
    "    \"station_name\",\n",
    "    \"old_rid_group\",\n",
    "    \"new_rid_group\",\n",
    "    \"ospar_region\",\n",
    "    \"mean_q_1000m3/day\",\n",
    "]\n",
    "unwant_cols = [\n",
    "    \"nve_vassdrag_nr\",\n",
    "    \"lat\",\n",
    "    \"lon\",\n",
    "    \"utm_north\",\n",
    "    \"utm_east\",\n",
    "    \"utm_zone\",\n",
    "    \"station_type\",\n",
    "    \"year\",\n",
    "]\n",
    "par_cols = [i for i in lds_all.columns if i not in (st_cols + unwant_cols)]\n",
    "\n",
    "for col in unwant_cols:\n",
    "    del lds_all[col]\n",
    "\n",
    "lds_all = lds_all[st_cols + par_cols]\n",
    "\n",
    "# Write output\n",
    "out_csv = f\"/home/jovyan/shared/common/elveovervakingsprogrammet/results/measured_loads/loads_and_flows_all_sites_{year}.csv\"\n",
    "lds_all.to_csv(out_csv, encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Loads for the RID_20 rivers through time\n",
    "\n",
    "The code below is taken from Section 3 of [notebook 3](http://nbviewer.jupyter.org/github/JamesSample/rid/blob/master/notebooks/estimate_loads.ipynb).\n",
    "\n",
    "Note the use of the `'samp_sel'` argument in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Period of interest\n",
    "st_yr, end_yr = 1990, year\n",
    "\n",
    "# Container for results\n",
    "loads_list = []\n",
    "\n",
    "# Loop over sites\n",
    "for stn_id in rid_20_df[\"station_id\"].values:\n",
    "    # Loop over years\n",
    "    for data_yr in range(st_yr, end_yr + 1):\n",
    "        print(\"Processing Station ID %s for %s\" % (stn_id, data_yr))\n",
    "\n",
    "        # Get loads\n",
    "        l_df = rid.estimate_loads(\n",
    "            stn_id, par_list, data_yr, engine, infer_missing=True, samp_sel=63\n",
    "        )\n",
    "\n",
    "        if l_df is not None:\n",
    "            # Name and reset index\n",
    "            l_df.index.name = \"station_id\"\n",
    "            l_df.reset_index(inplace=True)\n",
    "\n",
    "            # Add year\n",
    "            l_df[\"year\"] = data_yr\n",
    "\n",
    "            # Add to outout\n",
    "            loads_list.append(l_df)\n",
    "\n",
    "# Concatenate to new df\n",
    "lds_ts = pd.concat(loads_list, axis=0)\n",
    "\n",
    "# Join\n",
    "lds_q_ts = pd.merge(lds_ts, rid_20_df, how=\"left\", on=\"station_id\")\n",
    "lds_q_ts = pd.merge(lds_q_ts, q_df, how=\"left\", on=[\"station_id\", \"year\"])\n",
    "\n",
    "# Reorder cols and tidy\n",
    "st_cols = [\n",
    "    \"station_id\",\n",
    "    \"station_code\",\n",
    "    \"station_name\",\n",
    "    \"old_rid_group\",\n",
    "    \"new_rid_group\",\n",
    "    \"ospar_region\",\n",
    "    \"mean_q_1000m3/day\",\n",
    "]\n",
    "unwant_cols = [\n",
    "    \"nve_vassdrag_nr\",\n",
    "    \"lat\",\n",
    "    \"lon\",\n",
    "    \"utm_north\",\n",
    "    \"utm_east\",\n",
    "    \"utm_zone\",\n",
    "    \"station_type\",\n",
    "]\n",
    "par_cols = [i for i in lds_q_ts.columns if i not in (st_cols + unwant_cols)]\n",
    "\n",
    "for col in unwant_cols:\n",
    "    del lds_q_ts[col]\n",
    "\n",
    "lds_q_ts = lds_q_ts[st_cols + par_cols]\n",
    "\n",
    "# Save output\n",
    "out_csv = f\"/home/jovyan/shared/common/elveovervakingsprogrammet/results/measured_loads/loads_and_flows_rid_20_{st_yr}-{end_yr}.csv\"\n",
    "lds_q_ts.to_csv(out_csv, encoding=\"utf-8\", index=False)\n",
    "\n",
    "# Build multi-index on lds_ts for further processing\n",
    "lds_ts.set_index([\"station_id\", \"year\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# This code cell produces lots of Deprecation Warnings from Seaborn/Pandas.\n",
    "# %%capture suppresses all output from this cell to keep things tidy\n",
    "\n",
    "# Output folder for plots\n",
    "out_fold = f\"/home/jovyan/shared/common/elveovervakingsprogrammet/results/ts_plots/rid_plots_to_{year}\"\n",
    "if not os.path.isdir(out_fold):\n",
    "    os.mkdir(out_fold)\n",
    "\n",
    "# Loop over df\n",
    "for stn_id in rid_20_df[\"station_id\"].values:\n",
    "    # Get data for this station\n",
    "    df = lds_ts.loc[stn_id]\n",
    "\n",
    "    # Separate est and val cols to two dfs\n",
    "    cols = df.columns\n",
    "    est_cols = [i for i in cols if i.split(\"_\")[1] == \"Est\"]\n",
    "    val_cols = [i for i in cols if i.split(\"_\")[1] != \"Est\"]\n",
    "    val_df = df[val_cols]\n",
    "    est_df = df[est_cols]\n",
    "\n",
    "    # Convert to \"long\" format\n",
    "    val_df.reset_index(inplace=True)\n",
    "    val_df = pd.melt(val_df, id_vars=\"year\", var_name=\"par_unit\")\n",
    "    est_df.reset_index(inplace=True)\n",
    "    est_df = pd.melt(est_df, id_vars=\"year\", var_name=\"par_est\", value_name=\"est\")\n",
    "\n",
    "    # Get just par for joining\n",
    "    val_df[\"par\"] = val_df[\"par_unit\"].str.split(\"_\", expand=True)[0]\n",
    "    est_df[\"par\"] = est_df[\"par_est\"].str.split(\"_\", expand=True)[0]\n",
    "\n",
    "    # Join\n",
    "    df = pd.merge(val_df, est_df, how=\"left\", on=[\"year\", \"par\"])\n",
    "\n",
    "    # Extract cols of interest\n",
    "    df = df[[\"year\", \"par_unit\", \"value\", \"est\"]]\n",
    "\n",
    "    # Plot\n",
    "    g = sn.catplot(\n",
    "        x=\"year\",\n",
    "        y=\"value\",\n",
    "        hue=\"est\",\n",
    "        col=\"par_unit\",\n",
    "        col_wrap=3,\n",
    "        data=df,\n",
    "        kind=\"bar\",\n",
    "        dodge=False,\n",
    "        sharex=False,\n",
    "        sharey=False,\n",
    "        alpha=0.5,\n",
    "        aspect=2,\n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "    # Rotate tick labels and tidy\n",
    "    for ax in g.axes.flatten():\n",
    "        for tick in ax.get_xticklabels():\n",
    "            tick.set(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save\n",
    "    out_path = os.path.join(out_fold, f\"{stn_id}.png\")\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three files created above (`concs_and_flows_rid_11-36_{year}.csv`, `loads_and_flows_all_sites_{year}.csv` and `loads_and_flows_rid_11_1990-{year}.csv`) can now be imported into Excel and send to NIBIO. The data layout is illustrated here:\n",
    "\n",
    "C:\\Data\\James_Work\\Staff\\Oeyvind_K\\Elveovervakingsprogrammet\\Results\\Loads_CSVs\\rid_conc_and_loads_summaries_2016.xlsx\n",
    "\n",
    "**NB:** For neatness, a couple of columns can be manually reordered so that the \"flag\" columns always come before the data columns.\n",
    "\n",
    "## 4. Generate output tables for Word\n",
    "\n",
    "### 4.1. Table 1: Raw water chemistry\n",
    "\n",
    "The code below is based on Section 2 of [notebook 5](http://nbviewer.jupyter.org/github/JamesSample/rid/blob/master/notebooks/word_data_tables.ipynb).\n",
    "\n",
    "**Updated 24/09/2018**\n",
    "\n",
    "This function has been modified to refelect changes in the 2017-20 monitoring programme:\n",
    "\n",
    " 1. The Word template now has pages for just the 20 \"main\" rivers, not the 11 + 36 rivers, as previously <br><br>\n",
    " \n",
    " 2. Four new columns have been added for new parameters measured during 2017-20 (DOC, Part. C, Tot. Part. N and TDP) <br><br>\n",
    " \n",
    " 3. Hours and minutes have been removed from the date-time column to create space for the new columns <br><br>\n",
    " \n",
    " 4. I have corrected various typos in the database (and in the template):\n",
    " \n",
    "     * `'Tot.part. N'` > `'Tot. Part. N'`\n",
    "     * `'Vosso(Bolstadelvi)'` > `'Vosso (Bolstadelvi)'`\n",
    "     * `'Nidelva(Tr.heim)'` > `'Nidelva (Tr.heim)'`\n",
    "     * `'More than 70%LOD'` > `'More than 70% >LOD'` (template only) \n",
    "     \n",
    "**Updated 25.08.2020**\n",
    "\n",
    "For the 2019 data, \"Målselv\" has been replaced by a new station downstream, \"Målselv v/gml E6-brua\" (see e-mail from Øyvind receievd 18.08.2020 at 08:09 for details). I have updated the templates to reflect this.\n",
    "\n",
    "\n",
    "**Updated 19.05.2022**\n",
    "\n",
    "Flow data are not available for the new reporting deadline in June. Flow has therefore been removed from the template for 2021 onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_path = rid.copy_word_template(1, year)\n",
    "rid.write_word_water_chem_tables(\n",
    "    rid_20_df, year, tab_path, engine, samp_sel=63, extract_flow=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Table 2: Estimated loads at each site\n",
    "\n",
    "The code below is based on Section 3 of [notebook 5](http://nbviewer.jupyter.org/github/JamesSample/rid/blob/master/notebooks/word_data_tables.ipynb).\n",
    "\n",
    "**Updated 24/09/2018**\n",
    "\n",
    "For the 2017-20 programme, we will only report loads for the 20 \"main\" rivers, not all 155. I have therefore simplified the Word template by deleting unnecessary rows. The function itself is unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_path = rid.copy_word_template(2, year)\n",
    "loads_csv = f\"/home/jovyan/shared/common/elveovervakingsprogrammet/results/measured_loads/loads_and_flows_all_sites_{year}.csv\"\n",
    "\n",
    "# Drop Målselv as no longer monitored in main programme\n",
    "stn_df = rid_20_df.query(\"station_name != 'Målselv'\")\n",
    "\n",
    "rid.write_word_loads_table(stn_df, loads_csv, tab_path, engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
