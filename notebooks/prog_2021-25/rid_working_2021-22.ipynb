{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nivapy3 as nivapy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import useful_rid_code as rid\n",
    "\n",
    "sn.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Username:  ···\n",
      "Password:  ···············\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful.\n"
     ]
    }
   ],
   "source": [
    "# Connect to db\n",
    "engine = nivapy.da.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year of interest\n",
    "year = 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RID 2021-22: data processing notebook\n",
    "\n",
    "Some things are a little different compared to the previous programme. In particular, reporting deadlines have changed - see e-mail from Øyvind received 05.05.2022 at 11.04 for details.\n",
    "\n",
    "## 1. Add 2021 datasets\n",
    "\n",
    "### 1.1. Update flow datasets\n",
    "\n",
    "**Not necessary until autumn 2022**.\n",
    "\n",
    "The notebook `update_flow_nve_hydapi.ipynb` can be used to update flow datasets in RESA2. Note that not all datasets are available via HydAPI, so it is still necessary to request some datasets directly from Trine at NVE.\n",
    "\n",
    "### 1.2. Water chemistry quality control\n",
    "\n",
    "The water samples collected for this project are analysed by the NIVA laboratory and results are automatically transferred to the RESA2 database. Liv Bente has **not** quality-checked the 2021 data (see e-mail received 12.05.2022 at 14.40). **I have therefore done my best to quickly quality-assess the data in RESA using `compare_am_resa_rid.ipynb`. **\n",
    "\n",
    "### 1.3. Sample selections\n",
    "\n",
    "Previous analysis for the RID report only used water samples collected as part of the \"core\" monitoring programme (i.e. not flood samples or those collected under Option 3). For 2021-5, the option 3 samples have been moved to a separate project called Bk-stations, so not sure whether this is still relevant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read new site groupings (for 2017 to 2020)\n",
    "in_xlsx = r\"../../../../Data/RID_Sites_List_2017-2020.xlsx\"\n",
    "rid_20_df = pd.read_excel(in_xlsx, sheet_name=\"RID_20\")\n",
    "rid_135_df = pd.read_excel(in_xlsx, sheet_name=\"RID_135\")\n",
    "rid_155_df = pd.read_excel(in_xlsx, sheet_name=\"RID_All\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1. Option 3/Bk-stations\n",
    "\n",
    "In the programme for 2017-20, additional samples were collected under \"Option 3\". In the programme for 2021-25, this sampling takes place as an entirely separate project, called the \"Bk-stasjoner\" (RESA project ID 4591). Samples from these stations will be treated the same as the Option 3 samples previously (i.e. added to `sample_selection 65`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 stations in the Bk-project.\n",
      "The following Bk-stations are also part of the RID 155.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>station_code</th>\n",
       "      <th>station_name</th>\n",
       "      <th>old_rid_group</th>\n",
       "      <th>new_rid_group</th>\n",
       "      <th>ospar_region</th>\n",
       "      <th>station_type</th>\n",
       "      <th>nve_vassdrag_nr</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>utm_north</th>\n",
       "      <th>utm_east</th>\n",
       "      <th>utm_zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>29825</td>\n",
       "      <td>NORERAN</td>\n",
       "      <td>Ranaelva</td>\n",
       "      <td>rid_36</td>\n",
       "      <td>rid_135</td>\n",
       "      <td>NORWEGIAN SEA2</td>\n",
       "      <td>R</td>\n",
       "      <td>156.A0</td>\n",
       "      <td>66.322993</td>\n",
       "      <td>14.176984</td>\n",
       "      <td>7356155.0</td>\n",
       "      <td>463120.0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>29826</td>\n",
       "      <td>NORERØS</td>\n",
       "      <td>Røssåga</td>\n",
       "      <td>rid_36</td>\n",
       "      <td>rid_135</td>\n",
       "      <td>NORWEGIAN SEA2</td>\n",
       "      <td>R</td>\n",
       "      <td>155.A0</td>\n",
       "      <td>66.108993</td>\n",
       "      <td>13.806986</td>\n",
       "      <td>7332572.0</td>\n",
       "      <td>446087.0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29829</td>\n",
       "      <td>NTREVER</td>\n",
       "      <td>Verdalselva</td>\n",
       "      <td>rid_36</td>\n",
       "      <td>rid_135</td>\n",
       "      <td>NORWEGIAN SEA2</td>\n",
       "      <td>R</td>\n",
       "      <td>127.A0</td>\n",
       "      <td>63.791995</td>\n",
       "      <td>11.477993</td>\n",
       "      <td>7076205.0</td>\n",
       "      <td>622081.0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>29843</td>\n",
       "      <td>STREGAU</td>\n",
       "      <td>Gaula</td>\n",
       "      <td>rid_36</td>\n",
       "      <td>rid_135</td>\n",
       "      <td>NORWEGIAN SEA2</td>\n",
       "      <td>R</td>\n",
       "      <td>122.A24</td>\n",
       "      <td>63.285993</td>\n",
       "      <td>10.269990</td>\n",
       "      <td>7018085.0</td>\n",
       "      <td>563695.0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    station_id station_code station_name old_rid_group new_rid_group  \\\n",
       "23       29825      NORERAN     Ranaelva        rid_36       rid_135   \n",
       "24       29826      NORERØS      Røssåga        rid_36       rid_135   \n",
       "27       29829      NTREVER  Verdalselva        rid_36       rid_135   \n",
       "38       29843      STREGAU        Gaula        rid_36       rid_135   \n",
       "\n",
       "      ospar_region station_type nve_vassdrag_nr        lat        lon  \\\n",
       "23  NORWEGIAN SEA2            R          156.A0  66.322993  14.176984   \n",
       "24  NORWEGIAN SEA2            R          155.A0  66.108993  13.806986   \n",
       "27  NORWEGIAN SEA2            R          127.A0  63.791995  11.477993   \n",
       "38  NORWEGIAN SEA2            R         122.A24  63.285993  10.269990   \n",
       "\n",
       "    utm_north  utm_east  utm_zone  \n",
       "23  7356155.0  463120.0        33  \n",
       "24  7332572.0  446087.0        33  \n",
       "27  7076205.0  622081.0        32  \n",
       "38  7018085.0  563695.0        32  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Bk-stations\n",
    "bk_df = nivapy.da.select_resa_project_stations([4591], engine)\n",
    "print(len(bk_df), \"stations in the Bk-project.\")\n",
    "\n",
    "# Find any Bk-stations also in the \"main\" project\n",
    "bk_in_155 = set(rid_155_df[\"station_id\"]).intersection(set(bk_df[\"station_id\"]))\n",
    "print(\"The following Bk-stations are also part of the RID 155.\")\n",
    "bk_in_155 = rid_155_df.query(\"station_id in @bk_in_155\")\n",
    "bk_in_155"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/nivapy3-0.1-py3.9.egg/nivapy3/da.py:433: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stn_df[\"station_id\"].drop_duplicates(inplace=True)\n",
      "/opt/conda/lib/python3.9/site-packages/nivapy3-0.1-py3.9.egg/nivapy3/da.py:555: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stn_df[\"station_id\"].drop_duplicates(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 parameters available for the selected stations and dates.\n",
      "47 samples to be linked to 'Option 3'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>station_code</th>\n",
       "      <th>station_name</th>\n",
       "      <th>sample_date</th>\n",
       "      <th>depth1</th>\n",
       "      <th>depth2</th>\n",
       "      <th>ALK_mmol/l</th>\n",
       "      <th>As (filtrert)_µg/l</th>\n",
       "      <th>Ca_mg/l</th>\n",
       "      <th>Cd (filtrert)_µg/l</th>\n",
       "      <th>...</th>\n",
       "      <th>SPM_mg/l</th>\n",
       "      <th>SiO2_mg SiO2/l</th>\n",
       "      <th>Si_mg/l</th>\n",
       "      <th>TOC_mg C/l</th>\n",
       "      <th>TOTN_µg/l N</th>\n",
       "      <th>TOTP_µg/l P</th>\n",
       "      <th>TURB860_FNU</th>\n",
       "      <th>Temp_oC</th>\n",
       "      <th>Zn_µg/l</th>\n",
       "      <th>pH_</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29825</td>\n",
       "      <td>NORERAN</td>\n",
       "      <td>Ranaelva</td>\n",
       "      <td>2021-02-16 13:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.74</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29825</td>\n",
       "      <td>NORERAN</td>\n",
       "      <td>Ranaelva</td>\n",
       "      <td>2021-03-09 12:10:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.83</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29825</td>\n",
       "      <td>NORERAN</td>\n",
       "      <td>Ranaelva</td>\n",
       "      <td>2021-03-22 09:50:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.77</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29825</td>\n",
       "      <td>NORERAN</td>\n",
       "      <td>Ranaelva</td>\n",
       "      <td>2021-04-10 10:20:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.98</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.936</td>\n",
       "      <td>1.40</td>\n",
       "      <td>230.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29825</td>\n",
       "      <td>NORERAN</td>\n",
       "      <td>Ranaelva</td>\n",
       "      <td>2021-05-04 10:20:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.828</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.92</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.808</td>\n",
       "      <td>1.30</td>\n",
       "      <td>210.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_id station_code station_name         sample_date  depth1  depth2  \\\n",
       "                                                                              \n",
       "0       29825      NORERAN     Ranaelva 2021-02-16 13:00:00       0       0   \n",
       "1       29825      NORERAN     Ranaelva 2021-03-09 12:10:00       0       0   \n",
       "2       29825      NORERAN     Ranaelva 2021-03-22 09:50:00       0       0   \n",
       "3       29825      NORERAN     Ranaelva 2021-04-10 10:20:00       0       0   \n",
       "4       29825      NORERAN     Ranaelva 2021-05-04 10:20:00       0       0   \n",
       "\n",
       "   ALK_mmol/l  As (filtrert)_µg/l  Ca_mg/l  Cd (filtrert)_µg/l  ...  SPM_mg/l  \\\n",
       "                                                                ...             \n",
       "0         NaN                 NaN     4.32                 NaN  ...      0.28   \n",
       "1         NaN                 NaN     4.77                 NaN  ...      0.20   \n",
       "2         NaN                 NaN     5.19                 NaN  ...      0.37   \n",
       "3         NaN                 NaN    15.60                 NaN  ...      1.98   \n",
       "4       0.828                 NaN    16.40                 NaN  ...      1.92   \n",
       "\n",
       "   SiO2_mg SiO2/l  Si_mg/l  TOC_mg C/l  TOTN_µg/l N  TOTP_µg/l P  TURB860_FNU  \\\n",
       "                                                                                \n",
       "0            1.19    0.571        0.74         93.0          1.0         0.30   \n",
       "1            1.22    0.592        0.83         54.0          1.0         0.30   \n",
       "2            1.26    0.599        0.77        120.0          1.0         0.30   \n",
       "3            2.00    0.936        1.40        230.0          4.0         0.90   \n",
       "4            1.71    0.808        1.30        210.0          4.0         0.75   \n",
       "\n",
       "   Temp_oC  Zn_µg/l   pH_  \n",
       "                           \n",
       "0      0.0      NaN  7.34  \n",
       "1      0.6      NaN  7.36  \n",
       "2      0.6      NaN  7.32  \n",
       "3      0.7      NaN  7.74  \n",
       "4      3.9      NaN  7.91  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get data from Bk-stations in RID 155 for year of interest\n",
    "bk_par_df = nivapy.da.select_resa_station_parameters(\n",
    "    bk_in_155, f\"{year}-01-01\", f\"{year}-12-31\", engine\n",
    ")\n",
    "bk_wc_df, bk_dup_df = nivapy.da.select_resa_water_chemistry(\n",
    "    bk_in_155, bk_par_df, f\"{year}-01-01\", f\"{year}-12-31\", engine\n",
    ")\n",
    "print(len(bk_wc_df), \"samples to be linked to 'Option 3'.\")\n",
    "bk_wc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add samples from Bk-stations also in RID 155 to \"option 3\"\n",
    "ws_ids = []\n",
    "for idx, row in bk_wc_df.iterrows():\n",
    "    sql = (\n",
    "        \"SELECT water_sample_id FROM resa2.water_samples \"\n",
    "        \"WHERE station_id = %s \"\n",
    "        \"AND TRUNC(sample_date) = DATE '%s' \"\n",
    "        \"AND depth1 = %s \"\n",
    "        \"AND depth2 = %s\"\n",
    "        % (\n",
    "            row[\"station_id\"],\n",
    "            row[\"sample_date\"].strftime(\"%Y-%m-%d\"),\n",
    "            row[\"depth1\"],\n",
    "            row[\"depth2\"],\n",
    "        )\n",
    "    )\n",
    "    ws_id = engine.execute(sql).fetchall()[0]\n",
    "    assert len(ws_id) == 1\n",
    "    ws_id = ws_id[0]\n",
    "    ws_ids.append(ws_id)\n",
    "\n",
    "ws_df = pd.DataFrame(\n",
    "    {\n",
    "        \"water_sample_id\": ws_ids,\n",
    "    }\n",
    ")\n",
    "ws_df[\"sample_selection_id\"] = 65\n",
    "\n",
    "assert len(bk_wc_df) == len(ws_df)\n",
    "\n",
    "# ws_df.to_sql(\n",
    "#     \"sample_selections\", con=engine, schema=\"resa2\", if_exists=\"append\", index=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2. Flood samples\n",
    "\n",
    "Not relevant for 2021 (?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get WS IDs for flood samples\n",
    "# fl_xlsx = f\"../../../Data/flood_samples/flood_samples_{year}.xlsx\"\n",
    "# fl_df = pd.read_excel(fl_xlsx, sheet_name=f\"all_flood_samples_{year}\")\n",
    "\n",
    "# ws_ids = []\n",
    "# for idx, row in fl_df.iterrows():\n",
    "#     sql = (\n",
    "#         \"SELECT water_sample_id FROM resa2.water_samples \"\n",
    "#         \"WHERE station_id = %s \"\n",
    "#         \"AND TRUNC(sample_date) = DATE '%s' \"\n",
    "#         \"AND depth1 = %s \"\n",
    "#         \"AND depth2 = %s\"\n",
    "#         % (\n",
    "#             row[\"station_id\"],\n",
    "#             row[\"sample_date\"].strftime(\"%Y-%m-%d\"),\n",
    "#             row[\"depth1\"],\n",
    "#             row[\"depth2\"],\n",
    "#         )\n",
    "#     )\n",
    "#     ws_id = engine.execute(sql).fetchall()[0]\n",
    "#     assert len(ws_id) == 1\n",
    "#     ws_id = ws_id[0]\n",
    "#     ws_ids.append(ws_id)\n",
    "\n",
    "# ws_df = pd.DataFrame(\n",
    "#     {\n",
    "#         \"water_sample_id\": ws_ids,\n",
    "#     }\n",
    "# )\n",
    "# ws_df[\"sample_selection_id\"] = 64\n",
    "\n",
    "# assert len(fl_df) == len(ws_df)\n",
    "\n",
    "# # ws_df.to_sql(\n",
    "# #     \"sample_selections\", con=engine, schema=\"resa2\", if_exists=\"append\", index=False\n",
    "# # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3. Main programme\n",
    "\n",
    "Everything else (i.e. not Option 3 or flood) is assumed to be part of the main programme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257 samples in the main programme.\n"
     ]
    }
   ],
   "source": [
    "# Get flood and Option 3 samples\n",
    "sql = (\n",
    "    \"SELECT water_sample_id \"\n",
    "    \"FROM resa2.sample_selections \"\n",
    "    \"WHERE sample_selection_id IN (64, 65)\"\n",
    ")\n",
    "oth_ws = pd.read_sql_query(sql, engine)\n",
    "assert oth_ws[\"water_sample_id\"].is_unique\n",
    "\n",
    "# Get all WS associated with core sites\n",
    "sql = (\n",
    "    \"SELECT water_sample_id FROM resa2.water_samples \"\n",
    "    \"WHERE station_id IN %s \"\n",
    "    \"AND sample_date >= DATE '%s-01-01' \"\n",
    "    \"AND sample_date < DATE '%s-01-01'\"\n",
    "    % (str(tuple(rid_155_df[\"station_id\"].astype(int))), year, year + 1)\n",
    ")\n",
    "all_ws = pd.read_sql_query(sql, engine)\n",
    "assert all_ws[\"water_sample_id\"].is_unique\n",
    "\n",
    "# Remove flood and option 3 samples from core\n",
    "core_ws = set(all_ws[\"water_sample_id\"]) - set(oth_ws[\"water_sample_id\"])\n",
    "\n",
    "# Add to sample selections\n",
    "core_df = pd.DataFrame({\"water_sample_id\": list(core_ws)})\n",
    "core_df[\"sample_selection_id\"] = 63\n",
    "\n",
    "print(len(core_df), \"samples in the main programme.\")\n",
    "\n",
    "# core_df.to_sql(\n",
    "#     \"sample_selections\", con=engine, schema=\"resa2\", if_exists=\"append\", index=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tabulate raw water chemistry and flow\n",
    "\n",
    "### 2.1. Data for year of interest\n",
    "\n",
    "From 2017 onwards, water chemistry samples have been collected at 20 sites (`RID_20`). In 2018, the station TROEMÅL2 was added to the RID_20 selection, making 21 stations in total. Both TROEMÅL and TROEMÅL2 were monitored in 2018, but from 2019 onwards TROEMÅL2 replaced TROEMÅL in the main programme (although TROEMÅL is sometimes still included as part of Option 3).\n",
    "\n",
    "The data are exported to CSV format below.\n",
    "\n",
    "**Added 19.05.2022:** For 2021-5, I have added an extra kwarg named `extract_flow` to the function below. This is because flow data are not ready for the new reporting deadlines in June, so we need to be able to process the concentration part without flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing station ID 30019\n",
      "Processing station ID 29612\n",
      "Processing station ID 29779\n",
      "Processing station ID 29819\n",
      "Processing station ID 29820\n",
      "Processing station ID 29821\n",
      "Processing station ID 29822\n",
      "Processing station ID 29782\n",
      "Processing station ID 36225\n",
      "Processing station ID 29832\n",
      "Processing station ID 29783\n",
      "Processing station ID 29837\n",
      "Processing station ID 29842\n",
      "Processing station ID 29844\n",
      "Processing station ID 29778\n",
      "Processing station ID 29613\n",
      "Processing station ID 29848\n",
      "    No chemistry data found for station ID 29848.\n",
      "Processing station ID 29614\n",
      "Processing station ID 29615\n",
      "Processing station ID 29617\n",
      "Processing station ID 38005\n",
      "    WARNING\n",
      "    The database contains duplicated values for some station-date-parameter combinations.\n",
      "    Only the most recent values will be used, but you should check the repeated values are not errors.\n",
      "    The duplicated entries are returned in a separate dataframe.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Output CSV\n",
    "out_csv = f\"../../../../Results/Loads_CSVs/concs_and_flows_rid_20_{year}.csv\"\n",
    "df = rid.write_csv_water_chem(\n",
    "    rid_20_df, year, out_csv, engine, samp_sel=63, extract_flow=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Data for all years\n",
    "\n",
    "**Added 03.09.2018**. From 2017, we include estimates of trends in the \"main\" rivers for both loads and concentrations for the period from 1990 to present.\n",
    "\n",
    "**Added 19.05.2022**. Flow (Q) is commented-out below as flow data are not available in time for the June deadline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Container for data\n",
    "df_list = []\n",
    "\n",
    "# Dummy path for intermediate output (which isn't needed here)\n",
    "out_csv = r\"../../../../Results/Loads_CSVs/cons_and_flows_intermed.csv\"\n",
    "\n",
    "# Loop over years\n",
    "for data_yr in range(1990, year + 1):\n",
    "    # Get data\n",
    "    df = rid.write_csv_water_chem(\n",
    "        rid_20_df, data_yr, out_csv, engine, samp_sel=63, extract_flow=False\n",
    "    )\n",
    "\n",
    "    # Add to output\n",
    "    df_list.append(df)\n",
    "\n",
    "# Delete intermediate\n",
    "os.remove(out_csv)\n",
    "\n",
    "# Combine\n",
    "df = pd.concat(df_list, axis=0)\n",
    "\n",
    "# Reorder cols and tidy\n",
    "st_cols = [\n",
    "    \"station_id\",\n",
    "    \"station_code\",\n",
    "    \"station_name\",\n",
    "    \"old_rid_group\",\n",
    "    \"new_rid_group\",\n",
    "    \"ospar_region\",\n",
    "    \"sample_date\",\n",
    "    #    \"Qs_m3/s\",\n",
    "]\n",
    "par_cols = [i for i in df.columns if i not in st_cols]\n",
    "par_cols.sort()\n",
    "df = df[st_cols + par_cols]\n",
    "\n",
    "# Output CSV\n",
    "out_csv = f\"../../../../Results/Loads_CSVs/concs_and_flows_rid_20_1990-{year}.csv\"\n",
    "df.to_csv(out_csv, encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Estimate observed loads\n",
    "\n",
    "### 3.1. Annual flows\n",
    "\n",
    "First get a dataframe of annual flow volumes to join to the summary output. **NB:** This dataframe isn't actually used in the loads calculations - they are handled separately - it's just for the output CSVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sites of interest: combine all site dfs into one\n",
    "rid_all_df = pd.concat([rid_20_df, rid_135_df], axis=0)\n",
    "\n",
    "# Get flow data\n",
    "q_df = rid.get_flow_volumes(rid_all_df, 1990, year, engine)\n",
    "\n",
    "q_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Loads for all rivers\n",
    "\n",
    "The code below is taken from Section 2 of [notebook 3](http://nbviewer.jupyter.org/github/JamesSample/rid/blob/master/notebooks/estimate_loads.ipynb). Loads are calculated directly from contemporary observations for the RID_20, and they are inferred from historic concentrations for the RID_135 sites.\n",
    "\n",
    "As above, note the use of the `'samp_sel'` argument in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sites of interest: combine all site dfs into one\n",
    "rid_all_df = pd.concat([rid_20_df, rid_135_df], axis=0)\n",
    "\n",
    "# Pars of interest\n",
    "par_list = [\n",
    "    \"SPM\",\n",
    "    \"TOC\",\n",
    "    \"PO4-P\",\n",
    "    \"TOTP\",\n",
    "    \"NO3-N\",\n",
    "    \"NH4-N\",\n",
    "    \"TOTN\",\n",
    "    \"SiO2\",\n",
    "    \"Ag\",\n",
    "    \"As\",\n",
    "    \"Pb\",\n",
    "    \"Cd\",\n",
    "    \"Cu\",\n",
    "    \"Zn\",\n",
    "    \"Ni\",\n",
    "    \"Cr\",\n",
    "    \"Hg\",\n",
    "]\n",
    "\n",
    "# Container for results from each site\n",
    "loads_list = []\n",
    "\n",
    "# Loop over sites\n",
    "for stn_id in rid_all_df[\"station_id\"].values:\n",
    "    # Estimate loads at this site\n",
    "    loads_list.append(\n",
    "        rid.estimate_loads(\n",
    "            stn_id, par_list, year, engine, infer_missing=True, samp_sel=63\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Concatenate to new df\n",
    "lds_all = pd.concat(loads_list, axis=0)\n",
    "lds_all.index.name = \"station_id\"\n",
    "lds_all.reset_index(inplace=True)\n",
    "\n",
    "# Get flow data for year\n",
    "q_yr = q_df.query(\"year == @year\")\n",
    "\n",
    "# Join\n",
    "lds_all = pd.merge(lds_all, rid_all_df, how=\"left\", on=\"station_id\")\n",
    "lds_all = pd.merge(lds_all, q_yr, how=\"left\", on=\"station_id\")\n",
    "\n",
    "# Reorder cols and tidy\n",
    "st_cols = [\n",
    "    \"station_id\",\n",
    "    \"station_code\",\n",
    "    \"station_name\",\n",
    "    \"old_rid_group\",\n",
    "    \"new_rid_group\",\n",
    "    \"ospar_region\",\n",
    "    \"mean_q_1000m3/day\",\n",
    "]\n",
    "unwant_cols = [\n",
    "    \"nve_vassdrag_nr\",\n",
    "    \"lat\",\n",
    "    \"lon\",\n",
    "    \"utm_north\",\n",
    "    \"utm_east\",\n",
    "    \"utm_zone\",\n",
    "    \"station_type\",\n",
    "    \"year\",\n",
    "]\n",
    "par_cols = [i for i in lds_all.columns if i not in (st_cols + unwant_cols)]\n",
    "\n",
    "for col in unwant_cols:\n",
    "    del lds_all[col]\n",
    "\n",
    "lds_all = lds_all[st_cols + par_cols]\n",
    "\n",
    "# Write output\n",
    "out_csv = f\"../../../Results/Loads_CSVs/loads_and_flows_all_sites_{year}.csv\"\n",
    "lds_all.to_csv(out_csv, encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Loads for the RID_20 rivers through time\n",
    "\n",
    "The code below is taken from Section 3 of [notebook 3](http://nbviewer.jupyter.org/github/JamesSample/rid/blob/master/notebooks/estimate_loads.ipynb).\n",
    "\n",
    "Note the use of the `'samp_sel'` argument in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Period of interest\n",
    "st_yr, end_yr = 1990, year\n",
    "\n",
    "# Container for results\n",
    "loads_list = []\n",
    "\n",
    "# Loop over sites\n",
    "for stn_id in rid_20_df[\"station_id\"].values:\n",
    "    # Loop over years\n",
    "    for data_yr in range(st_yr, end_yr + 1):\n",
    "        print(\"Processing Station ID %s for %s\" % (stn_id, data_yr))\n",
    "\n",
    "        # Get loads\n",
    "        l_df = rid.estimate_loads(\n",
    "            stn_id, par_list, data_yr, engine, infer_missing=True, samp_sel=63\n",
    "        )\n",
    "\n",
    "        if l_df is not None:\n",
    "            # Name and reset index\n",
    "            l_df.index.name = \"station_id\"\n",
    "            l_df.reset_index(inplace=True)\n",
    "\n",
    "            # Add year\n",
    "            l_df[\"year\"] = data_yr\n",
    "\n",
    "            # Add to outout\n",
    "            loads_list.append(l_df)\n",
    "\n",
    "# Concatenate to new df\n",
    "lds_ts = pd.concat(loads_list, axis=0)\n",
    "\n",
    "# Join\n",
    "lds_q_ts = pd.merge(lds_ts, rid_20_df, how=\"left\", on=\"station_id\")\n",
    "lds_q_ts = pd.merge(lds_q_ts, q_df, how=\"left\", on=[\"station_id\", \"year\"])\n",
    "\n",
    "# Reorder cols and tidy\n",
    "st_cols = [\n",
    "    \"station_id\",\n",
    "    \"station_code\",\n",
    "    \"station_name\",\n",
    "    \"old_rid_group\",\n",
    "    \"new_rid_group\",\n",
    "    \"ospar_region\",\n",
    "    \"mean_q_1000m3/day\",\n",
    "]\n",
    "unwant_cols = [\n",
    "    \"nve_vassdrag_nr\",\n",
    "    \"lat\",\n",
    "    \"lon\",\n",
    "    \"utm_north\",\n",
    "    \"utm_east\",\n",
    "    \"utm_zone\",\n",
    "    \"station_type\",\n",
    "]\n",
    "par_cols = [i for i in lds_q_ts.columns if i not in (st_cols + unwant_cols)]\n",
    "\n",
    "for col in unwant_cols:\n",
    "    del lds_q_ts[col]\n",
    "\n",
    "lds_q_ts = lds_q_ts[st_cols + par_cols]\n",
    "\n",
    "# Save output\n",
    "out_csv = f\"../../../Results/Loads_CSVs/loads_and_flows_rid_20_{st_yr}-{end_yr}.csv\"\n",
    "lds_q_ts.to_csv(out_csv, encoding=\"utf-8\", index=False)\n",
    "\n",
    "# Build multi-index on lds_ts for further processing\n",
    "lds_ts.set_index([\"station_id\", \"year\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# This code cell produces lots of Deprecation Warnings from Seaborn/Pandas.\n",
    "# %%capture suppresses all output from this cell to keep things tidy\n",
    "\n",
    "# Output folder for plots\n",
    "out_fold = f\"../../../Results/TS_Plots/RID_Plots_To_{year}\"\n",
    "if os.path.isdir(out_fold) == False:\n",
    "    os.mkdir(out_fold)\n",
    "\n",
    "# Loop over df\n",
    "for stn_id in rid_20_df[\"station_id\"].values:\n",
    "    # Get data for this station\n",
    "    df = lds_ts.loc[stn_id]\n",
    "\n",
    "    # Separate est and val cols to two dfs\n",
    "    cols = df.columns\n",
    "    est_cols = [i for i in cols if i.split(\"_\")[1] == \"Est\"]\n",
    "    val_cols = [i for i in cols if i.split(\"_\")[1] != \"Est\"]\n",
    "    val_df = df[val_cols]\n",
    "    est_df = df[est_cols]\n",
    "\n",
    "    # Convert to \"long\" format\n",
    "    val_df.reset_index(inplace=True)\n",
    "    val_df = pd.melt(val_df, id_vars=\"year\", var_name=\"par_unit\")\n",
    "    est_df.reset_index(inplace=True)\n",
    "    est_df = pd.melt(est_df, id_vars=\"year\", var_name=\"par_est\", value_name=\"est\")\n",
    "\n",
    "    # Get just par for joining\n",
    "    val_df[\"par\"] = val_df[\"par_unit\"].str.split(\"_\", expand=True)[0]\n",
    "    est_df[\"par\"] = est_df[\"par_est\"].str.split(\"_\", expand=True)[0]\n",
    "\n",
    "    # Join\n",
    "    df = pd.merge(val_df, est_df, how=\"left\", on=[\"year\", \"par\"])\n",
    "\n",
    "    # Extract cols of interest\n",
    "    df = df[[\"year\", \"par_unit\", \"value\", \"est\"]]\n",
    "\n",
    "    # Plot\n",
    "    g = sn.factorplot(\n",
    "        x=\"year\",\n",
    "        y=\"value\",\n",
    "        hue=\"est\",\n",
    "        col=\"par_unit\",\n",
    "        col_wrap=3,\n",
    "        data=df,\n",
    "        kind=\"bar\",\n",
    "        dodge=False,\n",
    "        sharex=False,\n",
    "        sharey=False,\n",
    "        alpha=0.5,\n",
    "        aspect=2,\n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "    # Rotate tick labels and tidy\n",
    "    for ax in g.axes.flatten():\n",
    "        for tick in ax.get_xticklabels():\n",
    "            tick.set(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save\n",
    "    out_path = os.path.join(out_fold, f\"{stn_id}.png\")\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three files created above (`concs_and_flows_rid_11-36_{year}.csv`, `loads_and_flows_all_sites_{year}.csv` and `loads_and_flows_rid_11_1990-{year}.csv`) can now be imported into Excel and send to NIBIO. The data layout is illustrated here:\n",
    "\n",
    "C:\\Data\\James_Work\\Staff\\Oeyvind_K\\Elveovervakingsprogrammet\\Results\\Loads_CSVs\\rid_conc_and_loads_summaries_2016.xlsx\n",
    "\n",
    "**NB:** For neatness, a couple of columns can be manually reordered so that the \"flag\" columns always come before the data columns.\n",
    "\n",
    "## 4. Generate output tables for Word\n",
    "\n",
    "### 4.1. Table 1: Raw water chemistry\n",
    "\n",
    "The code below is based on Section 2 of [notebook 5](http://nbviewer.jupyter.org/github/JamesSample/rid/blob/master/notebooks/word_data_tables.ipynb).\n",
    "\n",
    "**Updated 24/09/2018**\n",
    "\n",
    "This function has been modified to refelect changes in the 2017-20 monitoring programme:\n",
    "\n",
    " 1. The Word template now has pages for just the 20 \"main\" rivers, not the 11 + 36 rivers, as previously <br><br>\n",
    " \n",
    " 2. Four new columns have been added for new parameters measured during 2017-20 (DOC, Part. C, Tot. Part. N and TDP) <br><br>\n",
    " \n",
    " 3. Hours and minutes have been removed from the date-time column to create space for the new columns <br><br>\n",
    " \n",
    " 4. I have corrected various typos in the database (and in the template):\n",
    " \n",
    "     * `'Tot.part. N'` > `'Tot. Part. N'`\n",
    "     * `'Vosso(Bolstadelvi)'` > `'Vosso (Bolstadelvi)'`\n",
    "     * `'Nidelva(Tr.heim)'` > `'Nidelva (Tr.heim)'`\n",
    "     * `'More than 70%LOD'` > `'More than 70% >LOD'` (template only) \n",
    "     \n",
    "**Updated 25.08.2020**\n",
    "\n",
    "For the 2019 data, \"Målselv\" has been replaced by a new station downstream, \"Målselv v/gml E6-brua\" (see e-mail from Øyvind receievd 18.08.2020 at 08:09 for details). I have updated the templates to reflect this.\n",
    "\n",
    "\n",
    "**Updated 19.05.2022**\n",
    "\n",
    "Flow data are not available for the new reporting deadline in June. Flow has therefore been removed from the template for 2021 onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Glomma ved Sarpsfoss\n",
      "    Extracting water chemistry data...\n",
      "    Writing sample dates...\n",
      "    Deleting empty rows...\n",
      "    Writing data values...\n",
      "    Writing summary statistics...\n",
      "    Done.\n",
      "Processing: Alna\n",
      "    Extracting water chemistry data...\n",
      "    Writing sample dates...\n",
      "    Deleting empty rows...\n",
      "    Writing data values...\n",
      "    Writing summary statistics...\n",
      "    Done.\n",
      "Processing: Drammenselva\n",
      "    Extracting water chemistry data...\n",
      "    Writing sample dates...\n",
      "    Deleting empty rows...\n",
      "    Writing data values...\n",
      "    Writing summary statistics...\n",
      "    Done.\n",
      "Processing: Numedalslågen\n",
      "    Extracting water chemistry data...\n",
      "    Writing sample dates...\n",
      "    Deleting empty rows...\n",
      "    Writing data values...\n",
      "    Writing summary statistics...\n",
      "    Done.\n",
      "Processing: Skienselva\n",
      "    Extracting water chemistry data...\n",
      "    Writing sample dates...\n",
      "    Deleting empty rows...\n",
      "    Writing data values...\n",
      "    Writing summary statistics...\n",
      "    Done.\n",
      "Processing: Otra\n",
      "    Extracting water chemistry data...\n",
      "    Writing sample dates...\n",
      "    Deleting empty rows...\n",
      "    Writing data values...\n",
      "    Writing summary statistics...\n",
      "    Done.\n",
      "Processing: Orreelva\n",
      "    Extracting water chemistry data...\n",
      "    Writing sample dates...\n",
      "    Deleting empty rows...\n",
      "    Writing data values...\n",
      "    Writing summary statistics...\n",
      "    Done.\n",
      "Processing: Vosso (Bolstadelvi)\n",
      "    Extracting water chemistry data...\n",
      "    Writing sample dates...\n",
      "    Deleting empty rows...\n",
      "    Writing data values...\n",
      "    Writing summary statistics...\n",
      "    Done.\n",
      "Processing: Orkla\n",
      "    Extracting water chemistry data...\n",
      "    Writing sample dates...\n",
      "    Deleting empty rows...\n",
      "    Writing data values...\n",
      "    Writing summary statistics...\n",
      "    Done.\n",
      "Processing: Vefsna\n",
      "    Extracting water chemistry data...\n",
      "    Writing sample dates...\n",
      "    Deleting empty rows...\n",
      "    Writing data values...\n",
      "    Writing summary statistics...\n",
      "    Done.\n",
      "Processing: Altaelva\n",
      "    Extracting water chemistry data...\n",
      "    Writing sample dates...\n",
      "    Deleting empty rows...\n",
      "    Writing data values...\n",
      "    Writing summary statistics...\n",
      "    Done.\n",
      "Processing: Bjerkreimselva\n",
      "    Extracting water chemistry data...\n",
      "    Writing sample dates...\n",
      "    Deleting empty rows...\n",
      "    Writing data values...\n",
      "    Writing summary statistics...\n",
      "    Done.\n",
      "Processing: Vikedalselva\n",
      "    Extracting water chemistry data...\n",
      "    Writing sample dates...\n",
      "    Deleting empty rows...\n",
      "    Writing data values...\n",
      "    Writing summary statistics...\n",
      "    Done.\n",
      "Processing: Nausta\n",
      "    Extracting water chemistry data...\n",
      "    Writing sample dates...\n",
      "    Deleting empty rows...\n",
      "    Writing data values...\n",
      "    Writing summary statistics...\n",
      "    Done.\n",
      "Processing: Driva\n",
      "    Extracting water chemistry data...\n",
      "    Writing sample dates...\n",
      "    Deleting empty rows...\n",
      "    Writing data values...\n",
      "    Writing summary statistics...\n",
      "    Done.\n",
      "Processing: Nidelva (Tr.heim)\n",
      "    Extracting water chemistry data...\n",
      "    Writing sample dates...\n",
      "    Deleting empty rows...\n",
      "    Writing data values...\n",
      "    Writing summary statistics...\n",
      "    Done.\n",
      "Processing: Målselva v/gml E6-brua\n",
      "    Extracting water chemistry data...\n",
      "    WARNING\n",
      "    The database contains duplicated values for some station-date-parameter combinations.\n",
      "    Only the most recent values will be used, but you should check the repeated values are not errors.\n",
      "    The duplicated entries are returned in a separate dataframe.\n",
      "\n",
      "    Writing sample dates...\n",
      "    Deleting empty rows...\n",
      "    Writing data values...\n",
      "    Writing summary statistics...\n",
      "    Done.\n",
      "Processing: Tanaelva\n",
      "    Extracting water chemistry data...\n",
      "    Writing sample dates...\n",
      "    Deleting empty rows...\n",
      "    Writing data values...\n",
      "    Writing summary statistics...\n",
      "    Done.\n",
      "Processing: Pasvikelva\n",
      "    Extracting water chemistry data...\n",
      "    Writing sample dates...\n",
      "    Deleting empty rows...\n",
      "    Writing data values...\n",
      "    Writing summary statistics...\n",
      "    Done.\n",
      "Processing: Vegårdselva\n",
      "    Extracting water chemistry data...\n",
      "    Writing sample dates...\n",
      "    Deleting empty rows...\n",
      "    Writing data values...\n",
      "    Writing summary statistics...\n",
      "    Done.\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "tab_path = rid.copy_word_template(1, year)\n",
    "rid.write_word_water_chem_tables(\n",
    "    rid_20_df, year, tab_path, engine, samp_sel=63, extract_flow=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Table 2: Estimated loads at each site\n",
    "\n",
    "The code below is based on Section 3 of [notebook 5](http://nbviewer.jupyter.org/github/JamesSample/rid/blob/master/notebooks/word_data_tables.ipynb).\n",
    "\n",
    "**Updated 24/09/2018**\n",
    "\n",
    "For the 2017-20 programme, we will only report loads for the 20 \"main\" rivers, not all 155. I have therefore simplified the Word template by deleting unnecessary rows. The function itself is unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_path = rid.copy_word_template(2, year)\n",
    "loads_csv = f\"../../../Results/Loads_CSVs/loads_and_flows_all_sites_{year}.csv\"\n",
    "\n",
    "# Drop Målselv as no longer monitored in main programme\n",
    "stn_df = rid_20_df.query(\"station_name != 'Målselv'\")\n",
    "\n",
    "rid.write_word_loads_table(stn_df, loads_csv, tab_path, engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
