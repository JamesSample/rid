{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import geopandas as gpd\n",
    "import geopandas.tools\n",
    "import matplotlib.pyplot as plt\n",
    "import nivapy3 as nivapy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyproj\n",
    "import seaborn as sn\n",
    "import useful_rid_code as rid\n",
    "from shapely.geometry import Point\n",
    "from sqlalchemy import types\n",
    "\n",
    "sn.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process model input datasets (2021-22)\n",
    "\n",
    "Modelling for the RID programme makes use of the following input datasets:\n",
    "\n",
    " * **Avløp** (sewage and other drainage). These datasets are provided by **Gisle Berge at SSB** and are sub-divided into\n",
    "     * Large treatment works (\"store anlegg\"; ≥ 50 p.e.)\n",
    "     * Small treatment works (\"små anlegg\"; < 50 p.e.)\n",
    "     * Other environmental pollutants (\"miljøgifter\") <br><br>\n",
    "     \n",
    " * **Fiskeoppdret** (Fish farming). Provided by **Knut Johan Johnsen at Fiskeridirektoratet** <br><br>\n",
    " \n",
    " * **Industri** (industrial point sources). Provided by **Glenn Storbråten at Miljødirektoratet** <br><br>\n",
    " \n",
    " * **Jordbruk** (land use and management activities). Provided by **Hans Olav Eggestad at NIBIO**\n",
    " \n",
    "In addition, an annual figure for the use of **copper in aquaculture** is provided by **Preben Danielsen at Miljødirektoratet**.\n",
    " \n",
    "The raw datasets must be restructured into a standardised format and added to the RESA2 database. Once in the database, they can be used to generate input files for TEOTIL2.\n",
    "\n",
    "This notebook processes the raw data and adds it to RESA2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Username:  ········\n",
      "Password:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful.\n"
     ]
    }
   ],
   "source": [
    "# Connect to db\n",
    "engine = nivapy.da.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year of interest\n",
    "year = 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Store anlegg, Miljøgifter and Industri\n",
    "\n",
    "These three datasets are all treated similarly, and there is some duplication between the files. \n",
    "\n",
    " * The **store anlegg** dataset is in wide format. Copy and rename the file, then change the worksheet name to `store_anlegg_{year}`. The header must also be tidied (see example datasets from previous years) and blank rows at the end of the worksheet can be deleted <br><br>\n",
    " \n",
    " * The **miljøgifter** dataset is in wide format. Copy and rename the file, then change the worksheet name to `miljogifter_{year}`. Also check that column headings are the same as in previous years <br><br>\n",
    " \n",
    " * The **industri** dataset is in long format and usually contains data for multiple years. Copy and rename the file, then rename the worksheet to `industry_{year}`. Delete rows above the header and check the header is the same as in previous years. Remember to filter the data to **only include the year of interest** (i.e. delete rows for other years)\n",
    "\n",
    "The data in these files must be added to two tables in RESA2:\n",
    "\n",
    " * The site data must be added to `RESA2.RID_PUNKTKILDER`. Most of the sites should already be there, but occasionally new sites are added. Any new stations must be be assigned lat/lon co-ordinates and the appropriate \"regine\" catchment ID (the latter being most important). This usually requires geocoding plus co-ordinate conversions and/or a spatial join to determine catchment IDs. <br><br>\n",
    " \n",
    "    **Note:** As of 2017, many (>120) of the stations already in the database were missing regine IDs and many more (>3000) were missing co-ordinate information. John Rune says we have previously asked Miljødirektoratet about this, but they have not been able to provide the missing data. During data processing for 2019/20, I noticed that some of the missing co-ordinate information *was* provided in more recent data submissions. However, if a site is already in the database (with missing spatial information), it will not be updated even if co-ordinates are provided in later years. In 2020, I created a notebook ([here](https://nbviewer.org/github/JamesSample/rid/blob/master/notebooks/update_renseanlegg_coords.ipynb)) to update co-ordinates where possible based on data submitted since 2016. This has reduced the number of sites without regine IDs to 30 (`SELECT count(*) from resa2.rid_punktkilder WHERE regine IS NULL;`). I have also modified the code in this notebook so that sites are only added to `RESA2.RID_PUNKTKILDER` when co-ordinate information is available and a regine ID has been successfully assigned. This usually means that some sites in the submission for each year must be ignored, but the benefit is that if the same sites are reported with complete information at a later date, they can then be added to the database and processed correctly. This seems preferable to adding incomplete data that cannot be used. <br><br>\n",
    " \n",
    " * The chemistry data for each site must be extracted and converted to \"long\" format, then added to `RESA2.RID_PUNKTKILDER_INPAR_VALUES`. Parameter IDs etc. are taken from `RESA2.RID_PUNKTKILDER_INPAR_DEF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raw (tidied) data\n",
    "\n",
    "# Store anlegg\n",
    "in_xlsx = f\"../../../../Data/point_data_{year}/avlop_stor_anlegg_{year}_raw.xlsx\"\n",
    "stan_df = pd.read_excel(in_xlsx, sheet_name=f\"store_anlegg_{year}\")\n",
    "\n",
    "# Miljøgifter\n",
    "in_xlsx = f\"../../../../Data/point_data_{year}/avlop_miljogifter_{year}_raw.xlsx\"\n",
    "milo_df = pd.read_excel(in_xlsx, sheet_name=f\"miljogifter_{year}\")\n",
    "\n",
    "# Industri\n",
    "in_xlsx = f\"../../../../Data/point_data_{year}/industri_{year}_raw.xlsx\"\n",
    "ind_df = pd.read_excel(in_xlsx, sheet_name=f\"industry_{year}\")\n",
    "\n",
    "# Drop blank rows\n",
    "stan_df.dropna(how=\"all\", inplace=True)\n",
    "milo_df.dropna(how=\"all\", inplace=True)\n",
    "ind_df.dropna(how=\"all\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Basic data checking\n",
    "\n",
    "All of the store anlegg and miljøgifter sites are classified as `RENSEANLEGG` in the `TYPE` column of `RESA2.RID_PUNKTKILDER`; industri sites are labeled `INDUSTRI`.\n",
    "\n",
    "The code below adds `TYPE` columns, merges site data from different sources, converts UTM co-ordinates to WGS84 decimal degrees and identifies sites not already in the database. Issues identified below (e.g. missing co-ordinates) should be corrected if possible before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following 7 locations are not already in the database:\n",
      "         anlegg_nr                anlegg_navn\n",
      "932       0618AL26          Jonstølane Felt A\n",
      "604       1579AL00                     Vevang\n",
      "988       3026AL00       Haukenes Renseanlegg\n",
      "1024      3040AL01     Lyserhøgda renseanlegg\n",
      "1043      3042AL00            Muren hyttefelt\n",
      "960       3044AL00              Nedre Grøolia\n",
      "1504  4638.0091.01  SafeClean AS avd Høyanger\n",
      "\n",
      "The following 18 locations do not have co-ordinates in this year's data:\n",
      "         anlegg_nr                               anlegg_navn\n",
      "1012      0612AL11                       Onsakervika camping\n",
      "932       0618AL26                         Jonstølane Felt A\n",
      "1053      0619AL76                     Skrindehaugen H23-H21\n",
      "1094      0631AL36                    Neset Skysstasjon r.a.\n",
      "1378      0827AL07                    Russmarken Renseanlegg\n",
      "1567      1238AL19            Tolo renseanlegg - Norheimsund\n",
      "1752      1244AL04                                       Vik\n",
      "1753      1244AL05                                 Norevågen\n",
      "1754      1244AL06                                Bekkjarvik\n",
      "437       1532AL20                               GO5 - Alnes\n",
      "604       1579AL00                                    Vevang\n",
      "687       1820AL07                       Austbø avløpsanlegg\n",
      "548   2311.0001.01  Hav Line - Slakteskipet Norwegian Gannet\n",
      "988       3026AL00                      Haukenes Renseanlegg\n",
      "1024      3040AL01                    Lyserhøgda renseanlegg\n",
      "1043      3042AL00                           Muren hyttefelt\n",
      "960       3044AL00                             Nedre Grøolia\n",
      "1504  4638.0091.01                 SafeClean AS avd Høyanger\n",
      "\n",
      "The following 7 locations are not in the database and do not have co-ordinates (and therefore must be ignored):\n",
      "         anlegg_nr                anlegg_navn\n",
      "932       0618AL26          Jonstølane Felt A\n",
      "604       1579AL00                     Vevang\n",
      "988       3026AL00       Haukenes Renseanlegg\n",
      "1024      3040AL01     Lyserhøgda renseanlegg\n",
      "1043      3042AL00            Muren hyttefelt\n",
      "960       3044AL00              Nedre Grøolia\n",
      "1504  4638.0091.01  SafeClean AS avd Høyanger\n"
     ]
    }
   ],
   "source": [
    "# Add TYPE cols\n",
    "stan_df[\"TYPE\"] = \"RENSEANLEGG\"\n",
    "milo_df[\"TYPE\"] = \"RENSEANLEGG\"\n",
    "ind_df[\"TYPE\"] = \"INDUSTRI\"\n",
    "\n",
    "# Get just stn info from each df\n",
    "stan_loc = stan_df[\n",
    "    [\"ANLEGGSNR\", \"ANLEGGSNAVN\", \"Kommunenr\", \"TYPE\", \"Sone\", \"UTM_E\", \"UTM_N\"]\n",
    "].copy()\n",
    "\n",
    "milo_loc = milo_df[\n",
    "    [\"ANLEGGSNR\", \"ANLEGGSNAVN\", \"KOMMUNE_NR\", \"TYPE\", \"SONEBELTE\", \"UTMOST\", \"UTMNORD\"]\n",
    "].copy()\n",
    "\n",
    "ind_loc = ind_df[\n",
    "    [\n",
    "        \"Anleggsnr\",\n",
    "        \"Anleggsnavn\",\n",
    "        \"Komm.nr\",\n",
    "        \"TYPE\",\n",
    "        \"Geografisk Longitude\",\n",
    "        \"Geografisk Latitude\",\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "\n",
    "# Rename cols\n",
    "stan_loc.columns = [\n",
    "    \"anlegg_nr\",\n",
    "    \"anlegg_navn\",\n",
    "    \"komm_no\",\n",
    "    \"TYPE\",\n",
    "    \"zone\",\n",
    "    \"east\",\n",
    "    \"north\",\n",
    "]\n",
    "milo_loc.columns = [\n",
    "    \"anlegg_nr\",\n",
    "    \"anlegg_navn\",\n",
    "    \"komm_no\",\n",
    "    \"TYPE\",\n",
    "    \"zone\",\n",
    "    \"east\",\n",
    "    \"north\",\n",
    "]\n",
    "ind_loc.columns = [\"anlegg_nr\", \"anlegg_navn\", \"komm_no\", \"TYPE\", \"lon\", \"lat\"]\n",
    "\n",
    "# Drop duplicates\n",
    "stan_loc.drop_duplicates(inplace=True)\n",
    "milo_loc.drop_duplicates(inplace=True)\n",
    "ind_loc.drop_duplicates(inplace=True)\n",
    "\n",
    "# Convert UTM Zone col to Pandas' nullable integer data type\n",
    "# (because proj. now complains about float UTM zones)\n",
    "stan_loc[\"zone\"] = stan_loc[\"zone\"].astype(pd.Int64Dtype())\n",
    "milo_loc[\"zone\"] = milo_loc[\"zone\"].astype(pd.Int64Dtype())\n",
    "\n",
    "# Convert UTM to lat/lon\n",
    "# \"Industri\" data is already in dd\n",
    "stan_loc = nivapy.spatial.utm_to_wgs84_dd(stan_loc, \"zone\", \"east\", \"north\")\n",
    "milo_loc = nivapy.spatial.utm_to_wgs84_dd(milo_loc, \"zone\", \"east\", \"north\")\n",
    "\n",
    "# Remove UTM data\n",
    "del stan_loc[\"zone\"], stan_loc[\"east\"], stan_loc[\"north\"]\n",
    "del milo_loc[\"zone\"], milo_loc[\"east\"], milo_loc[\"north\"]\n",
    "\n",
    "# Combine into single df\n",
    "loc_df = pd.concat([stan_loc, milo_loc, ind_loc], axis=0, sort=True)\n",
    "\n",
    "# The same site can be in multiple files, so drop duplicates\n",
    "loc_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Kommune nr. should be a 4 char string, not a float\n",
    "fmt = lambda x: \"%04d\" % x\n",
    "loc_df[\"komm_no\"] = loc_df[\"komm_no\"].apply(fmt)\n",
    "\n",
    "# Check ANLEGG_NR is unique\n",
    "assert loc_df.index.duplicated().all() == False, 'Some \"ANLEGGSNRs\" are duplicated.'\n",
    "\n",
    "# Check if any sites are not already in db\n",
    "sql = \"SELECT UNIQUE(ANLEGG_NR) FROM resa2.rid_punktkilder\"\n",
    "annr_df = pd.read_sql_query(sql, engine)\n",
    "\n",
    "not_in_db = set(loc_df[\"anlegg_nr\"].values) - set(annr_df[\"anlegg_nr\"].values)\n",
    "not_in_db_df = loc_df[loc_df[\"anlegg_nr\"].isin(list(not_in_db))][\n",
    "    [\"anlegg_nr\", \"anlegg_navn\"]\n",
    "].sort_values(\"anlegg_nr\")\n",
    "no_coords_df = loc_df.query(\"(lat!=lat) or (lon!=lon)\")[\n",
    "    [\"anlegg_nr\", \"anlegg_navn\"]\n",
    "].sort_values(\"anlegg_nr\")\n",
    "not_in_db_no_coords_df = not_in_db_df[\n",
    "    not_in_db_df[\"anlegg_nr\"].isin(no_coords_df[\"anlegg_nr\"])\n",
    "].sort_values(\"anlegg_nr\")\n",
    "\n",
    "print(f\"The following {len(not_in_db_df)} locations are not already in the database:\")\n",
    "print(not_in_db_df)\n",
    "\n",
    "print(\n",
    "    f\"\\nThe following {len(no_coords_df)} locations do not have co-ordinates \"\n",
    "    \"in this year's data:\"\n",
    ")\n",
    "print(no_coords_df)\n",
    "\n",
    "print(\n",
    "    f\"\\nThe following {len(not_in_db_no_coords_df)} locations are not in the \"\n",
    "    \"database and do not have co-ordinates (and therefore must be ignored):\"\n",
    ")\n",
    "print(not_in_db_no_coords_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Identify Regine Vassdragsnummer\n",
    "\n",
    "The shapefile here:\n",
    "\n",
    "    K:/Kart/Regine_2006/RegMinsteF.shp\n",
    "\n",
    "shows locations for all the regine catchments used by TEOTIL (see e-mail from John Rune received 29/06/2017 at 17.26). I've copied this file locally here:\n",
    "\n",
    "    ../../../Data/gis/shapefiles/RegMinsteF.shp\n",
    "\n",
    "and re-projected it to WGS84 geographic co-ordinates. The new file is called `reg_minste_f_wgs84.shp`.\n",
    "\n",
    "The code cell below identifies which regine polygon each point is located in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Not all sites have complete co-ordinate information. These rows will be dropped.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:122: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  arr = construct_1d_object_array_from_listlike(values)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TYPE</th>\n",
       "      <th>anlegg_navn</th>\n",
       "      <th>anlegg_nr</th>\n",
       "      <th>komm_no</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>VASSDRAGNR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RENSEANLEGG</td>\n",
       "      <td>Bekkelaget renseanlegg med tilførselstuneller ...</td>\n",
       "      <td>0301AL01</td>\n",
       "      <td>0301</td>\n",
       "      <td>59.883941</td>\n",
       "      <td>10.770102</td>\n",
       "      <td>006.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RENSEANLEGG</td>\n",
       "      <td>Grefsenkollen renseanlegg</td>\n",
       "      <td>0301AL27</td>\n",
       "      <td>0301</td>\n",
       "      <td>59.957869</td>\n",
       "      <td>10.803865</td>\n",
       "      <td>006.A3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RENSEANLEGG</td>\n",
       "      <td>Kobberhaugshytta</td>\n",
       "      <td>0301AL30</td>\n",
       "      <td>0301</td>\n",
       "      <td>60.036105</td>\n",
       "      <td>10.663652</td>\n",
       "      <td>007.B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RENSEANLEGG</td>\n",
       "      <td>Kikutstua renseanlegg</td>\n",
       "      <td>0301AL32</td>\n",
       "      <td>0301</td>\n",
       "      <td>60.076407</td>\n",
       "      <td>10.655419</td>\n",
       "      <td>006.D22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RENSEANLEGG</td>\n",
       "      <td>Wyllerløypa renseanlegg</td>\n",
       "      <td>0301AL33</td>\n",
       "      <td>0301</td>\n",
       "      <td>60.037892</td>\n",
       "      <td>10.665771</td>\n",
       "      <td>007.B1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          TYPE                                        anlegg_navn anlegg_nr  \\\n",
       "0  RENSEANLEGG  Bekkelaget renseanlegg med tilførselstuneller ...  0301AL01   \n",
       "1  RENSEANLEGG                          Grefsenkollen renseanlegg  0301AL27   \n",
       "2  RENSEANLEGG                                   Kobberhaugshytta  0301AL30   \n",
       "3  RENSEANLEGG                              Kikutstua renseanlegg  0301AL32   \n",
       "4  RENSEANLEGG                            Wyllerløypa renseanlegg  0301AL33   \n",
       "\n",
       "  komm_no        lat        lon VASSDRAGNR  \n",
       "0    0301  59.883941  10.770102     006.21  \n",
       "1    0301  59.957869  10.803865     006.A3  \n",
       "2    0301  60.036105  10.663652     007.B1  \n",
       "3    0301  60.076407  10.655419    006.D22  \n",
       "4    0301  60.037892  10.665771     007.B1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to Regine catchment shapefile\n",
    "reg_shp_path = r\"../../../../Data/gis/shapefiles/reg_minste_f_wgs84.shp\"\n",
    "\n",
    "# Spatial join\n",
    "loc_df = nivapy.spatial.identify_point_in_polygon(\n",
    "    loc_df, reg_shp_path, \"anlegg_nr\", \"VASSDRAGNR\", \"lat\", \"lon\"\n",
    ")\n",
    "\n",
    "loc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Restructuring site data\n",
    "\n",
    "Rename columns to match RESA2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TYPE</th>\n",
       "      <th>ANLEGG_NR</th>\n",
       "      <th>ANLEGG_NAVN</th>\n",
       "      <th>KNO</th>\n",
       "      <th>REGINE</th>\n",
       "      <th>LON_UTL</th>\n",
       "      <th>LAT_UTL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [TYPE, ANLEGG_NR, ANLEGG_NAVN, KNO, REGINE, LON_UTL, LAT_UTL]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename other cols to match RESA2\n",
    "loc_df[\"ANLEGG_NR\"] = loc_df[\"anlegg_nr\"]\n",
    "loc_df[\"ANLEGG_NAVN\"] = loc_df[\"anlegg_navn\"]\n",
    "loc_df[\"KNO\"] = loc_df[\"komm_no\"]\n",
    "loc_df[\"REGINE\"] = loc_df[\"VASSDRAGNR\"]\n",
    "loc_df[\"LON_UTL\"] = loc_df[\"lon\"]\n",
    "loc_df[\"LAT_UTL\"] = loc_df[\"lat\"]\n",
    "\n",
    "del loc_df[\"anlegg_nr\"], loc_df[\"anlegg_navn\"], loc_df[\"komm_no\"]\n",
    "del loc_df[\"VASSDRAGNR\"], loc_df[\"lon\"], loc_df[\"lat\"]\n",
    "\n",
    "# Get details for sites not already in db\n",
    "loc_upld = loc_df[loc_df[\"ANLEGG_NR\"].isin(list(not_in_db))].copy()\n",
    "\n",
    "# Drop rows where 'regine' is NaN (usually because of missing co-ordinates).\n",
    "# In the past, all rows have been added, leading to sites in the database\n",
    "# without co-ordinates. These then do not get updated if co-ordinates are\n",
    "# provided in later years. It is therefore better to only add sites with\n",
    "# co-ordinates, as sites with missing data this year may be completed in\n",
    "# subsequent years\n",
    "loc_upld.dropna(subset=[\"REGINE\"], inplace=True)\n",
    "\n",
    "loc_upld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add to RESA2.RID_PUNKTKILDER\n",
    "# loc_upld.to_sql(\n",
    "#     \"rid_punktkilder\", con=engine, schema=\"resa2\", if_exists=\"append\", index=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Restructuring values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Anlegg\n",
    "# Get cols of interest\n",
    "stan_vals = stan_df[[\"ANLEGGSNR\", \"MENGDE_P_UT_kg\", \"MENGDE_N_UT_kg\"]]\n",
    "\n",
    "# In RESA2.RID_PUNKTKILDER_INPAR_DEF, N is par_id 44 and P par_id 45\n",
    "stan_vals.columns = [\"ANLEGG_NR\", 45, 44]\n",
    "\n",
    "# Melt to \"long\" format\n",
    "stan_vals = pd.melt(\n",
    "    stan_vals,\n",
    "    id_vars=\"ANLEGG_NR\",\n",
    "    value_vars=[45, 44],\n",
    "    var_name=\"INP_PAR_ID\",\n",
    "    value_name=\"VALUE\",\n",
    ")\n",
    "\n",
    "# Drop NaN values\n",
    "stan_vals.dropna(how=\"any\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As far as I can tell from exploring the 2015 data in the database, the main columns of interest for Miljøgifter are given in `milo_dict`, below, together with the corresponding parameter IDs from `RESA2.RID_PUNKTKILDER_INPAR_DEF`. This hard-coding is a bit messy, but I can't see any database table providing a nice lookup between these values, so they're included here for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miljøgifter\n",
    "# Get cols of interest\n",
    "milo_dict = {\n",
    "    \"MILJOGIFTHG2\": 16,\n",
    "    \"MILJOGIFTPAH2\": 48,\n",
    "    \"MILJOGIFTPCB2\": 30,\n",
    "    \"MILJOGIFTCD2\": 8,\n",
    "    \"MILJOGIFTDEHP2\": 119,\n",
    "    \"MILJOGIFTAS2\": 2,\n",
    "    \"MILJOGIFTCR2\": 10,\n",
    "    \"MILJOGIFTPB2\": 28,\n",
    "    \"MILJOGIFTNI2\": 25,\n",
    "    \"MILJOGIFTCU2\": 15,\n",
    "    \"MILJOGIFTZN2\": 38,\n",
    "    \"KONSMENGDTOTP10\": 45,\n",
    "    \"KONSMENGDTOTN10\": 44,\n",
    "    \"KONSMENGDSS10\": 46,\n",
    "    \"ANLEGGSNR\": \"ANLEGG_NR\",\n",
    "}  # Make headings match RESA\n",
    "\n",
    "milo_vals = milo_df[milo_dict.keys()]\n",
    "\n",
    "# Get par IDs from dict\n",
    "milo_vals.columns = [milo_dict[i] for i in milo_vals.columns]\n",
    "\n",
    "# Melt to \"long\" format\n",
    "milo_vals = pd.melt(\n",
    "    milo_vals, id_vars=\"ANLEGG_NR\", var_name=\"INP_PAR_ID\", value_name=\"VALUE\"\n",
    ")\n",
    "\n",
    "# Drop NaN values\n",
    "milo_vals.dropna(how=\"any\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The industry data is already in \"long\" format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2034/580307874.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ind_vals[\"unit\"] = ind_vals[\"unit\"].str.capitalize()\n"
     ]
    }
   ],
   "source": [
    "# Industri\n",
    "# Get cols of interest\n",
    "ind_vals = ind_df[[\"Anleggsnr\", \"Komp.kode\", \"Mengde\", \"Enhet\"]]\n",
    "ind_vals.columns = [\"anlegg_nr\", \"name\", \"value\", \"unit\"]\n",
    "\n",
    "# Get par defs from db\n",
    "# Check if any sites are not already in db\n",
    "sql = \"SELECT * \" \"FROM resa2.rid_punktkilder_inpar_def\"\n",
    "par_df = pd.read_sql_query(sql, engine)\n",
    "del par_df[\"descr\"]\n",
    "\n",
    "# Convert all units to capitals\n",
    "ind_vals[\"unit\"] = ind_vals[\"unit\"].str.capitalize()\n",
    "par_df[\"unit\"] = par_df[\"unit\"].str.capitalize()\n",
    "\n",
    "# Join\n",
    "ind_vals = pd.merge(ind_vals, par_df, how=\"left\", on=[\"name\", \"unit\"])\n",
    "\n",
    "# Some parameters that are not of interest are not matched\n",
    "# Drop these\n",
    "ind_vals.dropna(how=\"any\", inplace=True)\n",
    "\n",
    "# Get just cols of interest\n",
    "ind_vals = ind_vals[[\"anlegg_nr\", \"in_pid\", \"value\"]]\n",
    "\n",
    "# Rename for db\n",
    "ind_vals.columns = [\"ANLEGG_NR\", \"INP_PAR_ID\", \"VALUE\"]\n",
    "\n",
    "# Convert col types\n",
    "ind_vals[\"INP_PAR_ID\"] = ind_vals[\"INP_PAR_ID\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ANLEGG_NR</th>\n",
       "      <th>INP_PAR_ID</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>VALUE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0101AL01</td>\n",
       "      <td>44</td>\n",
       "      <td>2021</td>\n",
       "      <td>709.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0101AL01</td>\n",
       "      <td>45</td>\n",
       "      <td>2021</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0101AL01</td>\n",
       "      <td>46</td>\n",
       "      <td>2021</td>\n",
       "      <td>181.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0101AL06</td>\n",
       "      <td>44</td>\n",
       "      <td>2021</td>\n",
       "      <td>454.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0101AL06</td>\n",
       "      <td>45</td>\n",
       "      <td>2021</td>\n",
       "      <td>18.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ANLEGG_NR  INP_PAR_ID  YEAR   VALUE\n",
       "0  0101AL01          44  2021  709.20\n",
       "1  0101AL01          45  2021    0.38\n",
       "2  0101AL01          46  2021  181.69\n",
       "3  0101AL06          44  2021  454.98\n",
       "4  0101AL06          45  2021   18.92"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine\n",
    "val_df = pd.concat([stan_vals, milo_vals, ind_vals], axis=0, sort=True)\n",
    "\n",
    "# Add column for year\n",
    "val_df[\"YEAR\"] = year\n",
    "\n",
    "# Explicitly set data types\n",
    "val_df[\"ANLEGG_NR\"] = val_df[\"ANLEGG_NR\"].astype(str)\n",
    "val_df[\"INP_PAR_ID\"] = val_df[\"INP_PAR_ID\"].astype(int)\n",
    "val_df[\"VALUE\"] = val_df[\"VALUE\"].astype(float)\n",
    "val_df[\"YEAR\"] = val_df[\"YEAR\"].astype(int)\n",
    "\n",
    "# Store Anlegg and Miljøgifter contain some duplicated information\n",
    "val_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Average any remaining duplciates (because sometimes the same value is reported with different precision)\n",
    "val_df = val_df.groupby([\"ANLEGG_NR\", \"INP_PAR_ID\", \"YEAR\"]).mean().reset_index()\n",
    "\n",
    "# Only add data for locations in the database\n",
    "sql = \"SELECT UNIQUE(anlegg_nr) FROM resa2.rid_punktkilder\"\n",
    "anleg_nrs = pd.read_sql(sql, engine)[\"anlegg_nr\"].to_list()\n",
    "val_df = val_df.query(\"ANLEGG_NR in @anleg_nrs\")\n",
    "\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7748"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Drop any existing values for this year\n",
    "# sql = f\"DELETE FROM resa2.rid_punktkilder_inpar_values WHERE year = {year}\"\n",
    "# res = engine.execute(sql)\n",
    "\n",
    "# # Add to RESA2.RID_PUNKTKILDER_INPAR_VALUES\n",
    "# val_df.to_sql(\n",
    "#     \"rid_punktkilder_inpar_values\",\n",
    "#     con=engine,\n",
    "#     schema=\"resa2\",\n",
    "#     if_exists=\"append\",\n",
    "#     index=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Små anlegg (small treatment works)\n",
    "\n",
    "Copy and rename the file, and rename the worksheet `sma_anlegg_{year}`. Delete rows above the header and delete unnecessary columns. The only columns required are `KOMMUNENR`, `SUM FOSFOR` and `SUM NITROGEN`, which should be renamed `KOMMUNENR`, `P_kg` and `N_kg`, respectively.\n",
    "\n",
    "This data is added directly to `RESA2.RID_KILDER_SPREDT_VALUES`. \n",
    "\n",
    "**Note:** The kommuner ID numbers in the små anlegg file should be present in \n",
    "\n",
    "    ../../../teotil2/data/core_input_data/regine_{year}.csv\n",
    "    \n",
    "Kommune IDs change from year to year, so they will usually need updating in TEOTIL - see [update_regine_kommune.ipynb](https://nbviewer.org/github/JamesSample/rid/blob/master/notebooks/update_regine_kommune.ipynb) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KOMM_NO</th>\n",
       "      <th>INP_PAR_ID</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>AR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0301</td>\n",
       "      <td>45</td>\n",
       "      <td>211.061250</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1101</td>\n",
       "      <td>45</td>\n",
       "      <td>830.907900</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1103</td>\n",
       "      <td>45</td>\n",
       "      <td>3219.234300</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1106</td>\n",
       "      <td>45</td>\n",
       "      <td>190.743525</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1108</td>\n",
       "      <td>45</td>\n",
       "      <td>1334.744295</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  KOMM_NO INP_PAR_ID        VALUE    AR\n",
       "0    0301         45   211.061250  2021\n",
       "1    1101         45   830.907900  2021\n",
       "2    1103         45  3219.234300  2021\n",
       "3    1106         45   190.743525  2021\n",
       "4    1108         45  1334.744295  2021"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read raw (tidied) data\n",
    "in_xlsx = f\"../../../../Data/point_data_{year}/avlop_sma_anlegg_{year}_raw.xlsx\"\n",
    "sman_df = pd.read_excel(in_xlsx, sheet_name=f\"sma_anlegg_{year}\")\n",
    "\n",
    "# Drop blank rows\n",
    "sman_df.dropna(how=\"all\", inplace=True)\n",
    "\n",
    "# Kommune nr. should be a 4 char string, not a float\n",
    "fmt = lambda x: \"%04d\" % x\n",
    "sman_df[\"KOMMUNENR\"] = sman_df[\"KOMMUNENR\"].apply(fmt)\n",
    "\n",
    "# Check if any kommuner are not already in TEOTIL\n",
    "reg_csv = f\"../../../../teotil2/data/core_input_data/regine_{year}.csv\"\n",
    "kmnr_df = pd.read_csv(reg_csv, sep=\";\", encoding=\"utf-8\")\n",
    "kmnr_df[\"komnr\"] = kmnr_df[\"komnr\"].apply(fmt)\n",
    "\n",
    "not_in_db = set(sman_df[\"KOMMUNENR\"].values) - set(kmnr_df[\"komnr\"].values)\n",
    "if len(not_in_db) > 0:\n",
    "    print(\n",
    "        f'\\nThe following {len(not_in_db)} kommuner are not in the TEOTIL \"regine\" file. Consider updating?:'\n",
    "    )\n",
    "    print(sman_df[sman_df[\"KOMMUNENR\"].isin(list(not_in_db))])\n",
    "\n",
    "# Get cols of interest for RID_KILDER_SPREDT_VALUES\n",
    "sman_df = sman_df[[\"KOMMUNENR\", \"P_kg\", \"N_kg\"]]\n",
    "\n",
    "# In RESA2.RID_PUNKTKILDER_INPAR_DEF, N is par_id 44 and P par_id 45\n",
    "sman_df.columns = [\"KOMM_NO\", 45, 44]\n",
    "\n",
    "# Melt to \"long\" format\n",
    "sman_df = pd.melt(\n",
    "    sman_df,\n",
    "    id_vars=\"KOMM_NO\",\n",
    "    value_vars=[45, 44],\n",
    "    var_name=\"INP_PAR_ID\",\n",
    "    value_name=\"VALUE\",\n",
    ")\n",
    "\n",
    "# Add column for year\n",
    "sman_df[\"AR\"] = year\n",
    "\n",
    "sman_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "712"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Drop any existing values for this year\n",
    "# sql = f\"DELETE FROM resa2.rid_kilder_spredt_values WHERE ar = {year}\"\n",
    "# res = engine.execute(sql)\n",
    "\n",
    "# # Add to RESA2.RID_KILDER_SPREDT_VALUES\n",
    "# sman_df.to_sql(\n",
    "#     \"rid_kilder_spredt_values\",\n",
    "#     con=engine,\n",
    "#     schema=\"resa2\",\n",
    "#     if_exists=\"append\",\n",
    "#     index=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fish farms\n",
    "\n",
    "The aquaculture data is usually encrypted and must be stored securely. Copy and rename the file, and change the worksheet name to `fiskeoppdrett_{year}`. Check that data have only been provided for **one year** and that the column names match submissions from previous years.\n",
    "\n",
    "These data must be added to two tables in RESA2:\n",
    "\n",
    " * First, the site data must be added to `RESA2.RID_KILDER_AQUAKULTUR`. Most of the sites should already be there, but occasionally new sites are added. Any new stations must be be assigned lat/lon co-ordinates and the appropriate \"regine\" catchment ID. This usually requires geocoding plus co-ordinate conversions and/or a spatial join to determine catchment IDs.\n",
    " \n",
    "    **Note:** The key ID fields in the raw data appear to be `LOKNR` and `LOKNAVN`. <br><br>\n",
    " \n",
    " * Secondly, the chemistry data for each site must be extracted and converted to \"long\" format, then added to `RESA2.RID_KILDER_AQKULT_VALUES`. Parameter IDs etc. are taken from `RESA2.RID_PUNKTKILDER_INPAR_DEF`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Basic data checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raw (tidied) data\n",
    "# Fish farms\n",
    "in_xlsx = f\"../../../../Data/point_data_{year}/fiske_oppdret_{year}_raw.xlsx\"\n",
    "fish_df = pd.read_excel(in_xlsx, sheet_name=f\"fiskeoppdrett_{year}\")\n",
    "\n",
    "# Drop no data\n",
    "fish_df.dropna(how=\"all\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any sites are not already in db\n",
    "sql = \"SELECT UNIQUE(NR) FROM resa2.rid_kilder_aquakultur\"\n",
    "aqua_df = pd.read_sql_query(sql, engine)\n",
    "\n",
    "not_in_db = set(fish_df[\"LOKNR\"].values) - set(aqua_df[\"nr\"].values)\n",
    "nidb_df = fish_df[fish_df[\"LOKNR\"].isin(list(not_in_db))][\n",
    "    [\"LOKNR\", \"LOKNAVN\", \"N_DESIMALGRADER_Y\", \"O_DESIMALGRADER_X\"]\n",
    "].drop_duplicates(subset=[\"LOKNR\"])\n",
    "if len(not_in_db) > 0:\n",
    "    print(f\"The following {len(not_in_db)} locations are not in the database:\")\n",
    "    print(nidb_df)\n",
    "\n",
    "# Check for missing co-ords\n",
    "no_coords_df = fish_df.query(\n",
    "    \"(N_DESIMALGRADER_Y!=N_DESIMALGRADER_Y) or \"\n",
    "    \"(O_DESIMALGRADER_X!=O_DESIMALGRADER_X)\"\n",
    ")[[\"LOKNR\", \"LOKNAVN\"]].sort_values(\"LOKNR\")\n",
    "if len(no_coords_df) > 0:\n",
    "    print(\n",
    "        f\"\\nThe following {len(no_coords_df)} locations do not have co-ordinates \"\n",
    "        \"in this year's data:\"\n",
    "    )\n",
    "    print(no_coords_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Geocode fish farms and add to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to Regine catchment shapefile\n",
    "reg_shp_path = r\"../../../../Data/gis/shapefiles/reg_minste_f_wgs84.shp\"\n",
    "\n",
    "# Spatial join\n",
    "if len(nidb_df) > 0:\n",
    "    loc_df = nivapy.spatial.identify_point_in_polygon(\n",
    "        nidb_df,\n",
    "        reg_shp_path,\n",
    "        \"LOKNR\",\n",
    "        \"VASSDRAGNR\",\n",
    "        \"N_DESIMALGRADER_Y\",\n",
    "        \"O_DESIMALGRADER_X\",\n",
    "    )\n",
    "\n",
    "    # Rename cols\n",
    "    loc_df.columns = [\"NR\", \"NAVN\", \"LENGDE\", \"BREDDE\", \"REGINE\"]\n",
    "\n",
    "    # Drop rows where 'REGINE' is NaN\n",
    "    no_reg = pd.isna(loc_df[\"REGINE\"])\n",
    "    if no_reg.sum() > 0:\n",
    "        no_reg_df = loc_df[no_reg]\n",
    "        print(\n",
    "            f\"The following {len(no_reg_df)} locations cannot be linked to a regine. \"\n",
    "            \"These sites will be ignored.\"\n",
    "        )\n",
    "        print(no_reg_df)\n",
    "\n",
    "        loc_df.dropna(subset=[\"REGINE\"], inplace=True)\n",
    "\n",
    "    print(f\"The following {len(loc_df)} locations will be added to the database.\")\n",
    "    print(loc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add to RESA2.RID_KILDER_AQUAKULTUR\n",
    "# loc_df.to_sql(\n",
    "#     \"rid_kilder_aquakultur\", con=engine, schema=\"resa2\", if_exists=\"append\", index=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Estimate nutrient and copper inputs\n",
    "\n",
    "The methodology here is a little unclear. The following is my best guess, based on the files located here:\n",
    "\n",
    "    K:\\Avdeling\\Vass\\316_Miljøinformatikk\\Prosjekter\\RID\\2016\\Rådata\\Fiskeoppdrett\n",
    "\n",
    "Old workflow:\n",
    "\n",
    " 1. Calculate the fish biomass from the raw data. See the equation in the `Biomasse` column of the spreadsheet *JSE_TEOTIL_2015.xlsx* <br><br>\n",
    " \n",
    " 2. Split the data according to salmon (\"laks\"; species ID 71101) and trout (\"øret\"; species ID 71401), then group by location and month, summing biomass and `FORFORBRUK_KILO` columns (see Fiskeoppdrett_biomasse_2016.accdb) <br><br>\n",
    " \n",
    " 3. Calculate production. This involves combining biomass for the current month with that for the previous month. See the calculations in e.g. *N_P_ørret_2015.xlsx*. <br><br>\n",
    " \n",
    " 4. Calculate NTAP and PTAP. **NB:** I don't know what these quantities are, so I'm just duplicating the Excel calculations in the code below. The functions are therefore not very well explained <br><br>\n",
    " \n",
    " 5. Estimate copper usage at each fish farm by scaling the total annual Cu usage in proportion to P production\n",
    " \n",
    "The annual copper figure is provided by Miljødirektoratet and it is assumed that 85% of the total is lost to water. Values for each year are stored in \n",
    "\n",
    "    ../../../Data/annual_copper_useage_aquaculture.xlsx\n",
    "    \n",
    "**This file should be updated with the latest figure before running the code below**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total annual copper lost to water from aquaculture is 1308.1 tonnes.\n"
     ]
    }
   ],
   "source": [
    "# Get annual copper usgae\n",
    "cu_xlsx = r\"../../../../Data/annual_copper_usage_aquaculture.xlsx\"\n",
    "cu_df = pd.read_excel(cu_xlsx, sheet_name=\"Sheet1\", index_col=0)\n",
    "tot_an_cu = cu_df.loc[year, \"tot_cu_tonnes\"]\n",
    "an_cu = 0.85 * tot_an_cu\n",
    "print(f\"The total annual copper lost to water from aquaculture is {an_cu:.1f} tonnes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ANLEGG_NR</th>\n",
       "      <th>INP_PAR_ID</th>\n",
       "      <th>AR</th>\n",
       "      <th>MANED</th>\n",
       "      <th>ART</th>\n",
       "      <th>VALUE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10029</td>\n",
       "      <td>39</td>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5354.154754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10041</td>\n",
       "      <td>39</td>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12420.154769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10050</td>\n",
       "      <td>39</td>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12225.427231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10054</td>\n",
       "      <td>39</td>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49565.907688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10080</td>\n",
       "      <td>39</td>\n",
       "      <td>2021</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70920.013671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ANLEGG_NR INP_PAR_ID    AR  MANED  ART         VALUE\n",
       "0      10029         39  2021      6  NaN   5354.154754\n",
       "1      10041         39  2021      6  NaN  12420.154769\n",
       "2      10050         39  2021      6  NaN  12225.427231\n",
       "3      10054         39  2021      6  NaN  49565.907688\n",
       "4      10080         39  2021      6  NaN  70920.013671"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estimate nutrient inputs from fish farns\n",
    "fish_nut = rid.estimate_fish_farm_nutrient_inputs(fish_df, year, an_cu)\n",
    "fish_nut.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2646"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Drop any existing values for this year\n",
    "# sql = f\"DELETE FROM resa2.rid_kilder_aqkult_values WHERE ar = {year}\"\n",
    "# res = engine.execute(sql)\n",
    "#\n",
    "# # Add to RESA2.RID_KILDER_AQKULT_VALUES\n",
    "# fish_nut.to_sql(\n",
    "#     \"rid_kilder_aqkult_values\",\n",
    "#     con=engine,\n",
    "#     schema=\"resa2\",\n",
    "#     if_exists=\"append\",\n",
    "#     index=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Land use\n",
    "\n",
    "The land use dataset is provided by NIBIO. The file usually gives errors when it is opened, but it can be saved again as a `.xlsx` without problems. \n",
    "\n",
    "Open the file and `Save as`, then rename the worksheet to `jordbruk_{year}`. Tidy the header to match previous submissions and correct any issues with Norwegian characters in the `omrade` column. \n",
    "\n",
    "The entry for Oslo (`osl1`; fylke_sone = 3_1) is usually missing from the data provided by NIBIO. This row should be added manually to the Excel file and the values should be set identical to those for område `ake2`. This works because the land areas in TEOTIL's `fysone_land_areas.csv` have been made identical for `osl1` and `ake2` (even though this is not correct), so the inputs in terms of kg/km2 are calculated as being the same for both regions, which is what is required.\n",
    " \n",
    "These data are added to the table `RESA2.RID_AGRI_INPUTS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>omrade</th>\n",
       "      <th>year</th>\n",
       "      <th>n_diff_kg</th>\n",
       "      <th>n_point_kg</th>\n",
       "      <th>n_back_kg</th>\n",
       "      <th>p_diff_kg</th>\n",
       "      <th>p_point_kg</th>\n",
       "      <th>p_back_kg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>øst1</td>\n",
       "      <td>2021</td>\n",
       "      <td>739312</td>\n",
       "      <td>8338</td>\n",
       "      <td>157590</td>\n",
       "      <td>50055</td>\n",
       "      <td>674</td>\n",
       "      <td>3563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>øst2</td>\n",
       "      <td>2021</td>\n",
       "      <td>1160956</td>\n",
       "      <td>5451</td>\n",
       "      <td>194922</td>\n",
       "      <td>25004</td>\n",
       "      <td>457</td>\n",
       "      <td>3422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>øst3</td>\n",
       "      <td>2021</td>\n",
       "      <td>223248</td>\n",
       "      <td>1048</td>\n",
       "      <td>39159</td>\n",
       "      <td>6990</td>\n",
       "      <td>90</td>\n",
       "      <td>759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ake1</td>\n",
       "      <td>2021</td>\n",
       "      <td>1608982</td>\n",
       "      <td>9164</td>\n",
       "      <td>237631</td>\n",
       "      <td>81335</td>\n",
       "      <td>1470</td>\n",
       "      <td>5554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ake2</td>\n",
       "      <td>2021</td>\n",
       "      <td>757050</td>\n",
       "      <td>1905</td>\n",
       "      <td>117703</td>\n",
       "      <td>29993</td>\n",
       "      <td>245</td>\n",
       "      <td>2221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  omrade  year  n_diff_kg  n_point_kg  n_back_kg  p_diff_kg  p_point_kg  \\\n",
       "0   øst1  2021     739312        8338     157590      50055         674   \n",
       "1   øst2  2021    1160956        5451     194922      25004         457   \n",
       "2   øst3  2021     223248        1048      39159       6990          90   \n",
       "3   ake1  2021    1608982        9164     237631      81335        1470   \n",
       "4   ake2  2021     757050        1905     117703      29993         245   \n",
       "\n",
       "   p_back_kg  \n",
       "0       3563  \n",
       "1       3422  \n",
       "2        759  \n",
       "3       5554  \n",
       "4       2221  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to (tidied) Bioforsk data\n",
    "lu_xlsx = f\"../../../../Data/point_data_{year}/jordbruk_{year}.xlsx\"\n",
    "lu_df = pd.read_excel(lu_xlsx)\n",
    "\n",
    "lu_df[\"year\"] = year\n",
    "\n",
    "# Order cols\n",
    "lu_df = lu_df[\n",
    "    [\n",
    "        \"omrade\",\n",
    "        \"year\",\n",
    "        \"n_diff_kg\",\n",
    "        \"n_point_kg\",\n",
    "        \"n_back_kg\",\n",
    "        \"p_diff_kg\",\n",
    "        \"p_point_kg\",\n",
    "        \"p_back_kg\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "lu_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Drop any existing values for this year\n",
    "# sql = f\"DELETE FROM resa2.rid_agri_inputs WHERE year = {year}\"\n",
    "# res = engine.execute(sql)\n",
    "\n",
    "# # Write to RESA\n",
    "# lu_df.to_sql(\n",
    "#     name=\"rid_agri_inputs\",\n",
    "#     con=engine,\n",
    "#     schema=\"resa2\",\n",
    "#     index=False,\n",
    "#     if_exists=\"append\",\n",
    "#     dtype={\"omrade\": types.VARCHAR(lu_df[\"omrade\"].str.len().max())},\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
