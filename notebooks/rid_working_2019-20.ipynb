{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "import nivapy3 as nivapy\n",
    "import useful_rid_code as rid\n",
    "\n",
    "sn.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Username:  ···\n",
      "Password:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful.\n"
     ]
    }
   ],
   "source": [
    "# Connect to db\n",
    "engine = nivapy.da.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RID 2019-20: data processing notebook\n",
    "\n",
    "Previous notebooks have developed the tools and methodology required to implement the RID workflow. Note that the programme for 2017 - 2020 requires some changes, which are described [here](http://nbviewer.jupyter.org/github/JamesSample/rid/blob/master/notebooks/programme_changes_2017-18.ipynb). In particular, we now have 20 \"core\" sites and 135 \"other\" sites, instead of the previous 11/36/108 split.\n",
    "\n",
    "This notebook keeps a record of the processing for the 2019 data.\n",
    "\n",
    "## 1. Add 2019 datasets\n",
    "\n",
    "### 1.1. Update flow datasets\n",
    "\n",
    "[This notebook](http://nbviewer.jupyter.org/github/JamesSample/rid/blob/master/notebooks/update_flow_datasets.ipynb) documents the processing and upload of the NVE flow datasets (both modelled and observed) for each year since 2016. The RESA2 database now contains a complete record of all the discharge data required for this year's RID processing.\n",
    "\n",
    "### 1.2. Water chemistry quality control\n",
    "\n",
    "The water samples collected for this project are analysed by the NIVA laboratory and results are automatically transferred to the RESA2 database. Liv Bente has quality-checked the 2019 data (see e-mail received 02.07.2020 at 15:45) and the necessary corrections have been made in the database. \n",
    "\n",
    "### 1.3. Sample selections\n",
    "\n",
    "The analysis for the RID report should only use water samples collected as part of the \"core\" monitoring programme (i.e. not flood samples or those collected under Option 3). In 2019, **no flood samples were taken**, but the **Option 3 samples need to be separated from the \"core\" sampling**.\n",
    "\n",
    "Liv Bente has a spreadsheet here:\n",
    "\n",
    "    K:\\Prosjekter\\Ferskvann\\16384 Elveovervåkingsprogrammet\\2019\\4. Data\\4. Kvalitetsikring\\4. Juni20_filer til opprett JES\\RESA-data_opsj 3 2019_29jun20.xlsx\n",
    "    \n",
    "that lists all the Option 3 samples in 2019. The code below add these to `SAMPLE_SELECTION` 65."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read new site groupings (for 2017 to 2020)\n",
    "in_xlsx = r\"../../../Data/RID_Sites_List_2017-2020.xlsx\"\n",
    "rid_20_df = pd.read_excel(in_xlsx, sheet_name=\"RID_20\")\n",
    "rid_135_df = pd.read_excel(in_xlsx, sheet_name=\"RID_135\")\n",
    "rid_155_df = pd.read_excel(in_xlsx, sheet_name=\"RID_All\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get WS IDs for Option 3\n",
    "op3_xlsx = r\"../../../Data/option_3/option_3_2019.xlsx\"\n",
    "op3_df = pd.read_excel(op3_xlsx, sheet_name=\"all_op3_samples_2019\")\n",
    "\n",
    "ws_ids = []\n",
    "for idx, row in op3_df.iterrows():\n",
    "    sql = (\n",
    "        \"SELECT water_sample_id FROM resa2.water_samples \"\n",
    "        \"WHERE station_id = %s \"\n",
    "        \"AND TRUNC(sample_date) = DATE '%s' \"\n",
    "        \"AND depth1 = %s \"\n",
    "        \"AND depth2 = %s\"\n",
    "        % (\n",
    "            row[\"station_id\"],\n",
    "            row[\"sample_date\"].strftime(\"%Y-%m-%d\"),\n",
    "            row[\"depth1\"],\n",
    "            row[\"depth2\"],\n",
    "        )\n",
    "    )\n",
    "    ws_id = engine.execute(sql).fetchall()[0]\n",
    "    assert len(ws_id) == 1\n",
    "    ws_id = ws_id[0]\n",
    "    ws_ids.append(ws_id)\n",
    "\n",
    "ws_df = pd.DataFrame({\"water_sample_id\": ws_ids,})\n",
    "ws_df[\"sample_selection_id\"] = 65\n",
    "\n",
    "assert len(op3_df) == len(ws_df)\n",
    "\n",
    "#ws_df.to_sql('sample_selections', con=engine, schema='resa2',\n",
    "#             if_exists='append', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get flood and Option 3 samples\n",
    "sql = (\n",
    "    \"SELECT water_sample_id \"\n",
    "    \"FROM resa2.sample_selections \"\n",
    "    \"WHERE sample_selection_id IN (64, 65)\"\n",
    ")\n",
    "oth_ws = pd.read_sql_query(sql, engine)\n",
    "assert oth_ws[\"water_sample_id\"].is_unique\n",
    "\n",
    "# Get all WS associated with core sites for 2019\n",
    "sql = (\n",
    "    \"SELECT water_sample_id FROM resa2.water_samples \"\n",
    "    \"WHERE station_id IN %s \"\n",
    "    \"AND sample_date >= DATE '2019-01-01' \"\n",
    "    \"AND sample_date < DATE '2020-01-01'\"\n",
    "    % str(tuple(rid_155_df[\"station_id\"].astype(int)))\n",
    ")\n",
    "all_ws = pd.read_sql_query(sql, engine)\n",
    "assert all_ws[\"water_sample_id\"].is_unique\n",
    "\n",
    "# Remove flood and option 3 samples from core\n",
    "core_ws = set(all_ws[\"water_sample_id\"]) - set(oth_ws[\"water_sample_id\"])\n",
    "\n",
    "# Add to sample selections\n",
    "core_df = pd.DataFrame({\"water_sample_id\": list(core_ws)})\n",
    "core_df[\"sample_selection_id\"] = 63\n",
    "\n",
    "#core_df.to_sql('sample_selections', con=engine, schema='resa2',\n",
    "#              if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tabulate raw water chemistry and flow\n",
    "\n",
    "### 2.1. Data for year of interest\n",
    "\n",
    "From 2017 onwards, water chemistry samples have been collected at 20 sites (`RID_20`). Note that for this year (2019 data) the station TROEMÅL2 has been added to the RID_20 selection. This makes 21 stations in total. Both TROEMÅL and TROEMÅL2 were monitored in 2018, but from 2019 onwards TROEMÅL2 has replaced TROEMÅL.\n",
    "\n",
    "The data are exported to CSV format below.\n",
    "\n",
    "**Remember to update the year!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year of interest\n",
    "year = 2019\n",
    "\n",
    "# Output CSV\n",
    "out_csv = r\"../../../Results/Loads_CSVs/concs_and_flows_rid_20_%s.csv\" % year\n",
    "\n",
    "df = rid.write_csv_water_chem(rid_20_df, year, out_csv, engine, samp_sel=63)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Data for all years\n",
    "\n",
    "**Added 03.09.2018**. In 2017, we wanted to estimate trends in the \"main\" rivers for both loads and concentrations for the period from 1990 to 2017. Because of the changes to the programme in 2017, Eva needed concentrations tabulating for all years, not just 2017 - see e-mail from Øyvind received 31.08.2018 at 14.04. \n",
    "\n",
    "This may not be necessary after 2017, but I ran the code in 2018 and will run it here for 2019 too, as the output may be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year of interest\n",
    "end_year = 2019\n",
    "\n",
    "# Container for data\n",
    "df_list = []\n",
    "\n",
    "# Dummy path for intermediate output (which isn't needed here)\n",
    "out_csv = r\"../../../Results/Loads_CSVs/cons_and_flows_intermed.csv\"\n",
    "\n",
    "# Loop over years\n",
    "for year in range(1990, end_year + 1):\n",
    "    # Get data\n",
    "    df = rid.write_csv_water_chem(rid_20_df, year, out_csv, engine, samp_sel=63)\n",
    "\n",
    "    # Add to output\n",
    "    df_list.append(df)\n",
    "\n",
    "# Delete intermediate\n",
    "os.remove(out_csv)\n",
    "\n",
    "# Combine\n",
    "df = pd.concat(df_list, axis=0)\n",
    "\n",
    "# Reorder cols and tidy\n",
    "st_cols = [\n",
    "    \"station_id\",\n",
    "    \"station_code\",\n",
    "    \"station_name\",\n",
    "    \"old_rid_group\",\n",
    "    \"new_rid_group\",\n",
    "    \"ospar_region\",\n",
    "    \"sample_date\",\n",
    "    \"Qs_m3/s\",\n",
    "]\n",
    "par_cols = [i for i in df.columns if i not in st_cols]\n",
    "par_cols.sort()\n",
    "df = df[st_cols + par_cols]\n",
    "\n",
    "# Output CSV\n",
    "out_csv = r\"../../../Results/Loads_CSVs/concs_and_flows_rid_20_1990-%s.csv\" % end_year\n",
    "df.to_csv(out_csv, encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Estimate observed loads\n",
    "\n",
    "### 3.1. Annual flows\n",
    "\n",
    "First get a dataframe of annual flow volumes to join to the summary output. **NB:** This dataframe isn't actually used in the loads calculations - they are handled separately - it's just for the output CSVs.\n",
    "\n",
    "**Remember to update the years!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sites of interest: combine all site dfs into one\n",
    "rid_all_df = pd.concat([rid_20_df, rid_135_df], axis=0)\n",
    "\n",
    "# Get flow data\n",
    "q_df = rid.get_flow_volumes(rid_all_df, 1990, 2019, engine)\n",
    "\n",
    "q_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Loads for all rivers\n",
    "\n",
    "The code below is taken from Section 2 of [notebook 3](http://nbviewer.jupyter.org/github/JamesSample/rid/blob/master/notebooks/estimate_loads.ipynb), but this time run using the 2019 data. Loads are calculated directly from contemporary observations for the RID_20, and they are inferred from historic concentrations for the RID_135 sites.\n",
    "\n",
    "As above, note the use of the `'samp_sel'` argument in the code below.\n",
    "\n",
    "**Remember to change the year below!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sites of interest: combine all site dfs into one\n",
    "rid_all_df = pd.concat([rid_20_df, rid_135_df], axis=0)\n",
    "\n",
    "# Pars of interest\n",
    "par_list = [\n",
    "    \"SPM\",\n",
    "    \"TOC\",\n",
    "    \"PO4-P\",\n",
    "    \"TOTP\",\n",
    "    \"NO3-N\",\n",
    "    \"NH4-N\",\n",
    "    \"TOTN\",\n",
    "    \"SiO2\",\n",
    "    \"Ag\",\n",
    "    \"As\",\n",
    "    \"Pb\",\n",
    "    \"Cd\",\n",
    "    \"Cu\",\n",
    "    \"Zn\",\n",
    "    \"Ni\",\n",
    "    \"Cr\",\n",
    "    \"Hg\",\n",
    "]\n",
    "\n",
    "# Year of interest\n",
    "year = 2019\n",
    "\n",
    "# Container for results from each site\n",
    "loads_list = []\n",
    "\n",
    "# Loop over sites\n",
    "for stn_id in rid_all_df[\"station_id\"].values:\n",
    "    # Estimate loads at this site\n",
    "    loads_list.append(\n",
    "        rid.estimate_loads(\n",
    "            stn_id, par_list, year, engine, infer_missing=True, samp_sel=63\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Concatenate to new df\n",
    "lds_all = pd.concat(loads_list, axis=0)\n",
    "lds_all.index.name = \"station_id\"\n",
    "lds_all.reset_index(inplace=True)\n",
    "\n",
    "# Get flow data for year\n",
    "q_yr = q_df.query(\"year == @year\")\n",
    "\n",
    "# Join\n",
    "lds_all = pd.merge(lds_all, rid_all_df, how=\"left\", on=\"station_id\")\n",
    "lds_all = pd.merge(lds_all, q_yr, how=\"left\", on=\"station_id\")\n",
    "\n",
    "# Reorder cols and tidy\n",
    "st_cols = [\n",
    "    \"station_id\",\n",
    "    \"station_code\",\n",
    "    \"station_name\",\n",
    "    \"old_rid_group\",\n",
    "    \"new_rid_group\",\n",
    "    \"ospar_region\",\n",
    "    \"mean_q_1000m3/day\",\n",
    "]\n",
    "unwant_cols = [\n",
    "    \"nve_vassdrag_nr\",\n",
    "    \"lat\",\n",
    "    \"lon\",\n",
    "    \"utm_north\",\n",
    "    \"utm_east\",\n",
    "    \"utm_zone\",\n",
    "    \"station_type\",\n",
    "    \"year\",\n",
    "]\n",
    "par_cols = [i for i in lds_all.columns if i not in (st_cols + unwant_cols)]\n",
    "\n",
    "for col in unwant_cols:\n",
    "    del lds_all[col]\n",
    "\n",
    "lds_all = lds_all[st_cols + par_cols]\n",
    "\n",
    "# Write output\n",
    "out_csv = r\"../../../Results/Loads_CSVs/loads_and_flows_all_sites_%s.csv\" % year\n",
    "lds_all.to_csv(out_csv, encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Loads for the RID_20 rivers through time\n",
    "\n",
    "The code below is taken from Section 3 of [notebook 3](http://nbviewer.jupyter.org/github/JamesSample/rid/blob/master/notebooks/estimate_loads.ipynb), but this time the bar charts include data from 2019 for the RID_20 stations.\n",
    "\n",
    "Note the use of the `'samp_sel'` argument in the code below.\n",
    "\n",
    "**Remember to change the years below!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Period of interest\n",
    "st_yr, end_yr = 1990, 2019\n",
    "\n",
    "# Container for results\n",
    "loads_list = []\n",
    "\n",
    "# Loop over sites\n",
    "for stn_id in rid_20_df[\"station_id\"].values:\n",
    "    # Loop over years\n",
    "    for year in range(st_yr, end_yr + 1):\n",
    "        print(\"Processing Station ID %s for %s\" % (stn_id, year))\n",
    "\n",
    "        # Get loads\n",
    "        l_df = rid.estimate_loads(\n",
    "            stn_id, par_list, year, engine, infer_missing=True, samp_sel=63\n",
    "        )\n",
    "\n",
    "        if l_df is not None:\n",
    "            # Name and reset index\n",
    "            l_df.index.name = \"station_id\"\n",
    "            l_df.reset_index(inplace=True)\n",
    "\n",
    "            # Add year\n",
    "            l_df[\"year\"] = year\n",
    "\n",
    "            # Add to outout\n",
    "            loads_list.append(l_df)\n",
    "\n",
    "# Concatenate to new df\n",
    "lds_ts = pd.concat(loads_list, axis=0)\n",
    "\n",
    "# Join\n",
    "lds_q_ts = pd.merge(lds_ts, rid_20_df, how=\"left\", on=\"station_id\")\n",
    "lds_q_ts = pd.merge(lds_q_ts, q_df, how=\"left\", on=[\"station_id\", \"year\"])\n",
    "\n",
    "# Reorder cols and tidy\n",
    "st_cols = [\n",
    "    \"station_id\",\n",
    "    \"station_code\",\n",
    "    \"station_name\",\n",
    "    \"old_rid_group\",\n",
    "    \"new_rid_group\",\n",
    "    \"ospar_region\",\n",
    "    \"mean_q_1000m3/day\",\n",
    "]\n",
    "unwant_cols = [\n",
    "    \"nve_vassdrag_nr\",\n",
    "    \"lat\",\n",
    "    \"lon\",\n",
    "    \"utm_north\",\n",
    "    \"utm_east\",\n",
    "    \"utm_zone\",\n",
    "    \"station_type\",\n",
    "]\n",
    "par_cols = [i for i in lds_q_ts.columns if i not in (st_cols + unwant_cols)]\n",
    "\n",
    "for col in unwant_cols:\n",
    "    del lds_q_ts[col]\n",
    "\n",
    "lds_q_ts = lds_q_ts[st_cols + par_cols]\n",
    "\n",
    "# Save output\n",
    "out_csv = r\"../../../Results/Loads_CSVs/loads_and_flows_rid_20_%s-%s.csv\" % (\n",
    "    st_yr,\n",
    "    end_yr,\n",
    ")\n",
    "lds_q_ts.to_csv(out_csv, encoding=\"utf-8\", index=False)\n",
    "\n",
    "# Build multi-index on lds_ts for further processing\n",
    "lds_ts.set_index([\"station_id\", \"year\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember to change the year in the folder path below!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# This code cell produces lots of Deprecation Warnings from Seaborn/Pandas.\n",
    "# %%capture suppresses all output from this cell to keep things tidy\n",
    "\n",
    "# Output folder for plots\n",
    "out_fold = r\"../../../Results/TS_Plots/RID_Plots_To_2019\"\n",
    "\n",
    "# Loop over df\n",
    "for stn_id in rid_20_df[\"station_id\"].values:\n",
    "    # Get data for this station\n",
    "    df = lds_ts.loc[stn_id]\n",
    "\n",
    "    # Separate est and val cols to two dfs\n",
    "    cols = df.columns\n",
    "    est_cols = [i for i in cols if i.split(\"_\")[1] == \"Est\"]\n",
    "    val_cols = [i for i in cols if i.split(\"_\")[1] != \"Est\"]\n",
    "    val_df = df[val_cols]\n",
    "    est_df = df[est_cols]\n",
    "\n",
    "    # Convert to \"long\" format\n",
    "    val_df.reset_index(inplace=True)\n",
    "    val_df = pd.melt(val_df, id_vars=\"year\", var_name=\"par_unit\")\n",
    "    est_df.reset_index(inplace=True)\n",
    "    est_df = pd.melt(est_df, id_vars=\"year\", var_name=\"par_est\", value_name=\"est\")\n",
    "\n",
    "    # Get just par for joining\n",
    "    val_df[\"par\"] = val_df[\"par_unit\"].str.split(\"_\", expand=True)[0]\n",
    "    est_df[\"par\"] = est_df[\"par_est\"].str.split(\"_\", expand=True)[0]\n",
    "\n",
    "    # Join\n",
    "    df = pd.merge(val_df, est_df, how=\"left\", on=[\"year\", \"par\"])\n",
    "\n",
    "    # Extract cols of interest\n",
    "    df = df[[\"year\", \"par_unit\", \"value\", \"est\"]]\n",
    "\n",
    "    # Plot\n",
    "    g = sn.factorplot(\n",
    "        x=\"year\",\n",
    "        y=\"value\",\n",
    "        hue=\"est\",\n",
    "        col=\"par_unit\",\n",
    "        col_wrap=3,\n",
    "        data=df,\n",
    "        kind=\"bar\",\n",
    "        dodge=False,\n",
    "        sharex=False,\n",
    "        sharey=False,\n",
    "        alpha=0.5,\n",
    "        aspect=2,\n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "    # Rotate tick labels and tidy\n",
    "    for ax in g.axes.flatten():\n",
    "        for tick in ax.get_xticklabels():\n",
    "            tick.set(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save\n",
    "    out_path = os.path.join(out_fold, \"%s.png\" % stn_id)\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three files created above (`concs_and_flows_rid_11-36_year.csv`, `loads_and_flows_all_sites_year.csv` and `loads_and_flows_rid_11_1990-year.csv`) can now be imported into Excel and send to NIBIO. The data layout is illustrated here:\n",
    "\n",
    "C:\\Data\\James_Work\\Staff\\Oeyvind_K\\Elveovervakingsprogrammet\\Results\\Loads_CSVs\\rid_conc_and_loads_summaries_2016.xlsx\n",
    "\n",
    "**NB:** For neatness, a couple of columns can be manually reordered so that the \"flag\" columns always come before the data columns.\n",
    "\n",
    "## 4. Generate output tables for Word\n",
    "\n",
    "### 4.1. Table 1: Raw water chemistry\n",
    "\n",
    "The code below is based on Section 2 of [notebook 5](http://nbviewer.jupyter.org/github/JamesSample/rid/blob/master/notebooks/word_data_tables.ipynb).\n",
    "\n",
    "**Updated 24/09/2018**\n",
    "\n",
    "This function has been modified to refelect changes in the 2017-20 monitoring programme:\n",
    "\n",
    " 1. The Word template now has pages for just the 20 \"main\" rivers, not the 11 + 36 rivers, as previously <br><br>\n",
    " \n",
    " 2. Four new columns have been added for new parameters measured during 2017-20 (DOC, Part. C, Tot. Part. N and TDP) <br><br>\n",
    " \n",
    " 3. Hours and minutes have been removed from the date-time column to create space for the new columns <br><br>\n",
    " \n",
    " 4. I have corrected various typos in the database (and in the template):\n",
    " \n",
    "     * `'Tot.part. N'` > `'Tot. Part. N'`\n",
    "     * `'Vosso(Bolstadelvi)'` > `'Vosso (Bolstadelvi)'`\n",
    "     * `'Nidelva(Tr.heim)'` > `'Nidelva (Tr.heim)'`\n",
    "     * `'More than 70%LOD'` > `'More than 70% >LOD'` (template only) \n",
    "     \n",
    "**Updated 25.08.2020**\n",
    "\n",
    "For the 2019 data, \"Målselv\" has been replaced by a new station downstream, \"Målselv v/gml E6-brua\" (see e-mail from Øyvind receievd 18.08.2020 at 08:09 for details). I have updated the templates to reflect this.\n",
    "\n",
    "**Remember to change the year below!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to *COPIED* template for editing\n",
    "in_docx = r\"../../../Results/Word_Tables/2020Analysis_2019Data/Table_1_2019.docx\"\n",
    "\n",
    "# Write tables\n",
    "rid.write_word_water_chem_tables(rid_20_df, 2019, in_docx, engine, samp_sel=63)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Table 2: Estimated loads at each site\n",
    "\n",
    "The code below is based on Section 3 of [notebook 5](http://nbviewer.jupyter.org/github/JamesSample/rid/blob/master/notebooks/word_data_tables.ipynb).\n",
    "\n",
    "**Updated 24/09/2018**\n",
    "\n",
    "For the 2017-20 programme, we will only report loads for the 20 \"main\" rivers, not all 155. I have therefore simplified the Word template by deleting unnecessary rows. The function itself is unchanged.\n",
    "\n",
    "**Remember to change the year in the file paths below!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:\n",
      "    Vegårdselva (AAGEVEG)...\n",
      "    Drammenselva (BUSEDRA)...\n",
      "    Altaelva (FINEALT)...\n",
      "    Pasvikelva (FINEPAS)...\n",
      "    Tanaelva (FINETAN)...\n",
      "    Vosso (Bolstadelvi) (HOREVOS)...\n",
      "    Driva (MROEDRI)...\n",
      "    Vefsna (NOREVEF)...\n",
      "    Alna (OSLEALN)...\n",
      "    Bjerkreimselva (ROGEBJE)...\n",
      "    Orreelva (ROGEORR)...\n",
      "    Vikedalselva (ROGEVIK)...\n",
      "    Nausta (SFJENAU)...\n",
      "    Nidelva (Tr.heim) (STRENID)...\n",
      "    Orkla (STREORK)...\n",
      "    Skienselva (TELESKI)...\n",
      "    Otra (VAGEOTR)...\n",
      "    Numedalslågen (VESENUM)...\n",
      "    Glomma ved Sarpsfoss (ØSTEGLO)...\n",
      "    Målselva v/gml E6-brua (TROEMÅL2)...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "# Path to *COPIED* template for editing\n",
    "in_docx = r\"../../../Results/Word_Tables/2020Analysis_2019Data/Table_2_2019.docx\"\n",
    "\n",
    "# Read loads data (from \"loads notebook\")\n",
    "loads_csv = r\"../../../Results/Loads_CSVs/loads_and_flows_all_sites_2019.csv\"\n",
    "\n",
    "# Drop Målselv as not monitored in 2019\n",
    "stn_df = rid_20_df.query(\"station_name != 'Målselv'\")\n",
    "\n",
    "# Write tables\n",
    "rid.write_word_loads_table(stn_df, loads_csv, in_docx, engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
