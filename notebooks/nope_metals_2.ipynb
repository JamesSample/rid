{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import glob\n",
    "import imp\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import nivapy3 as nivapy\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "from branca.colormap import linear\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOPE metals\n",
    "\n",
    "Initial ideas for developing a new metals module for NOPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Username:  ···\n",
      "Password:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful.\n"
     ]
    }
   ],
   "source": [
    "# Connect to db\n",
    "engine = nivapy.da.connect(src=\"nivabase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom RID functions\n",
    "rid_func_path = r\"useful_rid_code.py\"\n",
    "rid = imp.load_source(\"useful_rid_code\", rid_func_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Existing method\n",
    "\n",
    "For the RID programme, in unmonitored areas we currently estimate loads of metals by summing all the point inputs and assuming they make it to the coast. In other words, we ignore catchment retention and any diffuse inputs. The aim of this notebook is to develop an approach that is more rigorous, but as a starting point it would be good to know how well the existing method actually works. The new method should obviously be demonstrably better than what we have already.\n",
    "\n",
    "### 1.1. Observed loads for monitored sites\n",
    "\n",
    "Since 1990, the RID programme has gathered a lot of observed data for the 155 \"monitored\" rivers. We can use this to test how well summing the point inputs works for metals. This might also provide some clues regarding which aspects of the model should be improved first.\n",
    "\n",
    "The code below extracts observed metals data for the RID stations where measurements are available. \n",
    "\n",
    "**Note:** Some of the monitored rivers haven't actually been monitored since ~2004. The `RID_20` rivers have more consistent monitoring and might be better as a test, although there are fewer of them. Adjust the code below to specify the sites to be used as \"observed\" benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read station data\n",
    "in_xlsx = r\"../../../Data/RID_Sites_List_2017-2020.xlsx\"\n",
    "stn_df = pd.read_excel(in_xlsx, sheet_name=\"RID_20\")\n",
    "stn_df = stn_df.query(\"station_id != 38005\")\n",
    "\n",
    "# Period of interest\n",
    "st_yr, end_yr = 1990, 2018\n",
    "\n",
    "# Pars of interest\n",
    "par_list = [\n",
    "    \"As\",\n",
    "    \"Pb\",\n",
    "    \"Cd\",\n",
    "    \"Cu\",\n",
    "    \"Zn\",\n",
    "    \"Ni\",\n",
    "    \"Cr\",\n",
    "    \"Hg\",\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Container for results\n",
    "loads_list = []\n",
    "\n",
    "# Loop over sites\n",
    "for stn_id in stn_df[\"station_id\"].values:\n",
    "    # Loop over years\n",
    "    for year in range(st_yr, end_yr + 1):\n",
    "        print(\"Processing Station ID %s for %s\" % (stn_id, year))\n",
    "\n",
    "        # Get loads\n",
    "        l_df = rid.estimate_loads(stn_id, par_list, year, engine, infer_missing=False)\n",
    "\n",
    "        if l_df is not None:\n",
    "            # Name and reset index\n",
    "            l_df.index.name = \"station_id\"\n",
    "            l_df.reset_index(inplace=True)\n",
    "\n",
    "            # Add year\n",
    "            l_df[\"year\"] = year\n",
    "\n",
    "            # Add to outout\n",
    "            loads_list.append(l_df)\n",
    "\n",
    "# Concatenate to new df\n",
    "lds_ts = pd.concat(loads_list, axis=0)\n",
    "\n",
    "# Build multi-index\n",
    "lds_ts.set_index([\"station_id\", \"year\"], inplace=True)\n",
    "\n",
    "# Save output\n",
    "out_csv = f\"../../../NOPE/Metals/obs_metals_{st_yr}-{end_yr}.csv\"\n",
    "lds_ts.to_csv(out_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Avoids re-running the above cell, which is slow\n",
    "\n",
    "# Read obs data\n",
    "in_csv = f\"../../../NOPE/Metals/obs_metals_{st_yr}-{end_yr}.csv\"\n",
    "obs_df = pd.read_csv(in_csv)\n",
    "\n",
    "# Convery Hg to tonnes and remove units\n",
    "obs_df.set_index([\"station_id\", \"year\"], inplace=True)\n",
    "obs_df[\"Hg_kg\"] = obs_df[\"Hg_kg\"] / 1000\n",
    "obs_df.columns = [i.split(\"_\")[0] for i in obs_df.columns]\n",
    "obs_df.reset_index(inplace=True)\n",
    "\n",
    "obs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Sum point inputs for \"observed\" sites\n",
    "\n",
    "The code below builds a simplified NOPE network and uses it to accumulate point metal discharges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Build \"empty\" network\n",
    "net_csv = r\"../../../NOPE/NOPE_Core_Input_Data/regine_2018_onwards.csv\"\n",
    "net_df = pd.read_csv(net_csv, sep=\";\")\n",
    "net_df = net_df[[\"regine\", \"regine_ned\"]]\n",
    "\n",
    "# Build graph\n",
    "g = nx.DiGraph()\n",
    "\n",
    "# Add nodes\n",
    "for idx, row in net_df.iterrows():\n",
    "    nd = row[\"regine\"]\n",
    "    g.add_node(nd, local={}, accum={})\n",
    "\n",
    "# Add edges\n",
    "for idx, row in net_df.iterrows():\n",
    "    fr_nd = row[\"regine\"]\n",
    "    to_nd = row[\"regine_ned\"]\n",
    "    g.add_edge(fr_nd, to_nd)\n",
    "\n",
    "# Check directed tree\n",
    "assert nx.is_tree(g), \"g is not a valid tree.\"\n",
    "assert nx.is_directed_acyclic_graph(g), \"g is not a valid DAG.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Container for results\n",
    "data_dict = defaultdict(list)\n",
    "\n",
    "# Loop over years\n",
    "for year in range(st_yr, end_yr + 1):\n",
    "    # Group point metal inputs by regine\n",
    "    sql = (\n",
    "        \"SELECT a.regine, \"\n",
    "        \"  b.name, \"\n",
    "        \"  b.unit, \"\n",
    "        \"  SUM(c.value * d.factor) as value \"\n",
    "        \"FROM RESA2.RID_PUNKTKILDER a, \"\n",
    "        \"RESA2.RID_PUNKTKILDER_OUTPAR_DEF b, \"\n",
    "        \"RESA2.RID_PUNKTKILDER_INPAR_VALUES c, \"\n",
    "        \"RESA2.RID_PUNKTKILDER_INP_OUTP d \"\n",
    "        \"WHERE a.anlegg_nr = c.anlegg_nr \"\n",
    "        \"AND d.in_pid = c.inp_par_id \"\n",
    "        \"AND d.out_pid = b.out_pid \"\n",
    "        \"AND c.year = %s \"\n",
    "        \"GROUP BY a.regine, b.name, b.unit \"\n",
    "        \"ORDER BY a.regine\" % year\n",
    "    )\n",
    "    df = pd.read_sql(sql, engine)\n",
    "\n",
    "    # Get cols of interest\n",
    "    df = df[df[\"name\"].isin(par_list)]\n",
    "\n",
    "    # Tidy\n",
    "    df[\"par\"] = df[\"name\"] + \"_\" + df[\"unit\"]\n",
    "    del df[\"name\"], df[\"unit\"]\n",
    "\n",
    "    # Pivot\n",
    "    df = df.pivot(index=\"regine\", columns=\"par\", values=\"value\")\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    # Join to network data and tidy\n",
    "    df = pd.merge(net_df, df, how=\"left\", on=\"regine\")\n",
    "    df.fillna(0, inplace=True)\n",
    "    df.index = df[\"regine\"]\n",
    "    del df[\"regine\"], df[\"regine_ned\"]\n",
    "\n",
    "    # Update network properties\n",
    "    for idx, row in df.iterrows():\n",
    "        g.nodes[idx][\"local\"] = row.to_dict()\n",
    "\n",
    "    # Accumulate downstream\n",
    "    # Process nodes in topo order from headwaters down\n",
    "    for nd in list(nx.topological_sort(g))[:-1]:\n",
    "        # Get catchments directly upstream\n",
    "        preds = list(g.predecessors(nd))\n",
    "\n",
    "        if len(preds) > 0:\n",
    "            # Accumulate total input from upstream\n",
    "            tot_dict = defaultdict(int)  # Defaults to 0\n",
    "\n",
    "            # Loop over upstream catchments\n",
    "            for pred in preds:\n",
    "                # Loop over quantities of interest\n",
    "                for col in df.columns:\n",
    "                    tot_dict[col] += g.nodes[pred][\"accum\"][col]\n",
    "\n",
    "            # Assign outputs\n",
    "            for col in df.columns:\n",
    "                g.nodes[nd][\"accum\"][col] = g.nodes[nd][\"local\"][col] + tot_dict[col]\n",
    "\n",
    "        else:\n",
    "            # No upstream inputs\n",
    "            for col in df.columns:\n",
    "                g.nodes[nd][\"accum\"][col] = g.nodes[nd][\"local\"][col]\n",
    "\n",
    "    # Get accumulated loads for RID-155 rivers\n",
    "    for idx, row in stn_df.iterrows():\n",
    "        data_dict[\"station_id\"].append(row[\"station_id\"])\n",
    "        data_dict[\"year\"].append(year)\n",
    "        for col in df.columns:\n",
    "            data_dict[col].append(g.nodes[row[\"nve_vassdrag_nr\"]][\"accum\"][col])\n",
    "\n",
    "# Build df\n",
    "mod_df = pd.DataFrame(data_dict)\n",
    "\n",
    "# Reorder cols and remove units\n",
    "mod_df.set_index([\"station_id\", \"year\"], inplace=True)\n",
    "mod_df.columns = [i.split(\"_\")[0] for i in mod_df.columns]\n",
    "mod_df.reset_index(inplace=True)\n",
    "\n",
    "mod_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Compare observed fluxes to aggregated point inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Join mod and obs\n",
    "df = pd.merge(\n",
    "    obs_df, mod_df, how=\"left\", on=[\"station_id\", \"year\"], suffixes=[\"_obs\", \"_mod\"]\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Pars of interest (ignore Ag)\n",
    "par_list = [\"As\", \"Pb\", \"Cd\", \"Cu\", \"Zn\", \"Ni\", \"Cr\", \"Hg\"]\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(par_list):\n",
    "    axes[idx].plot(df[\"%s_obs\" % col], df[\"%s_mod\" % col], \"ro\")\n",
    "    axes[idx].plot(df[\"%s_obs\" % col], df[\"%s_obs\" % col], \"k-\")\n",
    "    axes[idx].set_title(col, fontsize=20)\n",
    "    axes[idx].set_xlabel(\"Observed\", fontsize=14)\n",
    "    axes[idx].set_ylabel(\"Summed point discharges\", fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from this plot that the existing approach does not work very well. At least there is plenty of scope for improvement! \n",
    "\n",
    "However, following a bit of experimentation, one interesting feature of the above dataset is that **summing point discharges seems to work OK for Cd, Cu and Zn for station ID 29778 (STREORK/Orkla)** - see the plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot just 29778\n",
    "ork_df = df.query(\"station_id == 29778\")\n",
    "\n",
    "# Pars of interest (ignore Ag)\n",
    "par_list = [\"Cd\", \"Cu\", \"Zn\"]\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(par_list):\n",
    "    sc = axes[idx].scatter(\n",
    "        ork_df[\"%s_obs\" % col],\n",
    "        ork_df[\"%s_mod\" % col],\n",
    "        c=ork_df[\"year\"],\n",
    "        vmin=st_yr,\n",
    "        vmax=end_yr,\n",
    "        cmap=\"coolwarm\",\n",
    "        s=50,\n",
    "    )\n",
    "    axes[idx].plot(ork_df[\"%s_obs\" % col], ork_df[\"%s_obs\" % col], \"k-\")\n",
    "    fig.colorbar(sc, ax=axes[idx], label=\"Year\")\n",
    "    axes[idx].set_title(col, fontsize=20)\n",
    "    axes[idx].set_xlabel(\"Observed\", fontsize=14)\n",
    "    axes[idx].set_ylabel(\"Summed point discharges\", fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship is far from perfect, but there's a clear positive correlation between observed loads and summed point discharges at this one site. It would be interesting to know why the approach works here, but not elsewhere. Presumably there is a major point source of Cd, Cu and Zn somewhere near our monitoring station?\n",
    "\n",
    "As a check, create a map showing the Orkla montioring site and all nearby point sources reporting Zn discharges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Read 2016 Zn data\n",
    "sql = (\n",
    "    \"SELECT a.lat_utl AS latitude, \"\n",
    "    \"  a.lon_utl AS longitude, \"\n",
    "    \"  a.anlegg_navn AS station_name, \"\n",
    "    \"  a.anlegg_nr AS station_code, \"\n",
    "    \"  (c.value * d.factor) AS value \"\n",
    "    \"FROM RESA2.RID_PUNKTKILDER a, \"\n",
    "    \"RESA2.RID_PUNKTKILDER_OUTPAR_DEF b, \"\n",
    "    \"RESA2.RID_PUNKTKILDER_INPAR_VALUES c, \"\n",
    "    \"RESA2.RID_PUNKTKILDER_INP_OUTP d \"\n",
    "    \"WHERE a.anlegg_nr = c.anlegg_nr \"\n",
    "    \"AND d.in_pid = c.inp_par_id \"\n",
    "    \"AND d.out_pid = b.out_pid \"\n",
    "    \"AND c.year = 2016 \"\n",
    "    \"AND b.name = 'Zn'\"\n",
    ")\n",
    "zn_df = pd.read_sql(sql, engine)\n",
    "\n",
    "# Get monitoring station 29778\n",
    "ork_stn = stn_df.query(\"station_id == 29778\")\n",
    "\n",
    "# Map Zn discharges\n",
    "map1 = nivapy.spatial.quickmap(\n",
    "    zn_df, popup=\"station_name\", cluster=True, kartverket=True, aerial_imagery=True\n",
    ")\n",
    "\n",
    "# Add location of monitoring station\n",
    "marker = folium.CircleMarker(\n",
    "    location=[ork_stn.iloc[0][\"lat\"], ork_stn.iloc[0][\"lon\"]],\n",
    "    radius=5,\n",
    "    weight=1,\n",
    "    color=\"black\",\n",
    "    fill_color=\"red\",\n",
    "    fill_opacity=1,\n",
    ")\n",
    "marker.add_to(map1)\n",
    "\n",
    "map1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zooming in on this map, we see that the site `Løkken Gruber (1636.0010.01)` is just upstream of the monitoring station. Let's see what this station reported in 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Read 2016 data for Løkken Gruber\n",
    "sql = (\n",
    "    \"SELECT a.anlegg_navn, \"\n",
    "    \"  a.anlegg_nr, \"\n",
    "    \"  a.type, \"\n",
    "    \"  b.name, \"\n",
    "    \"  b.unit, \"\n",
    "    \"  (c.value * d.factor) AS value \"\n",
    "    \"FROM RESA2.RID_PUNKTKILDER a, \"\n",
    "    \"RESA2.RID_PUNKTKILDER_OUTPAR_DEF b, \"\n",
    "    \"RESA2.RID_PUNKTKILDER_INPAR_VALUES c, \"\n",
    "    \"RESA2.RID_PUNKTKILDER_INP_OUTP d \"\n",
    "    \"WHERE a.anlegg_nr = c.anlegg_nr \"\n",
    "    \"AND d.in_pid = c.inp_par_id \"\n",
    "    \"AND d.out_pid = b.out_pid \"\n",
    "    \"AND c.year = 2016 \"\n",
    "    \"AND a.anlegg_nr = '1636.0010.01'\"\n",
    ")\n",
    "pt_df = pd.read_sql(sql, engine)\n",
    "\n",
    "pt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so it's pretty clear that Løkken Gruber is a major source of Cd, Cu and Zn, which makes sense. In fact, a quick Google shows that Løkken Gruber is an old copper mining region with some serious pollution potential - Miljødirektoratet even have a [dedicated website](http://www.miljostatus.no/lokken-gruber) describing the issues and its impacts on the Orkla. \n",
    "\n",
    "Overall, this is a good sign that our data actually make sense, although it would be interesting to know how Miljødirektoratet assess the \"point discharges\" from this location. If it's an old mine then the pollutiuon is likely to be \"semi-diffuse\", in which case Miljødirektoratet might just monitor concentrations downstream of the site and use these to estimate loads. If this is true, then what what the plots above really show is just how hard it is to consistently estimate river fluxes.\n",
    "\n",
    "### 1.4. Summary of existing approach\n",
    "\n",
    "In general, **summing point discharges works poorly as a way of estimating metal discharges to the coast**. However, for at least one location (STREORK/Orkla), it works OK, because we have a major mining plant (Løkken Gruber) located directly upsteram of the monitoring station. In this case, there is a clear positive relationship between recorded point discharges and monitored loads. This implies that, **in cases where point sources of metals dominate, our data are good enough to pick this up**. However, it's pretty clear that other sources (presumably diffuse?) dominate in the vast majority of Norwegian catchments.\n",
    "\n",
    "So, we need to consider diffuse inputs...\n",
    "\n",
    "## 2. Atmospheric deposition\n",
    "\n",
    "Good quality atmospheric deposition data for metals seems to be difficult to find. I have e-mailed Wenche Aas at NILU to ask what datasets are available, but for metals it seems information is limited (see e-mail from Wenche received 08/11/2017 at 10:44). The best data I can find come from the [Meteorological Synthesizing Centre-East](http://www.msceast.org/index.php/pollution-assessment/emep-domain-menu/data-hm-pop-menu), which is part of EMEP. They have modelled deposition data for Cd, Hg and Pb for Norway for the years 2014 and 2015. Data are provided on a 50 km EMEP grid and are saved locally here:\n",
    "\n",
    "C:\\Data\\James_Work\\Staff\\Oeyvind_K\\Elveovervakingsprogrammet\\NOPE\\Metals\\Raw_Datasets\\EMEP\n",
    "\n",
    "Based on this data, I can estimate atmospheric inputs of Cd, Hg and Pb to each regine catchment. This can be added to the point source data (section 1.2, above) and compared to the observations to see if the fit to the observed data improves.\n",
    "\n",
    "### 2.1. Explore MSC-E data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Read EMEP shapefile\n",
    "in_shp = (\n",
    "    r\"C:\\Data\\James_Work\\Staff\\Oeyvind_K\\Elveovervakingsprogrammet\\NOPE\\Metals\"\n",
    "    r\"\\Raw_Datasets\\EMEP\\reprojected_emep_grid_50\\emep_scandinavia.shp\"\n",
    ")\n",
    "gdf = gpd.read_file(in_shp)\n",
    "\n",
    "# Get cols of interest\n",
    "gdf = gdf[[\"geometry\", \"i50\", \"j50\"]]\n",
    "\n",
    "# Read MSC-E dep data\n",
    "# Files to read\n",
    "dep_fold = (\n",
    "    r\"C:\\Data\\James_Work\\Staff\\Oeyvind_K\\Elveovervakingsprogrammet\"\n",
    "    r\"\\NOPE\\Metals\\Raw_Datasets\\EMEP\"\n",
    ")\n",
    "search_path = os.path.join(dep_fold, \"*.txt\")\n",
    "file_list = glob.glob(search_path)\n",
    "df_list = []\n",
    "\n",
    "# Loop over files\n",
    "for fpath in file_list:\n",
    "    # Get info\n",
    "    par, _, year = os.path.split(fpath)[1].split(\"_\")\n",
    "    year = year[:4]\n",
    "    dep_df = pd.read_csv(\n",
    "        fpath,\n",
    "        delim_whitespace=True,\n",
    "        skiprows=1,\n",
    "        header=None,\n",
    "        names=[\"i50\", \"j50\", \"%s_%s\" % (par, year)],\n",
    "    )\n",
    "\n",
    "    # Set i_j as idx\n",
    "    dep_df[\"i_j\"] = dep_df[\"i50\"].map(str) + \"_\" + dep_df[\"j50\"].map(str)\n",
    "    dep_df.index = dep_df[\"i_j\"]\n",
    "    del dep_df[\"i_j\"], dep_df[\"i50\"], dep_df[\"j50\"]\n",
    "\n",
    "    # Add to output\n",
    "    df_list.append(dep_df)\n",
    "\n",
    "# Merge and tidy\n",
    "dep_df = pd.concat(df_list, axis=1)\n",
    "dep_df.reset_index(inplace=True)\n",
    "dep_df[\"i50\"], dep_df[\"j50\"] = [\n",
    "    i.astype(int) for i in dep_df[\"i_j\"].str.split(\"_\", 1).str\n",
    "]\n",
    "del dep_df[\"i_j\"]\n",
    "\n",
    "# Join spatial info\n",
    "gdf = gdf.merge(dep_df, on=[\"i50\", \"j50\"], how=\"inner\")\n",
    "\n",
    "# Convert crs to UTM Z33\n",
    "gdf = gdf.to_crs(epsg=\"32633\")\n",
    "\n",
    "# Save\n",
    "out_shp = (\n",
    "    r\"C:\\Data\\James_Work\\Staff\\Oeyvind_K\\Elveovervakingsprogrammet\"\n",
    "    r\"\\NOPE\\Metals\\Raw_Datasets\\EMEP\\msc-e_atmos_dep_utm_z33n.shp\"\n",
    ")\n",
    "gdf.to_file(out_shp)\n",
    "\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, axes = plt.subplots(ncols=3, nrows=2, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop over data\n",
    "cols = [\"cd_2014\", \"hg_2014\", \"pb_2014\", \"cd_2015\", \"hg_2015\", \"pb_2015\"]\n",
    "\n",
    "for idx, col in enumerate(cols):\n",
    "    # Choropleth\n",
    "    gdf.plot(\n",
    "        ax=axes[idx],\n",
    "        column=col,\n",
    "        cmap=\"coolwarm\",\n",
    "        scheme=\"QUANTILES\",\n",
    "        k=9,\n",
    "        edgecolor=\"black\",\n",
    "        alpha=0.5,\n",
    "        legend=True,\n",
    "    )\n",
    "\n",
    "    axes[idx].set_aspect(\"equal\")\n",
    "    axes[idx].xaxis.set_ticklabels([])\n",
    "    axes[idx].yaxis.set_ticklabels([])\n",
    "    axes[idx].set_title(col.capitalize(), fontsize=20)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# User input\n",
    "par = \"pb_2015\"\n",
    "unit = \"kg/km2/yr\"\n",
    "\n",
    "# Choropleth map\n",
    "map2 = nivapy.spatial.choropleth_from_gdf(\n",
    "    gdf,\n",
    "    par,\n",
    "    geom=\"geometry\",\n",
    "    tiles=\"Stamen Terrain\",\n",
    "    fill_color=\"YlOrRd\",\n",
    "    fill_opacity=0.4,\n",
    "    line_opacity=1,\n",
    "    legend_name=\"%s deposition (%s)\" % (par, unit),\n",
    ")\n",
    "map2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that on the maps above, Pb is in kg/km2/yr, while Cd and Hg are in g/km2/yr.\n",
    "\n",
    "### 2.2. Atmospheric inputs by regine\n",
    "\n",
    "The next step is to intersect the deposition data with the regine catchments to estimate atmospheric inputs of metals for each catchment. With the MSC-E data, I can only do this for three metals in two years, but at least this will allow me to guage whether there is a clear relationship between deposition and river fluxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Read regine shapefile\n",
    "reg_shp = (\n",
    "    r\"C:\\Data\\James_Work\\Staff\\Oeyvind_K\\Elveovervakingsprogrammet\"\n",
    "    r\"\\Data\\gis\\shapefiles\\RegMinsteF.shp\"\n",
    ")\n",
    "reg_gdf = gpd.read_file(reg_shp)\n",
    "\n",
    "# Get cols of interest\n",
    "reg_gdf = reg_gdf[[\"geometry\", \"VASSDRAGNR\"]]\n",
    "reg_gdf.columns = [\"geometry\", \"regine\"]\n",
    "\n",
    "# Convert crs to UTM Z33\n",
    "reg_gdf = reg_gdf.to_crs(epsg=\"32633\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_overlays(df1, df2, how=\"intersection\"):\n",
    "    \"\"\"Hugely improves performance compared to gpd.overlay(df1, df2, how='intersection').\n",
    "    An improved version should eventually be available within Geopandas itself.\n",
    "\n",
    "    From here:\n",
    "        https://github.com/geopandas/geopandas/issues/400\n",
    "    \"\"\"\n",
    "    import geopandas as gpd\n",
    "    import pandas as pd\n",
    "\n",
    "    df1 = df1.copy()\n",
    "    df2 = df2.copy()\n",
    "    df1[\"geometry\"] = df1.geometry.buffer(0)\n",
    "    df2[\"geometry\"] = df2.geometry.buffer(0)\n",
    "    if how == \"intersection\":\n",
    "        # Spatial Index to create intersections\n",
    "        spatial_index = df2.sindex\n",
    "        df1[\"bbox\"] = df1.geometry.apply(lambda x: x.bounds)\n",
    "        df1[\"histreg\"] = df1.bbox.apply(lambda x: list(spatial_index.intersection(x)))\n",
    "        pairs = df1[\"histreg\"].to_dict()\n",
    "        nei = []\n",
    "        for i, j in pairs.items():\n",
    "            for k in j:\n",
    "                nei.append([i, k])\n",
    "\n",
    "        pairs = gpd.GeoDataFrame(nei, columns=[\"idx1\", \"idx2\"], crs=df1.crs)\n",
    "        pairs = pairs.merge(df1, left_on=\"idx1\", right_index=True)\n",
    "        pairs = pairs.merge(\n",
    "            df2, left_on=\"idx2\", right_index=True, suffixes=[\"_1\", \"_2\"]\n",
    "        )\n",
    "        pairs[\"Intersection\"] = pairs.apply(\n",
    "            lambda x: (x[\"geometry_1\"].intersection(x[\"geometry_2\"])).buffer(0), axis=1\n",
    "        )\n",
    "        pairs = gpd.GeoDataFrame(pairs, columns=pairs.columns, crs=df1.crs)\n",
    "        cols = pairs.columns.tolist()\n",
    "        cols.remove(\"geometry_1\")\n",
    "        cols.remove(\"geometry_2\")\n",
    "        cols.remove(\"histreg\")\n",
    "        cols.remove(\"bbox\")\n",
    "        cols.remove(\"Intersection\")\n",
    "        dfinter = pairs[cols + [\"Intersection\"]].copy()\n",
    "        dfinter.rename(columns={\"Intersection\": \"geometry\"}, inplace=True)\n",
    "        dfinter = gpd.GeoDataFrame(dfinter, columns=dfinter.columns, crs=pairs.crs)\n",
    "        dfinter = dfinter.loc[dfinter.geometry.is_empty == False]\n",
    "        return dfinter\n",
    "\n",
    "    elif how == \"difference\":\n",
    "        spatial_index = df2.sindex\n",
    "        df1[\"bbox\"] = df1.geometry.apply(lambda x: x.bounds)\n",
    "        df1[\"histreg\"] = df1.bbox.apply(lambda x: list(spatial_index.intersection(x)))\n",
    "        df1[\"new_g\"] = df1.apply(\n",
    "            lambda x: reduce(\n",
    "                lambda x, y: x.difference(y).buffer(0),\n",
    "                [x.geometry] + list(df2.iloc[x.histreg].geometry),\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        df1.geometry = df1.new_g\n",
    "        df1 = df1.loc[df1.geometry.is_empty == False].copy()\n",
    "        df1.drop([\"bbox\", \"histreg\", new_g], axis=1, inplace=True)\n",
    "        return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Intersection\n",
    "int_gdf = spatial_overlays(reg_gdf, gdf, how=\"intersection\")\n",
    "\n",
    "# Save\n",
    "out_shp = (\n",
    "    r\"C:\\Data\\James_Work\\Staff\\Oeyvind_K\\Elveovervakingsprogrammet\"\n",
    "    r\"\\NOPE\\Metals\\Raw_Datasets\\EMEP\\regine_atmos_dep_intersection.shp\"\n",
    ")\n",
    "int_gdf.to_file(out_shp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Areas for each intersected polygon\n",
    "int_gdf[\"area_km2\"] = int_gdf[\"geometry\"].area / 1.0e6\n",
    "\n",
    "# Loads per regine part\n",
    "for col in cols:\n",
    "    int_gdf[\"%s_ld\" % col] = int_gdf[col] * int_gdf[\"area_km2\"]  # g for Cd and Hg\n",
    "    # kg for Pb\n",
    "\n",
    "# Sum regine areas in intersected file\n",
    "agg_df = pd.DataFrame(int_gdf).groupby(\"regine\").sum()\n",
    "\n",
    "# Convert back to avg dep rates per regine\n",
    "for col in cols:\n",
    "    agg_df[col] = agg_df[\"%s_ld\" % col] / agg_df[\"area_km2\"]\n",
    "\n",
    "# Tidy\n",
    "agg_df.reset_index(inplace=True)\n",
    "\n",
    "# Join back to regine gdf\n",
    "reg_gdf = reg_gdf.merge(agg_df, on=\"regine\", how=\"left\")\n",
    "\n",
    "# Get regine areas in original shp\n",
    "reg_gdf[\"area_km2\"] = reg_gdf[\"geometry\"].area / 1.0e6\n",
    "\n",
    "# Loads per regine\n",
    "for col in cols:\n",
    "    # Get par\n",
    "    par = col.split(\"_\")[0]\n",
    "\n",
    "    # Calc loads, converting to tonnes\n",
    "    if par == \"pb\":  # kgs\n",
    "        reg_gdf[\"%s_ld\" % col] = reg_gdf[col] * reg_gdf[\"area_km2\"] / 1000.0\n",
    "\n",
    "    else:  # g\n",
    "        reg_gdf[\"%s_ld\" % col] = reg_gdf[col] * reg_gdf[\"area_km2\"] / 1.0e6\n",
    "\n",
    "# Get cols of interest\n",
    "reg_gdf = reg_gdf[\n",
    "    [\n",
    "        \"geometry\",\n",
    "        \"regine\",\n",
    "        \"area_km2\",\n",
    "        \"cd_2014_ld\",\n",
    "        \"hg_2014_ld\",\n",
    "        \"pb_2014_ld\",\n",
    "        \"cd_2015_ld\",\n",
    "        \"hg_2015_ld\",\n",
    "        \"pb_2015_ld\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Save\n",
    "out_shp = (\n",
    "    r\"C:\\Data\\James_Work\\Staff\\Oeyvind_K\\Elveovervakingsprogrammet\"\n",
    "    r\"\\NOPE\\Metals\\Raw_Datasets\\EMEP\\regine_atmos_dep.shp\"\n",
    ")\n",
    "reg_gdf.to_file(out_shp)\n",
    "\n",
    "reg_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Accumulate point and atmospheric inputs\n",
    "\n",
    "The code below combines the point and atmospheric inputs and accumulates them over the catchment network (assuming no retention)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data of interest\n",
    "st_yr, end_yr = 2014, 2015\n",
    "\n",
    "par_list = [\"Cd\", \"Hg\", \"Pb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Convert gdf to df\n",
    "reg_df = pd.DataFrame(\n",
    "    reg_gdf[\n",
    "        [\n",
    "            \"regine\",\n",
    "            \"area_km2\",\n",
    "            \"cd_2014_ld\",\n",
    "            \"hg_2014_ld\",\n",
    "            \"pb_2014_ld\",\n",
    "            \"cd_2015_ld\",\n",
    "            \"hg_2015_ld\",\n",
    "            \"pb_2015_ld\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Container for results\n",
    "data_dict = defaultdict(list)\n",
    "\n",
    "# Loop over years\n",
    "for year in range(st_yr, end_yr + 1):\n",
    "    # Group point metal inputs by regine\n",
    "    sql = (\n",
    "        \"SELECT a.regine, \"\n",
    "        \"  b.name, \"\n",
    "        \"  b.unit, \"\n",
    "        \"  SUM(c.value * d.factor) as value \"\n",
    "        \"FROM RESA2.RID_PUNKTKILDER a, \"\n",
    "        \"RESA2.RID_PUNKTKILDER_OUTPAR_DEF b, \"\n",
    "        \"RESA2.RID_PUNKTKILDER_INPAR_VALUES c, \"\n",
    "        \"RESA2.RID_PUNKTKILDER_INP_OUTP d \"\n",
    "        \"WHERE a.anlegg_nr = c.anlegg_nr \"\n",
    "        \"AND d.in_pid = c.inp_par_id \"\n",
    "        \"AND d.out_pid = b.out_pid \"\n",
    "        \"AND c.year = %s \"\n",
    "        \"GROUP BY a.regine, b.name, b.unit \"\n",
    "        \"ORDER BY a.regine\" % year\n",
    "    )\n",
    "    df = pd.read_sql(sql, engine)\n",
    "\n",
    "    # Get cols of interest\n",
    "    df = df[df[\"name\"].isin(par_list)]\n",
    "\n",
    "    # Tidy\n",
    "    df[\"par\"] = df[\"name\"] + \"_\" + df[\"unit\"]\n",
    "    del df[\"name\"], df[\"unit\"]\n",
    "\n",
    "    # Pivot\n",
    "    df = df.pivot(index=\"regine\", columns=\"par\", values=\"value\")\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    # Join to network data and tidy\n",
    "    df = pd.merge(net_df, df, how=\"left\", on=\"regine\")\n",
    "\n",
    "    # Join atmospheric data\n",
    "    df = pd.merge(df, reg_df, how=\"left\", on=\"regine\")\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    # Sum point and atmos\n",
    "    for par in par_list:\n",
    "        df[\"%s_tonn\" % par] = df[\"%s_tonn\" % par] + df[\"%s_%s_ld\" % (par.lower(), year)]\n",
    "\n",
    "    # Get cols of interest\n",
    "    df = df[\n",
    "        [\n",
    "            \"regine\",\n",
    "        ]\n",
    "        + [\"%s_tonn\" % i for i in par_list]\n",
    "    ]\n",
    "    df.index = df[\"regine\"]\n",
    "    del df[\"regine\"]\n",
    "\n",
    "    # Update network properties\n",
    "    for idx, row in df.iterrows():\n",
    "        g.node[idx][\"local\"] = row.to_dict()\n",
    "\n",
    "    # Accumulate downstream\n",
    "    # Process nodes in topo order from headwaters down\n",
    "    for nd in nx.topological_sort(g)[:-1]:\n",
    "        # Get catchments directly upstream\n",
    "        preds = g.predecessors(nd)\n",
    "\n",
    "        if len(preds) > 0:\n",
    "            # Accumulate total input from upstream\n",
    "            tot_dict = defaultdict(int)  # Defaults to 0\n",
    "\n",
    "            # Loop over upstream catchments\n",
    "            for pred in preds:\n",
    "                # Loop over quantities of interest\n",
    "                for col in df.columns:\n",
    "                    tot_dict[col] += g.node[pred][\"accum\"][col]\n",
    "\n",
    "            # Assign outputs\n",
    "            for col in df.columns:\n",
    "                g.node[nd][\"accum\"][col] = g.node[nd][\"local\"][col] + tot_dict[col]\n",
    "\n",
    "        else:\n",
    "            # No upstream inputs\n",
    "            for col in df.columns:\n",
    "                g.node[nd][\"accum\"][col] = g.node[nd][\"local\"][col]\n",
    "\n",
    "    # Get accumulated loads for RID-155 rivers\n",
    "    for idx, row in stn_df.iterrows():\n",
    "        data_dict[\"station_id\"].append(row[\"station_id\"])\n",
    "        data_dict[\"year\"].append(year)\n",
    "        for col in df.columns:\n",
    "            data_dict[col].append(g.node[row[\"nve_vassdrag_nr\"]][\"accum\"][col])\n",
    "\n",
    "# Build df\n",
    "mod_df = pd.DataFrame(data_dict)\n",
    "\n",
    "# Reorder cols and remove units\n",
    "mod_df.set_index([\"station_id\", \"year\"], inplace=True)\n",
    "mod_df.columns = [i.split(\"_\")[0] for i in mod_df.columns]\n",
    "mod_df.reset_index(inplace=True)\n",
    "\n",
    "mod_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Compare to observed\n",
    "\n",
    "The plots below show how the summed point and atmospheric deposition data compare to the observed loads (assuming everything is transported to the coast)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Join mod and obs\n",
    "df = pd.merge(\n",
    "    mod_df, obs_df, how=\"left\", on=[\"station_id\", \"year\"], suffixes=[\"_mod\", \"_obs\"]\n",
    ")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(16, 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(par_list):\n",
    "    axes[idx].plot(df[\"%s_obs\" % col], df[\"%s_mod\" % col], \"ro\")\n",
    "    axes[idx].plot(df[\"%s_obs\" % col], df[\"%s_obs\" % col], \"k-\")\n",
    "    axes[idx].set_title(col, fontsize=20)\n",
    "    axes[idx].set_xlabel(\"Observed\", fontsize=14)\n",
    "    axes[idx].set_ylabel(\"Point and atmospheric inputs\", fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorporating atmospheric deposition clearly gives much better results than just allowing for point inputs (compare with the plots in section 1.3). For Cd and Pb, we actually get reasonable results by just assuming all inputs make it to the sea. For Hg, however, this approach **dramatically overestimates** fluxes, implying that either (i) the MSC-E simulations substantially over-predict Hg deposition, or (ii) much more Hg is retained in the freshwater/terrestrial environment than for Cd or Pb. Note also that the Hg concentrations measured in rivers as part of the RID programme are usually very low (often below the LOD), so our load estimates for Hg are poorly constrained and therefore difficult assess.\n",
    "\n",
    "### 2.5. Summary of atmospheric deposition\n",
    "\n",
    " * Combining the MSC-E simulations with point discharge estimates gives substantially improved flux estimates compared to using point values alone (even ignoring any catchment retention) <br><br>\n",
    " \n",
    " * Results for Cd and Pb look especially promising; for Hg, this approach looks like it would substantially overestimate fluxes <br><br>\n",
    " \n",
    " * The data imply that **Hg might be more strongly retained in catchments than Cd or Pb. Is there a physical/scientific basis for why this might be?** <br><br>\n",
    " \n",
    " * MSC-E only proivde data for 2014 and 2015 for three metals: Hg, Cd and Pb. I assume these datasets will be updated each year, but probably not in time for RID reporting (the 2016 data are not yet available, for example). <br><br>\n",
    " \n",
    " * MSC-E therefore provides a good starting point, but we need to find some way of generalising to other metals. This may not be straightforward, as the plots above suggest it is not reasonable to treat all metals the same way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
