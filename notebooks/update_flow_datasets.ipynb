{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imp\n",
    "import glob\n",
    "import os\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update RID flow datasets\n",
    "\n",
    "Each year, updated flow datasets (both modelled and observed) are obtained from NVE and added to RESA2. Tore has a number of Access files here:\n",
    "\n",
    "K:\\Avdeling\\Vass\\316_Miljøinformatikk\\Prosjekter\\RID\\Vannføring\n",
    "\n",
    "which handle the update process. The code in this notebook replaces this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Connect to db\n",
    "resa2_basic_path = (r'C:\\Data\\James_Work\\Staff\\Heleen_d_W\\ICP_Waters\\Upload_Template'\n",
    "                    r'\\useful_resa2_code.py')\n",
    "\n",
    "resa2_basic = imp.load_source('useful_resa2_code', resa2_basic_path)\n",
    "\n",
    "engine, conn = resa2_basic.connect_to_resa2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Observed discharge\n",
    "\n",
    "Observed time series are used **only** for the 11 main rivers (project `RID (O 25800 03)`) - all other calculations are based on modelled flows (from HBV). Discharge for these 11 sites can be obtained from NVE (mostly from the [Hydra II database](http://www4.nve.no/xhydra/)). Note that more than 11 discharge stations are involved, because at some chemistry sampling locations the flow is the sum of several NVE discharge series. \n",
    "\n",
    "The discharge stations associated with the 11 water chemistry sampling locations are listed in the table below, together with where the datasets come from. \n",
    "\n",
    "| Chem station ID | Chem station code | NVE station ID(s) | RESA flow station ID | Availability | Comment |\n",
    "|:---------------:|:-----------------:|:-----------------:|:--------------------:|:------------:|:-----------------------------------------------------------------------------------------------:|\n",
    "| 29612 | BUSEDRA | 12.285 | 57 | Hydra II | 2016 data not yet available 26/07/2016 |\n",
    "| 29613 | TELESKI | 16.153 + 16.133 | 59 | From Trine | Data for 16.153 not available - assume constant at 10 m3/s. Data for 16.133 supplied 28/06/2017 |\n",
    "| 29614 | VAGEOTR | 21.71 or 21.11 | 487 | Hydra II | 2016 data downloaded 19/06/2017 |\n",
    "| 29615 | VESENUM | 15.61 | 58 | Hydra II | 2016 data downloaded 19/06/2017 |\n",
    "| 29617 | ØSTEGLO | 2.605 | 56 | From Trine | 2016 data received 28/06/2017 |\n",
    "| 29778 | STREORK | 121.22 | 348 | Hydra II | 2016 data downloaded 19/06/2017 |\n",
    "| 29779 | FINEALT | 212.11 | 386 | Hydra II | 2016 data downloaded 26/07/2017 |\n",
    "| 29782 | NOREVEF | 151.5 | 351 | Hydra II | 2016 data downloaded 26/07/2017 |\n",
    "| 29783 | ROGEORR | 28.7 | 355 | Hydra II | 2016 data downloaded 19/06/2017 |\n",
    "| 29821 | HOREVOS | 62.5 | 546 | Hydra II | 2016 data downloaded 19/06/2017 |\n",
    "| 36225 | OSLEALN | 6.78 | 626 | Hydra II | 2016 data downloaded 26/07/2017 |\n",
    "\n",
    "Note the following:\n",
    "\n",
    " * Chemistry station 29613 should ideally use the sum of NVE series 16.133 and 16.153, but the latter is no longer available. Trine Fjeldstad at NVE can supply data for station 16.133 and we simply assume the input from 16.153 is constant at 10 $m^3/s$ (which is roughly equal to the long-term average). <br><br>\n",
    " \n",
    " * The discharge for chemistry station 29614 is **either** NVE station 21.71 **or** 21.11. Both stations should have exactly the same flow values in the HydraII database. Only one set of values are required - check HydraII to see which dataset is updated first. <br><br> \n",
    " \n",
    " * Discharge data for chemistry stations 29617 (NVE ID 2.605) and 36225 (NVE ID 6.78) are often delayed. Need to contact Trine at NVE early to avoid problems later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Year of interest\n",
    "year = 2016\n",
    "\n",
    "# Folder containing Hydra II data\n",
    "hyd_fold = (r'C:\\Data\\James_Work\\Staff\\Oeyvind_K\\Elveovervakingsprogrammet'\n",
    "            r'\\Data\\nve_observed\\hydra_ii')\n",
    "\n",
    "# Folder containing data from Trine\n",
    "tri_fold = (r'C:\\Data\\James_Work\\Staff\\Oeyvind_K\\Elveovervakingsprogrammet'\n",
    "            r'\\Data\\nve_observed\\from_trine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dict mapping NVE codes to RESA discharge station IDs\n",
    "stn_id_dict = {'12_285':57,\n",
    "               '16_133':59,   # Need to add 10 m3/s\n",
    "               '21_71':487,   # Could also use 21_11\n",
    "               '15_61':58,\n",
    "               '2_605':56,\n",
    "               '121_22':348,\n",
    "               '212_11':386,\n",
    "               '151_5':351,\n",
    "               '28_7':355,\n",
    "               '62_5':546,\n",
    "               '6_78':626}\n",
    "\n",
    "# List to store output\n",
    "df_list = []\n",
    "\n",
    "# Get list of Hydra II files to process\n",
    "search_path = os.path.join(hyd_fold, '*.csv')\n",
    "file_list = glob.glob(search_path)\n",
    "\n",
    "# Loop over Hydra II data\n",
    "for file_path in file_list:\n",
    "    # Get RESA station ID\n",
    "    name = os.path.split(file_path)[1][:-4]\n",
    "    stn_id = stn_id_dict[name]\n",
    "    \n",
    "    # Parse file\n",
    "    df = pd.read_csv(file_path, skiprows=2, index_col=0,\n",
    "                     parse_dates=True, header=None, \n",
    "                     names=['xdate', 'xvalue'], \n",
    "                     na_values='-9999')\n",
    "    \n",
    "    # Get just records for year of interest\n",
    "    df = df.truncate(before='%s-01-01' % year,\n",
    "                     after='%s-01-01' % (year+1))\n",
    "\n",
    "    # Linear interpolation of NaN\n",
    "    df['xvalue'].interpolate(method='linear', inplace=True)\n",
    "\n",
    "    # Add other required cols and tidy\n",
    "    df['dis_station_id'] = stn_id\n",
    "    df['xcomment'] = np.nan\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    # Reorder cols\n",
    "    df = df[['dis_station_id', 'xdate', 'xvalue', 'xcomment']]\n",
    "    \n",
    "    # Append to output\n",
    "    df_list.append(df)\n",
    "\n",
    "# Get list of files from Trine to process\n",
    "search_path = os.path.join(tri_fold, '*.csv')\n",
    "file_list = glob.glob(search_path)\n",
    "\n",
    "# Loop over files from Trine\n",
    "for file_path in file_list:\n",
    "    # Get RESA station ID\n",
    "    name = os.path.split(file_path)[1][:-4]\n",
    "    stn_id = stn_id_dict[name]\n",
    "    \n",
    "    # Parse file\n",
    "    df = pd.read_csv(file_path, skiprows=1, index_col=0,\n",
    "                     parse_dates=True, header=None, \n",
    "                     sep=';', names=['xdate', 'xvalue'],\n",
    "                     na_values='-9999')\n",
    "    \n",
    "    # Get just records for year of interest\n",
    "    df = df.truncate(before='%s-01-01' % year,\n",
    "                     after='%s-01-01' % (year+1))\n",
    "    \n",
    "    # Linear interpolation of NaN\n",
    "    df['xvalue'].interpolate(method='linear', inplace=True)\n",
    "\n",
    "    # Add dis_id and tidy\n",
    "    df['dis_station_id'] = stn_id\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    # Reorder cols\n",
    "    df = df[['dis_station_id', 'xdate', 'xvalue']]\n",
    "    \n",
    "    # Append to output\n",
    "    df_list.append(df)  \n",
    "\n",
    "# Stack data\n",
    "df = pd.concat(df_list, axis=0)\n",
    "\n",
    "# Check length of df is as expected \n",
    "# Get number of days in year of interest\n",
    "days = len(pd.date_range(start='%s-01-01' % year, \n",
    "                         end='%s-12-31' % year,\n",
    "                         freq='D'))\n",
    "\n",
    "assert len(df) == 11*days, 'Check datasets are complete for all sites.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3660 4026\n"
     ]
    }
   ],
   "source": [
    "# Add new rows to database\n",
    "df.to_sql('discharge_values', con=engine, schema='resa2', \n",
    "          if_exists='append', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modelled discharge\n",
    "\n",
    "Stein has supplied modelled data from HBV for the period from 1990 to 2016 (see e-mail received 13/06/2017 at 12.17). These data are stored locally here:\n",
    "\n",
    "C:\\Data\\James_Work\\Staff\\Oeyvind_K\\Elveovervakingsprogrammet\\Data\\hbv_modelled\\RID_2016\n",
    "\n",
    "and on the network here:\n",
    "\n",
    "K:\\Avdeling\\Vass\\316_Miljøinformatikk\\Prosjekter\\RID\\Vannføring\\Modellert\\nve_modellert_2017\n",
    "\n",
    "The flow files are named e.g. `hbv_00000001.var`, where the number corresponds to the NVE \"vassdragsområde\". These are listed in *vassomr.pdf* in the above folder, and they're also included in RESA2's `DISCHARGE_STATIONS` table. The vassdragsområde numbers are stored in the `NVE_SERINUMMER` field.\n",
    "\n",
    "Tore has an Access database in e.g.\n",
    "\n",
    "K:\\Avdeling\\Vass\\316_Miljøinformatikk\\Prosjekter\\RID\\Vannføring\\Modellert\\NVE_MODELLERT_2016\\vannføring\n",
    "\n",
    "that first deletes the modelled NVE values for each station from 1990 onwards and then adds the new data, which includes everything from 1990 plus the additional year of data. The code below does the same, and performs some basic checking of the data at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Year of interest\n",
    "year = 2016\n",
    "\n",
    "# Folder containing modelled data\n",
    "data_fold = (r'C:\\Data\\James_Work\\Staff\\Oeyvind_K\\Elveovervakingsprogrammet'\n",
    "             r'\\Data\\hbv_modelled\\RID_2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a list of files to process (only interested in flow here)\n",
    "search_path = os.path.join(data_fold, 'hbv_*.var')\n",
    "file_list = glob.glob(search_path)\n",
    "\n",
    "# Get number of days between 1990 and year of interest\n",
    "days_new = len(pd.date_range(start='1990-01-01', \n",
    "                             end='%s-12-31' % year,\n",
    "                             freq='D'))\n",
    "\n",
    "# Get number of days between 1990 and year before\n",
    "days_old = len(pd.date_range(start='1990-01-01', \n",
    "                             end='%s-12-31' % (year-1),\n",
    "                             freq='D'))\n",
    "\n",
    "# Loop over files\n",
    "for file_path in file_list:\n",
    "    # Get name and reg. nr.\n",
    "    name = os.path.split(file_path)[1]\n",
    "    reg_nr = int(name.split('_')[1][:-4])\n",
    "    \n",
    "    # Get RESA2 station ID\n",
    "    sql = (\"SELECT dis_station_id FROM resa2.discharge_stations \"\n",
    "           \"WHERE nve_serienummer = '%s'\" % reg_nr)\n",
    "    dis_id = pd.read_sql_query(sql, engine).iloc[0,0]\n",
    "\n",
    "    # Check number of post-1990 records already in db\n",
    "    # (should equal days_old)\n",
    "    sql = (\"SELECT COUNT(*) FROM resa2.discharge_values \"\n",
    "           \"WHERE dis_station_id = %s \"\n",
    "           \"AND xdate >= DATE '1990-01-01'\" % dis_id)    \n",
    "    cnt_old = pd.read_sql_query(sql, engine).iloc[0,0]    \n",
    "    assert cnt_old == days_old, 'Unexpected number of records already in database.'\n",
    "    \n",
    "    # Read new data\n",
    "    df = pd.read_csv(file_path, delim_whitespace=True, \n",
    "                     header=None, names=['XDATE', 'XVALUE'])\n",
    "    \n",
    "    # Convert dates\n",
    "    df['XDATE'] = pd.to_datetime(df['XDATE'], format='%Y%m%d/1200')\n",
    "\n",
    "    # Check st, end and length\n",
    "    assert df['XDATE'].iloc[0] == pd.Timestamp('1990-01-01'), 'New series does not start on 01/01/1990.'\n",
    "    assert df['XDATE'].iloc[-1] == pd.Timestamp('%s-12-31' % year), 'New series does not end on 31/12/%s.' % year\n",
    "    assert len(df) == days_new, 'Unexpected length for new series.'\n",
    "    \n",
    "    # Add station ID to df\n",
    "    df['DIS_STATION_ID'] = dis_id\n",
    "    \n",
    "    # Drop existing rows post-1990 for this site\n",
    "    sql = (\"DELETE FROM resa2.discharge_values \"\n",
    "           \"WHERE dis_station_id = %s \"\n",
    "           \"AND xdate >= DATE '1990-01-01'\" % dis_id)\n",
    "    res = conn.execute(sql)\n",
    "    \n",
    "    # Add new rows\n",
    "    df.to_sql('discharge_values', con=engine, schema='resa2', \n",
    "              if_exists='append', index=False)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
