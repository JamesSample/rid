{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import nivapy3 as nivapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update RID flow datasets\n",
    "\n",
    "Each year, updated flow datasets (both modelled and observed) are obtained from NVE and added to RESA2. Tore has a number of Access files here:\n",
    "\n",
    "K:\\Avdeling\\Vass\\316_Miljøinformatikk\\Prosjekter\\RID\\Vannføring\n",
    "\n",
    "which handle the update process. The code in this notebook replaces this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Username:  ···\n",
      "Password:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful.\n"
     ]
    }
   ],
   "source": [
    "# Connect to db\n",
    "engine = nivapy.da.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Observed discharge\n",
    "\n",
    "Observed time series are used **only** for the 11 main rivers (project `RID (O 25800 03)`) - all other calculations are based on modelled flows (from HBV). Discharge for these 11 sites can be obtained from NVE (mostly from the [Hydra II database](http://www4.nve.no/xhydra/)). Note that more than 11 discharge stations are involved, because at some chemistry sampling locations the flow is the sum of several NVE discharge series. Note also the following:\n",
    "\n",
    " * Chemistry station 29613 should ideally use the sum of NVE series 16.133 and 16.153, but the latter is no longer available. Trine Fjeldstad at NVE can supply data for station 16.133 and we simply assume the input from 16.153 is constant at 10 $m^3/s$ (which is roughly equal to the long-term average). <br><br>\n",
    " \n",
    " * The discharge for chemistry station 29614 is **either** NVE station 21.71 **or** 21.11. Both stations should have exactly the same flow values in the HydraII database. Only one set of values are required - check HydraII to see which dataset is updated first. <br><br> \n",
    " \n",
    " * Discharge data for chemistry stations 29617 (NVE ID 2.605) and 36225 (NVE ID 6.78) are often delayed. Need to contact Trine at NVE early to avoid problems later.\n",
    "\n",
    "### 1.1. Data for 2016\n",
    "\n",
    "The discharge stations associated with the 11 water chemistry sampling locations are listed in the table below, together with where the datasets came from in 2016. \n",
    "\n",
    "| Chem station ID | Chem station code | NVE station ID(s) | RESA flow station ID | Availability | Comment |\n",
    "|:---------------:|:-----------------:|:-----------------:|:--------------------:|:------------:|:-----------------------------------------------------------------------------------------------:|\n",
    "| 29612 | BUSEDRA | 12.285 | 57 | Hydra II | 2016 data downloaded 09/08/2017 |\n",
    "| 29613 | TELESKI | 16.153 + 16.133 | 59 | From Trine | Data for 16.153 not available - assume constant at 10 m3/s. Data for 16.133 supplied 28/06/2017 |\n",
    "| 29614 | VAGEOTR | 21.71 or 21.11 | 487 | Hydra II | 2016 data downloaded 19/06/2017 |\n",
    "| 29615 | VESENUM | 15.61 | 58 | Hydra II | 2016 data downloaded 19/06/2017 |\n",
    "| 29617 | ØSTEGLO | 2.605 | 56 | From Trine | 2016 data received 28/06/2017 |\n",
    "| 29778 | STREORK | 121.22 | 348 | Hydra II | 2016 data downloaded 19/06/2017 |\n",
    "| 29779 | FINEALT | 212.11 | 386 | Hydra II | 2016 data downloaded 26/07/2017 |\n",
    "| 29782 | NOREVEF | 151.5 | 351 | Hydra II | 2016 data downloaded 26/07/2017 |\n",
    "| 29783 | ROGEORR | 28.7 | 355 | Hydra II | 2016 data downloaded 19/06/2017 |\n",
    "| 29821 | HOREVOS | 62.5 | 546 | Hydra II | 2016 data downloaded 19/06/2017 |\n",
    "| 36225 | OSLEALN | 6.78 | 626 | Hydra II | 2016 data downloaded 26/07/2017 |\n",
    "\n",
    "### 1.2. Data for 2017\n",
    "\n",
    "For 2017, all datasets were supplied by Trine Fjeldstad at NVE (see e-mails received 15.08.2018 at 10.13 and 15.08.2018 at 11.52). For station_id 29614, Trine has provided data for station 21.71 (rather than 21.11). This is fine - see the second bullet point above.\n",
    "\n",
    "### 1.3. Data for 2018\n",
    "\n",
    "Most datasets were downloaded from Hydra-II on 08.08.2019. Data for three sites has been provided by Trine - see e-mail received 08.08.2019 at 12:14. This year, Trine has provided data for station 21.11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year of interest\n",
    "year = 2018\n",
    "\n",
    "# Folder containing Hydra II data\n",
    "hyd_fold = r'../../../Data/nve_observed/2018-19/hydra_ii'\n",
    "\n",
    "# Folder containing data from Trine\n",
    "tri_fold = r'../../../Data/nve_observed/2018-19/from_trine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dis_station_id</th>\n",
       "      <th>xdate</th>\n",
       "      <th>xvalue</th>\n",
       "      <th>xcomment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>348</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>45.396</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>348</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>46.759</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>348</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>41.772</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>348</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>45.874</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>348</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>42.909</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dis_station_id      xdate  xvalue  xcomment\n",
       "0             348 2018-01-01  45.396       NaN\n",
       "1             348 2018-01-02  46.759       NaN\n",
       "2             348 2018-01-03  41.772       NaN\n",
       "3             348 2018-01-04  45.874       NaN\n",
       "4             348 2018-01-05  42.909       NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dict mapping NVE codes to RESA discharge station IDs\n",
    "stn_id_dict = {'12_285':57,\n",
    "               '16_133':59,   # Need to add 10 m3/s\n",
    "               '21_11':487,   # Could also use 21_11\n",
    "               '15_61':58,\n",
    "               '2_605':56,\n",
    "               '121_22':348,\n",
    "               '212_11':386,\n",
    "               '151_5':351,\n",
    "               '28_7':355,\n",
    "               '62_5':546,\n",
    "               '6_78':626}\n",
    "\n",
    "# List to store output\n",
    "df_list = []\n",
    "\n",
    "# Get list of Hydra II files to process\n",
    "search_path = os.path.join(hyd_fold, '*.csv')\n",
    "file_list = glob.glob(search_path)\n",
    "\n",
    "# Loop over Hydra II data\n",
    "for file_path in file_list:\n",
    "    # Get RESA station ID\n",
    "    name = os.path.split(file_path)[1][:-4]\n",
    "    stn_id = stn_id_dict[name]\n",
    "    \n",
    "    # Parse file\n",
    "    df = pd.read_csv(file_path, skiprows=2, index_col=0,\n",
    "                     parse_dates=True, header=None, \n",
    "                     names=['xdate', 'xvalue'], \n",
    "                     na_values='-9999')\n",
    "    \n",
    "    # Get just records for year of interest\n",
    "    df = df.truncate(before='%s-01-01' % year,\n",
    "                     after='%s-01-01' % (year+1))\n",
    "\n",
    "    # Linear interpolation and back-filling of NaN\n",
    "    df['xvalue'].interpolate(method='linear', inplace=True)\n",
    "    df['xvalue'].fillna(method='backfill', inplace=True)\n",
    "    \n",
    "    # Remove HH:MM:SS part from dates\n",
    "    df = df.resample('D').mean()\n",
    "\n",
    "    # Add 10 m3/s to 16.133 (RESA2 ID 59)\n",
    "    if stn_id == 59:\n",
    "        df['xvalue'] = df['xvalue'] + 10.\n",
    "        \n",
    "    # Add other required cols and tidy\n",
    "    df['dis_station_id'] = stn_id\n",
    "    df['xcomment'] = np.nan\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    # Reorder cols\n",
    "    df = df[['dis_station_id', 'xdate', 'xvalue', 'xcomment']]\n",
    "    \n",
    "    # Append to output\n",
    "    df_list.append(df)\n",
    "\n",
    "# Get list of files from Trine to process\n",
    "search_path = os.path.join(tri_fold, '*.csv')\n",
    "file_list = glob.glob(search_path)\n",
    "\n",
    "# Loop over files from Trine\n",
    "for file_path in file_list:\n",
    "    # Get RESA station ID\n",
    "    name = os.path.split(file_path)[1][:-4]\n",
    "    stn_id = stn_id_dict[name]\n",
    "    \n",
    "    # Parse file\n",
    "    df = pd.read_csv(file_path, skiprows=1, index_col=0,\n",
    "                     parse_dates=True, header=None, \n",
    "                     sep=';', names=['xdate', 'xvalue'],\n",
    "                     na_values='-9999')\n",
    "    \n",
    "    # Get just records for year of interest\n",
    "    df = df.truncate(before='%s-01-01' % year,\n",
    "                     after='%s-01-01' % (year+1))\n",
    "    \n",
    "    # Linear interpolation and back-filling of NaN\n",
    "    df['xvalue'].interpolate(method='linear', inplace=True)\n",
    "    df['xvalue'].fillna(method='backfill', inplace=True)\n",
    "\n",
    "    # Remove HH:MM:SS part from dates\n",
    "    df = df.resample('D').mean()\n",
    "\n",
    "    # Add 10 m3/s to 16.133 (RESA2 ID 59)\n",
    "    if stn_id == 59:\n",
    "        df['xvalue'] = df['xvalue'] + 10.\n",
    "        \n",
    "    # Add dis_id and tidy\n",
    "    df['dis_station_id'] = stn_id\n",
    "    df['xcomment'] = np.nan\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    # Reorder cols\n",
    "    df = df[['dis_station_id', 'xdate', 'xvalue', 'xcomment']]\n",
    "    \n",
    "    # Append to output\n",
    "    df_list.append(df)  \n",
    "\n",
    "# Stack data\n",
    "df = pd.concat(df_list, axis=0)\n",
    "\n",
    "# Check length of df is as expected \n",
    "# Get number of days in year of interest\n",
    "days = len(pd.date_range(start='%s-01-01' % year, \n",
    "                         end='%s-12-31' % year,\n",
    "                         freq='D'))\n",
    "\n",
    "assert len(df) == 11*days, 'Check datasets are complete for all sites.'\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic checking\n",
    "assert df['xvalue'].dtypes == np.float64, 'Check for text in \"xvalue\" column.'\n",
    "assert pd.isnull(df['xvalue']).sum() == 0, 'Check for NaN in \"xvalue\" column.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new rows to database\n",
    "#df.to_sql('discharge_values', con=engine, schema='resa2', \n",
    "#          if_exists='append', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modelled discharge\n",
    "\n",
    "Each year, Stein Beldring supplies modelled data from HBV for the period from 1990 to the year of interest. These datasets are stored locally here:\n",
    "\n",
    "    ...Elveovervakingsprogrammet\\Data\\hbv_modelled\n",
    "\n",
    "and on the network here:\n",
    "\n",
    "K:\\Avdeling\\Vass\\316_Miljøinformatikk\\Prosjekter\\RID\\Vannføring\\Modellert\n",
    "\n",
    "The flow files are named e.g. `hbv_00000001.var`, where the number corresponds to the NVE \"vassdragsområde\". These are listed in *vassomr.pdf* in the above folder, and they're also included in RESA2's `DISCHARGE_STATIONS` table. The vassdragsområde numbers are stored in the `NVE_SERINUMMER` field.\n",
    "\n",
    "Tore has an Access database in e.g.\n",
    "\n",
    "K:\\Avdeling\\Vass\\316_Miljøinformatikk\\Prosjekter\\RID\\Vannføring\\Modellert\\NVE_MODELLERT_2016\\vannføring\n",
    "\n",
    "that first deletes the modelled NVE values for each station from 1990 onwards and then adds the new data, which includes everything from 1990 plus the additional year of data. The code below does the same, and performs some basic checking of the data at the same time.\n",
    "\n",
    "### Data delivery\n",
    "\n",
    " * **2016:** See e-mail from Stein received 13.06.2017 at 12.17\n",
    " * **2017:** See e-mail from Stein received 14.08.2018 at 08.51\n",
    " * **2018:** See e-mail from Stein received 31.05.2019 at 09.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year of interest\n",
    "year = 2018\n",
    "\n",
    "# Folder containing modelled data\n",
    "data_fold = r'../../../Data/hbv_modelled/RID_2018'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of files to process (only interested in flow here)\n",
    "search_path = os.path.join(data_fold, 'hbv_*.var')\n",
    "file_list = glob.glob(search_path)\n",
    "\n",
    "# Get number of days between 1990 and year of interest\n",
    "days_new = len(pd.date_range(start='1990-01-01', \n",
    "                             end='%s-12-31' % year,\n",
    "                             freq='D'))\n",
    "\n",
    "# Get number of days between 1990 and year before\n",
    "days_old = len(pd.date_range(start='1990-01-01', \n",
    "                             end='%s-12-31' % (year-1),\n",
    "                             freq='D'))\n",
    "\n",
    "# Loop over files\n",
    "for file_path in file_list:\n",
    "    # Get name and reg. nr.\n",
    "    name = os.path.split(file_path)[1]\n",
    "    reg_nr = int(name.split('_')[1][:-4])\n",
    "    \n",
    "    # Get RESA2 station ID\n",
    "    sql = (\"SELECT dis_station_id FROM resa2.discharge_stations \"\n",
    "           \"WHERE nve_serienummer = '%s'\" % reg_nr)\n",
    "    dis_id = pd.read_sql_query(sql, engine).iloc[0,0]\n",
    "\n",
    "    # Check number of post-1990 records already in db\n",
    "    # (should equal days_old)\n",
    "    sql = (\"SELECT COUNT(*) FROM resa2.discharge_values \"\n",
    "           \"WHERE dis_station_id = %s \"\n",
    "           \"AND xdate >= DATE '1990-01-01'\" % dis_id)    \n",
    "    cnt_old = pd.read_sql_query(sql, engine).iloc[0,0]    \n",
    "    assert cnt_old == days_old, 'Unexpected number of records already in database.'\n",
    "    \n",
    "    # Read new data\n",
    "    df = pd.read_csv(file_path, delim_whitespace=True, \n",
    "                     header=None, names=['XDATE', 'XVALUE'])\n",
    "    \n",
    "    # Convert dates\n",
    "    df['XDATE'] = pd.to_datetime(df['XDATE'], format='%Y%m%d/1200')\n",
    "\n",
    "    # Check st, end and length\n",
    "    assert df['XDATE'].iloc[0] == pd.Timestamp('1990-01-01'), 'New series does not start on 01/01/1990.'\n",
    "    assert df['XDATE'].iloc[-1] == pd.Timestamp('%s-12-31' % year), 'New series does not end on 31/12/%s.' % year\n",
    "    assert len(df) == days_new, 'Unexpected length for new series.'\n",
    "    \n",
    "    # Add station ID to df\n",
    "    df['DIS_STATION_ID'] = dis_id\n",
    "    \n",
    "    # Drop existing rows post-1990 for this site\n",
    "    sql = (\"DELETE FROM resa2.discharge_values \"\n",
    "           \"WHERE dis_station_id = %s \"\n",
    "           \"AND xdate >= DATE '1990-01-01'\" % dis_id)\n",
    "    res = engine.execute(sql)\n",
    "    \n",
    "    # Add new rows\n",
    "    df.to_sql('discharge_values', con=engine, schema='resa2', \n",
    "              if_exists='append', index=False)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
