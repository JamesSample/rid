{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a8a553-0c0d-4fa4-85d0-bbe9cec0bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import calendar\n",
    "import configparser\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nivapy3 as nivapy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "# Get API key for HydAPI\n",
    "config = configparser.RawConfigParser()\n",
    "config.read(\".nve-hydapi-key\")\n",
    "api_key = config.get(\"Auth\", \"key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cb4b69-cb5f-4989-be6f-28ebe0c9d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to db\n",
    "eng = nivapy.da.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cd0c15-9b36-4049-a528-3afd571cd2a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Update RID flow datasets\n",
    "\n",
    "Each year, updated flow datasets (both modelled and observed) are obtained from NVE and added to RESA2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9890dcba-5d98-4111-b153-97c315529e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year of interest\n",
    "year = 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14637092-2461-402f-bddd-74e8b97e64ad",
   "metadata": {},
   "source": [
    "## 1. Observed discharge\n",
    "\n",
    "Observed time series are used **only** for the 11 main rivers - all other calculations are based on modelled flows (from HBV). This notebook uses NVE's HydAPI to download data for the relevant stations where possible. Other datasets must be obtained directly from NVE (e-mail Trine Fjeldstad). Note that more than 11 discharge stations are involved, because at some chemistry sampling locations the flow is the sum of several NVE discharge series. Note also the following:\n",
    "\n",
    " * Chemistry station 29613 should ideally use the sum of NVE series 16.133 and 16.153, but the latter is no longer available. We simply assume the input from 16.153 is constant at 10 $m^3/s$ (which is roughly equal to the long-term average) <br><br>\n",
    " \n",
    " * The discharge for chemistry station 29614 is **either** NVE station 21.71 **or** 21.11. 21.11 is usually available first, but can check 21.71 too <br><br> \n",
    " \n",
    " * Discharge data for chemistry stations 29617 (NVE ID 2.605) and 36225 (NVE ID 6.78) are often delayed. Need to contact Trine at NVE early to avoid problems later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121b6584-bcdf-40cd-a69a-605745e380e0",
   "metadata": {},
   "source": [
    "### 1.1. Discharge stations\n",
    "\n",
    "The discharge stations associated with the 11 main water chemistry sampling locations are shown in the dataframe below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec357de-b50d-44e5-bb6a-bd99c2a1900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xl_path = r\"../data/rid_resa_nve_discharge_stations.xlsx\"\n",
    "resa_nve_df = pd.read_excel(xl_path, sheet_name=\"observed_stns\")\n",
    "\n",
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(resa_nve_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ab4d16-5032-487e-aedd-f4b515fd6b5c",
   "metadata": {},
   "source": [
    "### 1.2. Data from HydAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43b5fe6-ee15-4b4f-b10d-8a36d40ecf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stations from HydAPI\n",
    "nve_stn_df = nivapy.da.get_nve_hydapi_stations(api_key=api_key)\n",
    "nve_stn_ids = resa_nve_df[\"nve_id\"].values\n",
    "nve_stn_df = nve_stn_df.query(\"station_id in @nve_stn_ids\")\n",
    "print(f\"{len(nve_stn_df)} out of {len(resa_nve_df)} stations found in HydAPI:\")\n",
    "nve_stn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf0daa3-e3f6-4ab4-bb59-28b68ad22f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get discharge\n",
    "par_ids = [1001]\n",
    "st_dt = f\"{year}-01-01\"\n",
    "end_dt = f\"{year + 1}-01-01\"\n",
    "q_df = nivapy.da.query_nve_hydapi(\n",
    "    nve_stn_ids, par_ids, st_dt, end_dt, resolution=1440, api_key=api_key\n",
    ")\n",
    "q_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70714d4d-922b-4fdc-940e-8088ead30e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of records as expected\n",
    "days = 366 if calendar.isleap(year) else 365\n",
    "assert len(q_df) == len(nve_stn_df) * days, \"Number of records is not as expected.\"\n",
    "\n",
    "# Check quality control level\n",
    "print(\n",
    "    \"The following series have not completed quality control and will be dropped (i.e. 'quality' < 3;\"\n",
    ")\n",
    "print(\"see https://hydapi.nve.no/UserDocumentation/ for details):\\n\")\n",
    "print(q_df.query(\"quality != 3\")[[\"station_id\", \"station_name\"]].drop_duplicates())\n",
    "\n",
    "# Drop quality < 3\n",
    "q_df = q_df.query(\"quality == 3\")\n",
    "\n",
    "# Check for NaN\n",
    "if pd.isna(q_df[\"value\"]).sum() > 0:\n",
    "    print(\"\\n\\nThe following records contain NaN values:\\n\")\n",
    "    print(\n",
    "        q_df[pd.isna(q_df[\"value\"])][[\"station_id\", \"station_name\"]].drop_duplicates()\n",
    "    )\n",
    "\n",
    "# Check for negative\n",
    "if (q_df[\"value\"] < 0).sum() > 0:\n",
    "    print(\"\\n\\nThe following records contain values <0:\\n\")\n",
    "    print(q_df[q_df[\"value\"] < 0][[\"station_id\", \"station_name\"]].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bb0be4-3788-4f06-bf38-f5f052c1ec4b",
   "metadata": {},
   "source": [
    "Based on the output above we can **make a data request to NVE**. This typically involves requesting the records for stations **21.11.0** and **12.285.0** (which are not available via HydAPI), **plus** any sites that haven't yet finished quality control (listed above).\n",
    "\n",
    "The cell below uploads the valid HydAPI data to RESA2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8446e995-36de-4a89-8094-a3d31cffa491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store output\n",
    "df_list = []\n",
    "\n",
    "# Loop over Hydra II data\n",
    "for idx, row in resa_nve_df.iterrows():\n",
    "    nve_id = row[\"nve_id\"]\n",
    "    dis_stn_id = row[\"dis_station_id\"]\n",
    "\n",
    "    # Get flow for station\n",
    "    q_stn_df = q_df.query(\"station_id == @nve_id\").copy()\n",
    "\n",
    "    if len(q_stn_df) == 0:\n",
    "        print(f\"No data for NVE ID {nve_id}.\")\n",
    "\n",
    "    else:\n",
    "        assert len(q_stn_df) == days\n",
    "\n",
    "        # Remove HH:MM:SS part from dates\n",
    "        q_stn_df.set_index(\"datetime\", inplace=True)\n",
    "        q_stn_df = q_stn_df.resample(\"D\").mean()\n",
    "        q_stn_df.reset_index(inplace=True)\n",
    "        q_stn_df[\"datetime\"] = q_stn_df[\"datetime\"].dt.date\n",
    "\n",
    "        # Linear interpolation and back-filling of NaN\n",
    "        q_stn_df[\"value\"].interpolate(method=\"linear\", inplace=True)\n",
    "        q_stn_df[\"value\"].fillna(method=\"backfill\", inplace=True)\n",
    "\n",
    "        # Add 10 m3/s to 16.133 (RESA2 discharge station ID 59)\n",
    "        if dis_stn_id == 59:\n",
    "            q_stn_df[\"value\"] = q_stn_df[\"value\"] + 10\n",
    "\n",
    "        # Add other required cols and tidy\n",
    "        q_stn_df[\"dis_station_id\"] = dis_stn_id\n",
    "        q_stn_df[\"xcomment\"] = np.nan\n",
    "        q_stn_df[\"xvalue\"] = q_stn_df[\"value\"]\n",
    "        q_stn_df[\"xdate\"] = q_stn_df[\"datetime\"]\n",
    "\n",
    "        # Reorder cols\n",
    "        q_stn_df = q_stn_df[[\"dis_station_id\", \"xdate\", \"xvalue\", \"xcomment\"]]\n",
    "\n",
    "        # Append to output\n",
    "        df_list.append(q_stn_df)\n",
    "\n",
    "        # Check whether data already exist for this year\n",
    "        sql = (\n",
    "            \"SELECT count(*) FROM resa2.discharge_values \"\n",
    "            \"WHERE dis_station_id = %s \"\n",
    "            \"AND EXTRACT(YEAR FROM xdate) = %s \" % (dis_stn_id, year)\n",
    "        )\n",
    "        cnt_df = pd.read_sql(sql, eng)\n",
    "        cnt = cnt_df.iloc[0, 0]\n",
    "        if cnt > 0:\n",
    "            print(\n",
    "                \"%s data already exist for NVE \"\n",
    "                \"station %s (RESA2 ID %s).\" % (cnt, nve_id, dis_stn_id)\n",
    "            )\n",
    "\n",
    "# Stack data\n",
    "hydapi_q_df = pd.concat(df_list, axis=0)\n",
    "hydapi_q_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064bc2ef-9502-4ae9-a032-848855f78f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add new rows to database\n",
    "# hydapi_q_df.to_sql(\n",
    "#     \"discharge_values\", con=eng, schema=\"resa2\", if_exists=\"append\", index=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58122aae-cd89-44cd-9df3-24a2a6aef21c",
   "metadata": {},
   "source": [
    "### 1.3. Data from Trine\n",
    "\n",
    "Once all the missing datasets identified above have been obtained from NVE, the following code can be used to add them to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39e7655-959d-4925-9577-6cd03bf2435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing data from Trine\n",
    "tri_fold = f\"../../../Data/nve_observed/{year}-{year - 1999}/from_trine\"\n",
    "\n",
    "days = 366 if calendar.isleap(year) else 365\n",
    "\n",
    "# List to store output\n",
    "df_list = []\n",
    "\n",
    "# Get list of files from Trine to process\n",
    "search_path = os.path.join(tri_fold, \"*.csv\")\n",
    "file_list = glob.glob(search_path)\n",
    "\n",
    "# Loop over files from Trine\n",
    "for file_path in file_list:\n",
    "    # Get RESA station ID\n",
    "    nve_id = os.path.split(file_path)[1].split(\"_\")[0] + \".0\"\n",
    "    dis_stn_id = resa_nve_df.query(\"nve_id == @nve_id\")[\"dis_station_id\"].iloc[0]\n",
    "\n",
    "    # Parse file\n",
    "    q_stn_df = pd.read_csv(\n",
    "        file_path,\n",
    "        skiprows=1,\n",
    "        index_col=0,\n",
    "        parse_dates=True,\n",
    "        header=None,\n",
    "        sep=\";\",\n",
    "        names=[\"xdate\", \"xvalue\"],\n",
    "        na_values=\"-9999\",\n",
    "        encoding=\"cp1252\",\n",
    "    )\n",
    "\n",
    "    # Get just records for year of interest\n",
    "    q_stn_df = q_stn_df.truncate(\n",
    "        before=\"%s-01-01\" % year, after=\"%s-01-01\" % (year + 1)\n",
    "    )\n",
    "\n",
    "    # Remove HH:MM:SS part from dates\n",
    "    q_stn_df = q_stn_df.resample(\"D\").mean()\n",
    "    q_stn_df.reset_index(inplace=True)\n",
    "    q_stn_df[\"xdate\"] = q_stn_df[\"xdate\"].dt.date\n",
    "\n",
    "    # Linear interpolation and back-filling of NaN\n",
    "    q_stn_df[\"xvalue\"].interpolate(method=\"linear\", inplace=True)\n",
    "    q_stn_df[\"xvalue\"].fillna(method=\"backfill\", inplace=True)\n",
    "\n",
    "    # Add 10 m3/s to 16.133 (RESA2 ID 59)\n",
    "    if dis_stn_id == 59:\n",
    "        q_stn_df[\"xvalue\"] = q_stn_df[\"xvalue\"] + 10.0\n",
    "\n",
    "    # Add dis_id and tidy\n",
    "    q_stn_df[\"dis_station_id\"] = dis_stn_id\n",
    "    q_stn_df[\"xcomment\"] = np.nan\n",
    "\n",
    "    # Reorder cols\n",
    "    q_stn_df = q_stn_df[[\"dis_station_id\", \"xdate\", \"xvalue\", \"xcomment\"]]\n",
    "\n",
    "    # Append to output\n",
    "    df_list.append(q_stn_df)\n",
    "\n",
    "# Stack data\n",
    "tri_q_df = pd.concat(df_list, axis=0)\n",
    "\n",
    "assert (\n",
    "    len(tri_q_df) == len(file_list) * days\n",
    "), \"Datasets has an unexpected number of records.\"\n",
    "assert tri_q_df[\"xvalue\"].dtypes == np.float64, 'Check for text in \"xvalue\" column.'\n",
    "assert pd.isna(tri_q_df[\"xvalue\"]).sum() == 0, 'Check for NaN in \"xvalue\" column.'\n",
    "\n",
    "tri_q_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69de5717-b5a2-4ce5-9c17-f0d9fc5c92ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add new rows to database\n",
    "# tri_q_df.to_sql(\n",
    "#     \"discharge_values\", con=eng, schema=\"resa2\", if_exists=\"append\", index=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ac0b50-e37c-4a05-b93e-2d24aa255196",
   "metadata": {},
   "source": [
    "## 2. Modelled discharge\n",
    "\n",
    "Each year, Stein Beldring supplies modelled data from HBV for the period from 1990 to the year of interest. These datasets are stored locally here:\n",
    "\n",
    "    ...Elveovervakingsprogrammet\\Data\\hbv_modelled\n",
    "\n",
    "and on the network here:\n",
    "\n",
    "K:\\Avdeling\\Vass\\316_Miljøinformatikk\\Prosjekter\\RID\\Vannføring\\Modellert\n",
    "\n",
    "The flow files are named e.g. `hbv_00000001.var`, where the number corresponds to the NVE \"vassdragsområde\". These are listed in *vassomr.pdf* in the above folder, and they're also included in RESA2's `DISCHARGE_STATIONS` table. The vassdragsområde numbers are stored in the `NVE_SERINUMMER` field.\n",
    "\n",
    "Tore has an Access database in e.g.\n",
    "\n",
    "K:\\Avdeling\\Vass\\316_Miljøinformatikk\\Prosjekter\\RID\\Vannføring\\Modellert\\NVE_MODELLERT_2016\\vannføring\n",
    "\n",
    "that first deletes the modelled NVE values for each station from 1990 onwards and then adds the new data, which includes everything from 1990 plus the additional year of data. The code below does the same, and performs some basic checking of the data at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aab33e-b2ec-4cc9-9e04-44261900c156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing modelled data\n",
    "data_fold = f\"../../../Data/hbv_modelled/RID_{year}\"\n",
    "\n",
    "# Get a list of files to process (only interested in flow here)\n",
    "search_path = os.path.join(data_fold, \"hbv_*.var\")\n",
    "file_list = glob.glob(search_path)\n",
    "\n",
    "# Get number of days between 1990 and year of interest\n",
    "days_new = len(pd.date_range(start=\"1990-01-01\", end=\"%s-12-31\" % year, freq=\"D\"))\n",
    "\n",
    "# Get number of days between 1990 and year before\n",
    "days_old = len(pd.date_range(start=\"1990-01-01\", end=\"%s-12-31\" % (year - 1), freq=\"D\"))\n",
    "\n",
    "# Loop over files\n",
    "for file_path in file_list:\n",
    "    # Get name and reg. nr.\n",
    "    name = os.path.split(file_path)[1]\n",
    "    reg_nr = int(name.split(\"_\")[1][:-4])\n",
    "    print(f\"Processing {name}.\")\n",
    "\n",
    "    # Get RESA2 station ID\n",
    "    sql = (\n",
    "        \"SELECT dis_station_id FROM resa2.discharge_stations \"\n",
    "        \"WHERE nve_serienummer = '%s'\" % reg_nr\n",
    "    )\n",
    "    dis_id = pd.read_sql_query(sql, eng).iloc[0, 0]\n",
    "\n",
    "    # Check number of post-1990 records already in db\n",
    "    # (should equal days_old)\n",
    "    sql = (\n",
    "        \"SELECT COUNT(*) FROM resa2.discharge_values \"\n",
    "        \"WHERE dis_station_id = %s \"\n",
    "        \"AND xdate >= DATE '1990-01-01'\" % dis_id\n",
    "    )\n",
    "    cnt_old = pd.read_sql_query(sql, eng).iloc[0, 0]\n",
    "    assert cnt_old == days_old, \"Unexpected number of records already in database.\"\n",
    "\n",
    "    # Read new data\n",
    "    df = pd.read_csv(\n",
    "        file_path, delim_whitespace=True, header=None, names=[\"XDATE\", \"XVALUE\"]\n",
    "    )\n",
    "\n",
    "    # Convert dates\n",
    "    df[\"XDATE\"] = pd.to_datetime(df[\"XDATE\"], format=\"%Y%m%d/1200\")\n",
    "\n",
    "    # Check st, end and length\n",
    "    assert df[\"XDATE\"].iloc[0] == pd.Timestamp(\n",
    "        \"1990-01-01\"\n",
    "    ), \"New series does not start on 01/01/1990.\"\n",
    "    assert df[\"XDATE\"].iloc[-1] == pd.Timestamp(\"%s-12-31\" % year), (\n",
    "        \"New series does not end on 31/12/%s.\" % year\n",
    "    )\n",
    "    assert len(df) == days_new, \"Unexpected length for new series.\"\n",
    "\n",
    "    # Add station ID to df\n",
    "    df[\"DIS_STATION_ID\"] = dis_id\n",
    "\n",
    "#     # Drop existing rows post-1990 for this site\n",
    "#     sql = (\n",
    "#         \"DELETE FROM resa2.discharge_values \"\n",
    "#         \"WHERE dis_station_id = %s \"\n",
    "#         \"AND xdate >= DATE '1990-01-01'\" % dis_id\n",
    "#     )\n",
    "#     res = eng.execute(sql)\n",
    "\n",
    "#     # Add new rows\n",
    "#     df.to_sql(\n",
    "#         \"discharge_values\", con=eng, schema=\"resa2\", if_exists=\"append\", index=False\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
