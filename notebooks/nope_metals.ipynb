{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import getpass\n",
    "import folium\n",
    "import nivapy\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOPE metals\n",
    "\n",
    "We need to develop an extension module for NOPE to simulate metals. It's not clear yet exactly how this should be done, but the first step is to explore some of the datasets sent by Øyvind following our meeting in Grimstad in September.\n",
    "\n",
    "## 1. Data exploration\n",
    "\n",
    "### 1.1. Data from Øyvind\n",
    "\n",
    "#### 1.1.1. Spatial data\n",
    "\n",
    "One of the raw datasets is a little unusual. It was originally sent to Øyvind by Anders Finstad and then forwarded on to me (see e-mail from Øyvind received 28/09/2017 at 15.52). The file is called *ecco_biwa_db_storage.csv* and it's large (nearly 700 MB). Anders hasn't provided much background information, but it looks as though this is a direct dump from a spatial database and the file includes hexadecimal-encoded Well-Known Binary (WKB) spatial data.\n",
    "\n",
    "**Note:** The original file includes duplicated column headings `geom` and `ebint` (it looks as though this is due to a previous spatial join). I have therefore renamed the first occurrences to `geom2` and `ebint2`, respectively, to avoid naming conflicts.\n",
    "\n",
    "The file contains a huge amount of data (nearly 500 columns), so it'll take a bit of figuring out. I also need to parse the hex-encoded WKB into something that I can display on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corine_2000_continuous_urban_fabric_area_km2</th>\n",
       "      <th>corine_2000_discontinuous_urban_fabric_area_km2</th>\n",
       "      <th>corine_2000_industrial_or_commercial_units_area_km2</th>\n",
       "      <th>corine_2000_road_and_rail_networks_and_associated_land_area_km2</th>\n",
       "      <th>corine_2000_port_areas_area_km2</th>\n",
       "      <th>corine_2000_airports_area_km2</th>\n",
       "      <th>corine_2000_mineral_extraction_sites_area_km2</th>\n",
       "      <th>corine_2000_dump_sites_area_km2</th>\n",
       "      <th>corine_2000_construction_sites_area_km2</th>\n",
       "      <th>corine_2000_green_urban_areas_area_km2</th>\n",
       "      <th>...</th>\n",
       "      <th>a_</th>\n",
       "      <th>column_44</th>\n",
       "      <th>column_45</th>\n",
       "      <th>no3_ug_l</th>\n",
       "      <th>no3_p_vektbasert</th>\n",
       "      <th>dist_closest_ebint</th>\n",
       "      <th>no_lakes_in_250m</th>\n",
       "      <th>no_lakes_in_100m</th>\n",
       "      <th>no_lakes_in_10m</th>\n",
       "      <th>dist_2nd_closest_ebint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>82.414563</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>248.020800</td>\n",
       "      <td>0.396833</td>\n",
       "      <td>23.216747</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>48.164530</td>\n",
       "      <td>83.992481</td>\n",
       "      <td>46.80</td>\n",
       "      <td>230.305029</td>\n",
       "      <td>0.397763</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>58.657574</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.005200</td>\n",
       "      <td>0.169877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>85.204968</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>12.92</td>\n",
       "      <td>105.408840</td>\n",
       "      <td>0.215120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>55.653402</td>\n",
       "      <td>154.259399</td>\n",
       "      <td>6.75</td>\n",
       "      <td>44.289429</td>\n",
       "      <td>0.150644</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 495 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   corine_2000_continuous_urban_fabric_area_km2  \\\n",
       "0                                             0   \n",
       "1                                             0   \n",
       "2                                             0   \n",
       "3                                             0   \n",
       "4                                             0   \n",
       "\n",
       "   corine_2000_discontinuous_urban_fabric_area_km2  \\\n",
       "0                                                0   \n",
       "1                                                0   \n",
       "2                                                0   \n",
       "3                                                0   \n",
       "4                                                0   \n",
       "\n",
       "   corine_2000_industrial_or_commercial_units_area_km2  \\\n",
       "0                                                  0     \n",
       "1                                                  0     \n",
       "2                                                  0     \n",
       "3                                                  0     \n",
       "4                                                  0     \n",
       "\n",
       "   corine_2000_road_and_rail_networks_and_associated_land_area_km2  \\\n",
       "0                                                  0                 \n",
       "1                                                  0                 \n",
       "2                                                  0                 \n",
       "3                                                  0                 \n",
       "4                                                  0                 \n",
       "\n",
       "   corine_2000_port_areas_area_km2  corine_2000_airports_area_km2  \\\n",
       "0                                0                              0   \n",
       "1                                0                              0   \n",
       "2                                0                              0   \n",
       "3                                0                              0   \n",
       "4                                0                              0   \n",
       "\n",
       "   corine_2000_mineral_extraction_sites_area_km2  \\\n",
       "0                                              0   \n",
       "1                                              0   \n",
       "2                                              0   \n",
       "3                                              0   \n",
       "4                                              0   \n",
       "\n",
       "   corine_2000_dump_sites_area_km2  corine_2000_construction_sites_area_km2  \\\n",
       "0                                0                                        0   \n",
       "1                                0                                        0   \n",
       "2                                0                                        0   \n",
       "3                                0                                        0   \n",
       "4                                0                                        0   \n",
       "\n",
       "   corine_2000_green_urban_areas_area_km2           ...                   a_  \\\n",
       "0                                       0           ...            82.414563   \n",
       "1                                       0           ...            48.164530   \n",
       "2                                       0           ...            58.657574   \n",
       "3                                       0           ...            85.204968   \n",
       "4                                       0           ...            55.653402   \n",
       "\n",
       "    column_44  column_45    no3_ug_l  no3_p_vektbasert  dist_closest_ebint  \\\n",
       "0   82.000000        NaN  248.020800          0.396833           23.216747   \n",
       "1   83.992481      46.80  230.305029          0.397763            0.000000   \n",
       "2   47.000000        NaN   62.005200          0.169877            0.000000   \n",
       "3   52.000000      12.92  105.408840          0.215120            0.000000   \n",
       "4  154.259399       6.75   44.289429          0.150644            0.000000   \n",
       "\n",
       "   no_lakes_in_250m  no_lakes_in_100m  no_lakes_in_10m  dist_2nd_closest_ebint  \n",
       "0                 1                 1                0                     NaN  \n",
       "1                 1                 1                1                     NaN  \n",
       "2                 1                 1                1                     NaN  \n",
       "3                 1                 1                1                     NaN  \n",
       "4                 1                 1                1                     NaN  \n",
       "\n",
       "[5 rows x 495 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read raw data\n",
    "in_csv = (r'C:\\Data\\James_Work\\Staff\\Oeyvind_K\\Elveovervakingsprogrammet'\n",
    "          r'\\NOPE\\Metals\\Raw_Datasets\\ecco_biwa_db_storage.csv')\n",
    "df = pd.read_csv(in_csv, sep=';')\n",
    "\n",
    "# Tidy geom cols\n",
    "df['geom'] = df['geom2']\n",
    "del df['geom2'], df['ebint'], df['ebint2']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to use [Shapely](https://github.com/Toblerity/Shapely) to parse the WKB data, which converts the dataframe to a geodataframe. My intial idea was to then save this as a shapefile for display in ArcGIS. However, the column names in the file are too long for the shapefile format, and naming conflicts arise when the columns are truncated. An alternative is therefore to save the data in GeoJSON format, which is more flexible. Unfortunately, there's then no way to read this using ArcMap. QGIS can read GeoJSON, but a 700 MB JSON file is unwieldy and difficult to manipulate. The code below is nevertheless useful and worth recording for the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from shapely import wkb\n",
    "#from functools import partial # Makes it possible to use \"map\" with kwargs - see\n",
    "#                              # https://stackoverflow.com/a/13499853/505698\n",
    "#\n",
    "## We need to apply wkb.loads to the 'geom2' col, using the kwarg\n",
    "## hex=True. To do this, use 'partial' to create a new func\n",
    "#map_func = partial(wkb.loads, hex=True)\n",
    "#\n",
    "## Parse geometry data\n",
    "#geometry = df['geom2'].map(map_func)\n",
    "#\n",
    "## Delete plain text geom\n",
    "#df = df.drop('geom2', axis=1)\n",
    "#\n",
    "## Build gdf\n",
    "#crs = {'init': 'epsg:32633'} # Numbers look like UTM. Assume Zone 33N\n",
    "#gdf = gpd.GeoDataFrame(df, crs=crs, geometry=geometry)\n",
    "#\n",
    "## Write to GeoJSON\n",
    "## Works, but some encoding issues with special chars in some columns\n",
    "## Don't think utf-8 is supported\n",
    "#out_path = (r'C:\\Data\\James_Work\\Staff\\Oeyvind_K\\Elveovervakingsprogrammet'\n",
    "#            r'\\NOPE\\Metals\\Raw_Datasets\\ecco_biwa_db_storage.json')\n",
    "#gdf.to_file(out_path, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than working with a huge GeoJSON file, it would be better to put this information back into a spatial database. I've spent a few hours messing around with Spatialite, but the development is patchy and there are issues with getting everything installed correctly. Although it seems a bit over-the-top, it's actually much easier to just install PostGIS.\n",
    "\n",
    "Having [installed PostgreSQL 9.6 and PostGIS 2.4](http://www.bostongis.com/PrinterFriendly.aspx?content_name=postgis_tut01), I've created a new database called `niva_work`, which I'll use for manipulating spatial datasets in the future. For reference, having created this database, spatial extensions are enabled by right-clicking `Extensions` and choosing `PostGIS` from the list of extension names. I have then created a new schema called `nope_metals`, which I'll use for this project.\n",
    "\n",
    "I have also done some further development of NivaPy to make it easier to integrate spatial and non-spatial data processing in different databases - see this [notebook](http://nbviewer.jupyter.org/github/JamesSample/rid/blob/master/notebooks/oracle_postgis_test.ipynb) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username: ········\n",
      "Password: ········\n",
      "Connection successful.\n"
     ]
    }
   ],
   "source": [
    "# Connect to db\n",
    "pg_eng = nivapy.da.connect(src='postgres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write raw text data to new table\n",
    "df.to_sql('ecco_biwa', schema='nope_metals', index=True, \n",
    "          con=pg_eng, if_exists='replace')\n",
    "\n",
    "# Parse geom column from hex-encoded WKB. See:\n",
    "# https://gis.stackexchange.com/a/233215/2131\n",
    "sql = (\"ALTER TABLE niva_work.nope_metals.ecco_biwa \"\n",
    "       \"ALTER COLUMN geom TYPE geometry(MULTIPOLYGON, 25833) \" # UTM Zone 33N ETRS89\n",
    "       \"USING ST_SetSRID(geom, 25833)\")\n",
    "res = pg_eng.execute(sql)\n",
    "\n",
    "# Build spatial index\n",
    "sql = (\"CREATE INDEX nope_metals_ecco_biwa_gix \"\n",
    "       \"ON niva_work.nope_metals.ecco_biwa \"\n",
    "       \"USING GIST (geom)\")\n",
    "res = pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data can now be explored using QGIS (`Layer > Add layers > Add PostGIS layers`) by supplying the following credentials:\n",
    "\n",
    "    Name:     niva_work\n",
    "    Host:     localhost\n",
    "    Database: niva_work\n",
    "    \n",
    "The CSV has data for 4677 catchments across Fennoscandia, of which 990 are located in Norway. As demonstrated in this [notebook](http://nbviewer.jupyter.org/github/JamesSample/rid/blob/master/notebooks/oracle_postgis_test.ipynb), I'm pretty sure this is the \"1000 lakes\" dataset, where each polygon corresponds to a lake catchment area. A large number of parameters have been derived for each location, including air and water chemistry (but not much on metals), CORINE and NDVI statistics over time, population data etc.\n",
    "\n",
    "#### 1.1.2. Other datasets\n",
    "\n",
    "In addition to the spatial dataset described above, Øyvind has also supplied the following (via Tom Andersen):\n",
    "\n",
    " * **N1k_dat_29102014.txt**. Catchment characteristics for the \"1000 lakes\" survey\n",
    " \n",
    " * **ReginCC_data_170708b.txt**. Catchment properties for around 8000 regine catchments\n",
    " \n",
    "Both these datasets are potentially useful, but it is difficult to interpret exactly what they mean without more details regarding column headings etc. **Come back to this later**.\n",
    "\n",
    "### 1.2. External datasets\n",
    "\n",
    "The [Meteorological Synthesizing Centre-East (MSC-E)](http://www.msceast.org/index.php/pollution-assessment/emep-domain-menu/data-hm-pop-menu) is part of EMEP, and their website includes simulated deposition of some metals (Cd, Hg and Pb) for Norway in recent years (2014 and 2015). The data are provided on the 50 km EMEP grid and are saved locally here:\n",
    "\n",
    "C:\\Data\\James_Work\\Staff\\Oeyvind_K\\Elveovervakingsprogrammet\\NOPE\\Metals\\Raw_Datasets\\EMEP\n",
    "\n",
    "\n",
    " \n",
    "## 2. \"1000 lakes\" dataset\n",
    "\n",
    "Before exploring the datastes above in too much detail, I'd like to investigate the data we already have in RESA2. In 1995, NIVA undertook a national scale survey of 1000 lakes, which included testing for metals\n",
    "\n",
    "This seems like a good starting point. As an initial data exploration, I'll attempt the following workflow:\n",
    "\n",
    "  1. Extract metal concentrations (Ag, As, Pb, Cd, Cu, Cr, Ni, Hg and Zn) from RESA2 for all the lakes in the 1995 \"1000 lakes\" survey <br><br>\n",
    "  \n",
    "  2. Link this data to e.g. atmosphereic deposition, land use, populations etc. in the spatial data from Anders  <br><br>\n",
    "  \n",
    "  3. See if any relationships can be identified from which national scale metals loads can be estimated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. \"1000 lakes\" water chemistry\n",
    "\n",
    "**(This is copied from another notebook as it's useful, but no longer relevant at the original location).**\n",
    "\n",
    "The first step is to extract water chemistry data for the 1995 \"1000 lakes\" survey. Note that Hg wasn't measured during this survey, but most other metals of interest were. I also want to extract data for pH and TOC if this is available, as these factors affect the mobility of metals from the soil to the water."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Search projects\n",
    "prj_df = nivapy.da.search_projects(\"project_name LIKE 'Regional%'\", ora_eng)\n",
    "prj_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get stations for 1995\n",
    "stn_df = nivapy.da.select_project_stations([27,], ora_eng)\n",
    "\n",
    "print 'Number of stations:', len(stn_df)\n",
    "stn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pars of interest\n",
    "par_list = ['Ag', 'As', 'Pb', 'Cd', 'Cu', 'Zn', 'Ni', 'Cr', 'pH', 'TOC']\n",
    "\n",
    "# Container for results\n",
    "df_list = []\n",
    "\n",
    "# Loop over stations\n",
    "for stn_id in stn_df['station_id']:\n",
    "    # Get all Al data\n",
    "    wc_df, dup_df = nivapy.da.extract_water_chem(stn_id, par_list,\n",
    "                                                 '1995-01-01', \n",
    "                                                 '1995-12-31',\n",
    "                                                 ora_eng,\n",
    "                                                 plot=False)\n",
    "    \n",
    "    # Tidy\n",
    "    wc_df['station_id'] = stn_id\n",
    "    wc_df.reset_index(inplace=True)\n",
    "    df_list.append(wc_df)  \n",
    "\n",
    "# Combine output\n",
    "wc_df = pd.concat(df_list, axis=0)\n",
    "\n",
    "#  Annual means for each site\n",
    "wc_df['year'] = wc_df['sample_date'].dt.year\n",
    "an_df = wc_df.groupby(['year', 'station_id']).mean()\n",
    "\n",
    "# Reset index and join to stns\n",
    "an_df.columns = [i.split('_')[0] for i in an_df.columns]\n",
    "an_df.reset_index(inplace=True)\n",
    "del an_df['year']\n",
    "\n",
    "# Join and drop NaN\n",
    "wc_df = pd.merge(stn_df[['station_id', 'latitude', 'longitude']],\n",
    "                 an_df, how='left', on='station_id')\n",
    "wc_df.dropna(subset=['latitude', 'longitude'], inplace=True)\n",
    "\n",
    "# Set negative values to zero\n",
    "wc_df[wc_df < 0] = 0\n",
    "\n",
    "wc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Geology\n",
    "\n",
    "My initial idea of downloading the NGU 1:250k geology dataset has been abandoned, because it is only possible to download small regions at a time from the NGU website. This is frustrating, but as I only need a very simple representation of the geology I'll simply switch to the coarser [Global Lithological Map of the World](https://www.geo.uni-hamburg.de/en/geologie/forschung/geochemie/glim.html) instead.\n",
    "\n",
    "I have clipped this feature class to Fennoscandia using the shapefile here:\n",
    "\n",
    "C:\\Data\\James_Work\\Useful_Stuff\\GIS_Data\\Norway\\Shapefiles\\Fennoscandia.shp\n",
    "\n",
    "### 2.1. Add geology data to PostGIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_wkt_element(geom, srid):\n",
    "    \"\"\" Use Geoalchemy to convert Geopandas obsjects into\n",
    "        WKT elements.\n",
    "    \"\"\"\n",
    "    return WKTElement(geom.wkt, srid=srid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read geol shapefile\n",
    "in_shp = (r'C:\\Data\\James_Work\\Useful_Stuff\\GIS_Data\\Norway'\n",
    "          r'\\Shapefiles\\GLiM_Global_Geol_Fennoscandia.shp')\n",
    "gdf = gpd.read_file(in_shp)\n",
    "\n",
    "# Convert from UTM Z33N WGS84 to UTM Z33N ETRS89\n",
    "gdf = gdf.to_crs(epsg='25833')\n",
    "\n",
    "# Explode multipolys to polys (need to join attribute table back afterwards)\n",
    "exp_gdf = gdf.explode().reset_index()\n",
    "\n",
    "# Get properties as dataframe\n",
    "df = pd.DataFrame(gdf.reset_index()[['index', 'xx']])\n",
    "\n",
    "# Join props to exploded polys\n",
    "gdf = exp_gdf.merge(df, how='left', left_on='level_0', right_on='index')\n",
    "gdf = gdf[[0, 'xx']]\n",
    "gdf.columns = ['geom', 'lith']\n",
    "gdf.set_geometry('geom')\n",
    "\n",
    "# Calc geom col for PostGIS\n",
    "gdf['geom'] = gdf['geom'].apply(create_wkt_element, args=(25833,))\n",
    "\n",
    "gdf.to_sql('fenno_geol', schema='nope_metals', con=pg_eng,\n",
    "           if_exists='replace', index=True, \n",
    "           dtype={'geom':Geometry('POLYGON', srid=25833)})\n",
    "\n",
    "# Build spatial index\n",
    "sql = (\"CREATE INDEX nope_metals_fenno_geol_gix \"\n",
    "       \"ON niva_work.nope_metals.fenno_geol \"\n",
    "       \"USING GIST (geom)\")\n",
    "res = pg_eng.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Intersect geology and catchment data\n",
    "\n",
    "The code below calculates the intersection between the geology dataset and the lake catchments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract catchments \n",
    "sql = (\"SELECT geom2 AS geom \"\n",
    "       \"FROM niva_work.nope_metals.ecco_biwa \"\n",
    "       \"WHERE nation = 'Norway'\")\n",
    "crs = {'init': 'epsg:25833'} # UTM Zone 33N ETRS89\n",
    "cat_gdf = gpd.GeoDataFrame.from_postgis(sql, geom_col='geom', \n",
    "                                        crs=crs, con=pg_eng)\n",
    "\n",
    "# Extract geol\n",
    "sql = (\"SELECT geom, xx \"\n",
    "       \"FROM niva_work.nope_metals.fenno_geol\")\n",
    "crs = {'init': 'epsg:25833'} # UTM Zone 33N ETRS89\n",
    "geo_gdf = gpd.GeoDataFrame.from_postgis(sql, geom_col='geom', \n",
    "                                        crs=crs, con=pg_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Intersection\n",
    "int_gdf = rid.spatial_overlays(geo_gdf, cat_gdf, how='intersection')\n",
    "\n",
    "del int_gdf['geom_1'], int_gdf['geom_2']\n",
    "del int_gdf['idx1'], int_gdf['idx2']\n",
    "\n",
    "# Save\n",
    "out_shp = (r'C:\\Data\\James_Work\\Staff\\Oeyvind_K\\Elveovervakingsprogrammet'\n",
    "           r'\\NOPE\\Metals\\Raw_Datasets\\GIS\\lakes_1k_geol_inter.shp')\n",
    "int_gdf.to_file(out_shp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
