{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import geopandas as gpd\n",
    "import geopandas.tools\n",
    "import matplotlib.pyplot as plt\n",
    "import nivapy3 as nivapy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyproj\n",
    "import seaborn as sn\n",
    "import useful_rid_code as rid\n",
    "from shapely.geometry import Point\n",
    "from sqlalchemy import types\n",
    "\n",
    "sn.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process model input datasets (2020-21)\n",
    "\n",
    "Modelling for the RID programme makes use of the following input datasets:\n",
    "\n",
    " * **Avløp** (sewage and other drainage). These datasets are provided by **Gisle Berge at SSB** and are sub-divided into\n",
    "     * Large treatment works (\"store anlegg\"; ≥ 50 p.e.)\n",
    "     * Small treatment works (\"små anlegg\"; < 50 p.e.)\n",
    "     * Other environmental pollutants (\"miljøgifter\") <br><br>\n",
    "     \n",
    " * **Fiskeoppdret** (Fish farming). Provided by **Knut Johan Johnsen at Fiskeridirektoratet** <br><br>\n",
    " \n",
    " * **Industri** (industrial point sources). Provided by **Preben Danielsen at Miljødirektoratet** <br><br>\n",
    " \n",
    " * **Jordbruk** (land use and management activities). Provided by **Hans Olav Eggestad at NIBIO**\n",
    " \n",
    "In addition, an annual figure for the use of **copper in aquaculture** is provided by **Preben Danielsen at Miljødirektoratet**.\n",
    " \n",
    "The raw datasets must be restructured into a standardised format and added to the RESA2 database. Once in the database, they can be used to generate input files for TEOTIL2.\n",
    "\n",
    "This notebook processes the raw data and adds it to RESA2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to db\n",
    "engine = nivapy.da.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year of interest\n",
    "year = 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Store anlegg, Miljøgifter and Industri\n",
    "\n",
    "These three datasets are all treated similarly, and there is some duplication between the files. \n",
    "\n",
    " * The **store anlegg** dataset is in wide format. Copy and rename the file, then change the worksheet name to `store_anlegg_{year}`. The header must also be tidied (see example datasets from previous years) and blank rows at the end of the worksheet can be deleted <br><br>\n",
    " \n",
    " * The **miljøgifter** dataset is in wide format. Copy and rename the file, then change the worksheet name to `miljogifter_{year}`. Also check that column headings are the same as in previous years <br><br>\n",
    " \n",
    " * The **industri** dataset is in long format and usually contains data for multiple years. Copy and rename the file, then rename the worksheet to `industry_{year}`. Delete rows above the header and check the header is the same as in previous years. Remember to filter the data to **only include the year of interest** (i.e. delete rows for other years)\n",
    "\n",
    "The data in these files must be added to two tables in RESA2:\n",
    "\n",
    " * The site data must be added to `RESA2.RID_PUNKTKILDER`. Most of the sites should already be there, but occasionally new sites are added. Any new stations must be be assigned lat/lon co-ordinates and the appropriate \"regine\" catchment ID (the latter being most important). This usually requires geocoding plus co-ordinate conversions and/or a spatial join to determine catchment IDs. <br><br>\n",
    " \n",
    "    **Note:** As of 2017, many (>120) of the stations already in the database were missing regine IDs and many more (>3000) were missing co-ordinate information. John Rune says we have previously asked Miljødirektoratet about this, but they have not been able to provide the missing data. During data processing for 2019/20, I noticed that some of the missing co-ordinate information *was* provided in more recent data submissions. However, if a site is already in the database (with missing spatial information), it will not be updated even if co-ordinates are provided in later years. In 2020, I created a notebook ([here](https://nbviewer.org/github/JamesSample/rid/blob/master/notebooks/update_renseanlegg_coords.ipynb)) to update co-ordinates where possible based on data submitted since 2016. This has reduced the number of sites without regine IDs to 30 (`SELECT count(*) from resa2.rid_punktkilder WHERE regine IS NULL;`). I have also modified the code in this notebook so that sites are only added to `RESA2.RID_PUNKTKILDER` when co-ordinate information is available and a regine ID has been successfully assigned. This usually means that some sites in the submission for each year must be ignored, but the benefit is that if the same sites are reported with complete information at a later date, they can then be added to the database and processed correctly. This seems preferable to adding incomplete data that cannot be used. <br><br>\n",
    " \n",
    " * The chemistry data for each site must be extracted and converted to \"long\" format, then added to `RESA2.RID_PUNKTKILDER_INPAR_VALUES`. Parameter IDs etc. are taken from `RESA2.RID_PUNKTKILDER_INPAR_DEF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raw (tidied) data\n",
    "\n",
    "# Store anlegg\n",
    "in_xlsx = f\"../../../Data/point_data_{year}/avlop_stor_anlegg_{year}_raw.xlsx\"\n",
    "stan_df = pd.read_excel(in_xlsx, sheet_name=f\"store_anlegg_{year}\")\n",
    "\n",
    "# Miljøgifter\n",
    "in_xlsx = f\"../../../Data/point_data_{year}/avlop_miljogifter_{year}_raw.xlsx\"\n",
    "milo_df = pd.read_excel(in_xlsx, sheet_name=f\"miljogifter_{year}\")\n",
    "\n",
    "# Industri\n",
    "in_xlsx = f\"../../../Data/point_data_{year}/industri_{year}_raw.xlsx\"\n",
    "ind_df = pd.read_excel(in_xlsx, sheet_name=f\"industry_{year}\")\n",
    "\n",
    "# Drop blank rows\n",
    "stan_df.dropna(how=\"all\", inplace=True)\n",
    "milo_df.dropna(how=\"all\", inplace=True)\n",
    "ind_df.dropna(how=\"all\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Basic data checking\n",
    "\n",
    "All of the store anlegg and miljøgifter sites are classified as `RENSEANLEGG` in the `TYPE` column of `RESA2.RID_PUNKTKILDER`; industri sites are labeled `INDUSTRI`.\n",
    "\n",
    "The code below adds `TYPE` columns, merges site data from different sources, converts UTM co-ordinates to WGS84 decimal degrees and identifies sites not already in the database. Issues identified below (e.g. missing co-ordinates) should be corrected if possible before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add TYPE cols\n",
    "stan_df[\"TYPE\"] = \"RENSEANLEGG\"\n",
    "milo_df[\"TYPE\"] = \"RENSEANLEGG\"\n",
    "ind_df[\"TYPE\"] = \"INDUSTRI\"\n",
    "\n",
    "# Get just stn info from each df\n",
    "stan_loc = stan_df[\n",
    "    [\"ANLEGGSNR\", \"ANLEGGSNAVN\", \"Kommunenr\", \"TYPE\", \"Sone\", \"UTM_E\", \"UTM_N\"]\n",
    "].copy()\n",
    "\n",
    "milo_loc = milo_df[\n",
    "    [\"ANLEGGSNR\", \"ANLEGGSNAVN\", \"KOMMUNE_NR\", \"TYPE\", \"SONEBELTE\", \"UTMOST\", \"UTMNORD\"]\n",
    "].copy()\n",
    "\n",
    "ind_loc = ind_df[\n",
    "    [\n",
    "        \"Anleggsnr\",\n",
    "        \"Anleggsnavn\",\n",
    "        \"Komm.nr\",\n",
    "        \"TYPE\",\n",
    "        \"Geografisk Longitude\",\n",
    "        \"Geografisk Latitude\",\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "\n",
    "# Rename cols\n",
    "stan_loc.columns = [\n",
    "    \"anlegg_nr\",\n",
    "    \"anlegg_navn\",\n",
    "    \"komm_no\",\n",
    "    \"TYPE\",\n",
    "    \"zone\",\n",
    "    \"east\",\n",
    "    \"north\",\n",
    "]\n",
    "milo_loc.columns = [\n",
    "    \"anlegg_nr\",\n",
    "    \"anlegg_navn\",\n",
    "    \"komm_no\",\n",
    "    \"TYPE\",\n",
    "    \"zone\",\n",
    "    \"east\",\n",
    "    \"north\",\n",
    "]\n",
    "ind_loc.columns = [\"anlegg_nr\", \"anlegg_navn\", \"komm_no\", \"TYPE\", \"lon\", \"lat\"]\n",
    "\n",
    "# Drop duplicates\n",
    "stan_loc.drop_duplicates(inplace=True)\n",
    "milo_loc.drop_duplicates(inplace=True)\n",
    "ind_loc.drop_duplicates(inplace=True)\n",
    "\n",
    "# Convert UTM Zone col to Pandas' nullable integer data type\n",
    "# (because proj. now complains about float UTM zones)\n",
    "stan_loc[\"zone\"] = stan_loc[\"zone\"].astype(pd.Int64Dtype())\n",
    "milo_loc[\"zone\"] = milo_loc[\"zone\"].astype(pd.Int64Dtype())\n",
    "\n",
    "# Convert UTM to lat/lon\n",
    "# \"Industri\" data is already in dd\n",
    "stan_loc = nivapy.spatial.utm_to_wgs84_dd(stan_loc, \"zone\", \"east\", \"north\")\n",
    "milo_loc = nivapy.spatial.utm_to_wgs84_dd(milo_loc, \"zone\", \"east\", \"north\")\n",
    "\n",
    "# Remove UTM data\n",
    "del stan_loc[\"zone\"], stan_loc[\"east\"], stan_loc[\"north\"]\n",
    "del milo_loc[\"zone\"], milo_loc[\"east\"], milo_loc[\"north\"]\n",
    "\n",
    "# Combine into single df\n",
    "loc_df = pd.concat([stan_loc, milo_loc, ind_loc], axis=0, sort=True)\n",
    "\n",
    "# The same site can be in multiple files, so drop duplicates\n",
    "loc_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Kommune nr. should be a 4 char string, not a float\n",
    "fmt = lambda x: \"%04d\" % x\n",
    "loc_df[\"komm_no\"] = loc_df[\"komm_no\"].apply(fmt)\n",
    "\n",
    "# Check ANLEGG_NR is unique\n",
    "assert loc_df.index.duplicated().all() == False, 'Some \"ANLEGGSNRs\" are duplicated.'\n",
    "\n",
    "# Check if any sites are not already in db\n",
    "sql = \"SELECT UNIQUE(ANLEGG_NR) FROM resa2.rid_punktkilder\"\n",
    "annr_df = pd.read_sql_query(sql, engine)\n",
    "\n",
    "not_in_db = set(loc_df[\"anlegg_nr\"].values) - set(annr_df[\"anlegg_nr\"].values)\n",
    "not_in_db_df = loc_df[loc_df[\"anlegg_nr\"].isin(list(not_in_db))][\n",
    "    [\"anlegg_nr\", \"anlegg_navn\"]\n",
    "].sort_values(\"anlegg_nr\")\n",
    "no_coords_df = loc_df.query(\"(lat!=lat) or (lon!=lon)\")[\n",
    "    [\"anlegg_nr\", \"anlegg_navn\"]\n",
    "].sort_values(\"anlegg_nr\")\n",
    "not_in_db_no_coords_df = not_in_db_df[\n",
    "    not_in_db_df[\"anlegg_nr\"].isin(no_coords_df[\"anlegg_nr\"])\n",
    "].sort_values(\"anlegg_nr\")\n",
    "\n",
    "print(f\"The following {len(not_in_db_df)} locations are not already in the database:\")\n",
    "print(not_in_db_df)\n",
    "\n",
    "print(\n",
    "    f\"\\nThe following {len(no_coords_df)} locations do not have co-ordinates \"\n",
    "    \"in this year's data:\"\n",
    ")\n",
    "print(no_coords_df)\n",
    "\n",
    "print(\n",
    "    f\"\\nThe following {len(not_in_db_no_coords_df)} locations are not in the \"\n",
    "    \"database and do not have co-ordinates (and therefore must be ignored):\"\n",
    ")\n",
    "print(not_in_db_no_coords_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Identify Regine Vassdragsnummer\n",
    "\n",
    "The shapefile here:\n",
    "\n",
    "    K:/Kart/Regine_2006/RegMinsteF.shp\n",
    "\n",
    "shows locations for all the regine catchments used by TEOTIL (see e-mail from John Rune received 29/06/2017 at 17.26). I've copied this file locally here:\n",
    "\n",
    "    ../../../Data/gis/shapefiles/RegMinsteF.shp\n",
    "\n",
    "and re-projected it to WGS84 geographic co-ordinates. The new file is called `reg_minste_f_wgs84.shp`.\n",
    "\n",
    "The code cell below identifies which regine polygon each point is located in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to Regine catchment shapefile\n",
    "reg_shp_path = r\"../../../Data/gis/shapefiles/reg_minste_f_wgs84.shp\"\n",
    "\n",
    "# Spatial join\n",
    "loc_df = nivapy.spatial.identify_point_in_polygon(\n",
    "    loc_df, reg_shp_path, \"anlegg_nr\", \"VASSDRAGNR\", \"lat\", \"lon\"\n",
    ")\n",
    "\n",
    "loc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Restructuring site data\n",
    "\n",
    "Rename columns to match RESA2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename other cols to match RESA2\n",
    "loc_df[\"ANLEGG_NR\"] = loc_df[\"anlegg_nr\"]\n",
    "loc_df[\"ANLEGG_NAVN\"] = loc_df[\"anlegg_navn\"]\n",
    "loc_df[\"KNO\"] = loc_df[\"komm_no\"]\n",
    "loc_df[\"REGINE\"] = loc_df[\"VASSDRAGNR\"]\n",
    "loc_df[\"LON_UTL\"] = loc_df[\"lon\"]\n",
    "loc_df[\"LAT_UTL\"] = loc_df[\"lat\"]\n",
    "\n",
    "del loc_df[\"anlegg_nr\"], loc_df[\"anlegg_navn\"], loc_df[\"komm_no\"]\n",
    "del loc_df[\"VASSDRAGNR\"], loc_df[\"lon\"], loc_df[\"lat\"]\n",
    "\n",
    "# Get details for sites not already in db\n",
    "loc_upld = loc_df[loc_df[\"ANLEGG_NR\"].isin(list(not_in_db))].copy()\n",
    "\n",
    "# Drop rows where 'regine' is NaN (usually because of missing co-ordinates).\n",
    "# In the past, all rows have been added, leading to sites in the database\n",
    "# without co-ordinates. These then do not get updated if co-ordinates are\n",
    "# provided in later years. It is therefore better to only add sites with\n",
    "# co-ordinates, as sites with missing data this year may be completed in\n",
    "# subsequent years\n",
    "loc_upld.dropna(subset=[\"REGINE\"], inplace=True)\n",
    "\n",
    "loc_upld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to RESA2.RID_PUNKTKILDER\n",
    "# loc_upld.to_sql(\n",
    "#     \"rid_punktkilder\", con=engine, schema=\"resa2\", if_exists=\"append\", index=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Restructuring values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Anlegg\n",
    "# Get cols of interest\n",
    "stan_vals = stan_df[[\"ANLEGGSNR\", \"MENGDE_P_UT_kg\", \"MENGDE_N_UT_kg\"]]\n",
    "\n",
    "# In RESA2.RID_PUNKTKILDER_INPAR_DEF, N is par_id 44 and P par_id 45\n",
    "stan_vals.columns = [\"ANLEGG_NR\", 45, 44]\n",
    "\n",
    "# Melt to \"long\" format\n",
    "stan_vals = pd.melt(\n",
    "    stan_vals,\n",
    "    id_vars=\"ANLEGG_NR\",\n",
    "    value_vars=[45, 44],\n",
    "    var_name=\"INP_PAR_ID\",\n",
    "    value_name=\"VALUE\",\n",
    ")\n",
    "\n",
    "# Drop NaN values\n",
    "stan_vals.dropna(how=\"any\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As far as I can tell from exploring the 2015 data in the database, the main columns of interest for Miljøgifter are given in `milo_dict`, below, together with the corresponding parameter IDs from `RESA2.RID_PUNKTKILDER_INPAR_DEF`. This hard-coding is a bit messy, but I can't see any database table providing a nice lookup between these values, so they're included here for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miljøgifter\n",
    "# Get cols of interest\n",
    "milo_dict = {\n",
    "    \"MILJOGIFTHG2\": 16,\n",
    "    \"MILJOGIFTPAH2\": 48,\n",
    "    \"MILJOGIFTPCB2\": 30,\n",
    "    \"MILJOGIFTCD2\": 8,\n",
    "    \"MILJOGIFTDEHP2\": 119,\n",
    "    \"MILJOGIFTAS2\": 2,\n",
    "    \"MILJOGIFTCR2\": 10,\n",
    "    \"MILJOGIFTPB2\": 28,\n",
    "    \"MILJOGIFTNI2\": 25,\n",
    "    \"MILJOGIFTCU2\": 15,\n",
    "    \"MILJOGIFTZN2\": 38,\n",
    "    \"KONSMENGDTOTP10\": 45,\n",
    "    \"KONSMENGDTOTN10\": 44,\n",
    "    \"KONSMENGDSS10\": 46,\n",
    "    \"ANLEGGSNR\": \"ANLEGG_NR\",\n",
    "}  # Make headings match RESA\n",
    "\n",
    "milo_vals = milo_df[milo_dict.keys()]\n",
    "\n",
    "# Get par IDs from dict\n",
    "milo_vals.columns = [milo_dict[i] for i in milo_vals.columns]\n",
    "\n",
    "# Melt to \"long\" format\n",
    "milo_vals = pd.melt(\n",
    "    milo_vals, id_vars=\"ANLEGG_NR\", var_name=\"INP_PAR_ID\", value_name=\"VALUE\"\n",
    ")\n",
    "\n",
    "# Drop NaN values\n",
    "milo_vals.dropna(how=\"any\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The industry data is already in \"long\" format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Industri\n",
    "# Get cols of interest\n",
    "ind_vals = ind_df[[\"Anleggsnr\", \"Komp.kode\", \"Mengde\", \"Enhet\"]]\n",
    "ind_vals.columns = [\"anlegg_nr\", \"name\", \"value\", \"unit\"]\n",
    "\n",
    "# Get par defs from db\n",
    "# Check if any sites are not already in db\n",
    "sql = \"SELECT * \" \"FROM resa2.rid_punktkilder_inpar_def\"\n",
    "par_df = pd.read_sql_query(sql, engine)\n",
    "del par_df[\"descr\"]\n",
    "\n",
    "# Convert all units to capitals\n",
    "ind_vals[\"unit\"] = ind_vals[\"unit\"].str.capitalize()\n",
    "par_df[\"unit\"] = par_df[\"unit\"].str.capitalize()\n",
    "\n",
    "# Join\n",
    "ind_vals = pd.merge(ind_vals, par_df, how=\"left\", on=[\"name\", \"unit\"])\n",
    "\n",
    "# Some parameters that are not of interest are not matched\n",
    "# Drop these\n",
    "ind_vals.dropna(how=\"any\", inplace=True)\n",
    "\n",
    "# Get just cols of interest\n",
    "ind_vals = ind_vals[[\"anlegg_nr\", \"in_pid\", \"value\"]]\n",
    "\n",
    "# Rename for db\n",
    "ind_vals.columns = [\"ANLEGG_NR\", \"INP_PAR_ID\", \"VALUE\"]\n",
    "\n",
    "# Convert col types\n",
    "ind_vals[\"INP_PAR_ID\"] = ind_vals[\"INP_PAR_ID\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine\n",
    "val_df = pd.concat([stan_vals, milo_vals, ind_vals], axis=0, sort=True)\n",
    "\n",
    "# Add column for year\n",
    "val_df[\"YEAR\"] = year\n",
    "\n",
    "# Explicitly set data types\n",
    "val_df[\"ANLEGG_NR\"] = val_df[\"ANLEGG_NR\"].astype(str)\n",
    "val_df[\"INP_PAR_ID\"] = val_df[\"INP_PAR_ID\"].astype(int)\n",
    "val_df[\"VALUE\"] = val_df[\"VALUE\"].astype(float)\n",
    "val_df[\"YEAR\"] = val_df[\"YEAR\"].astype(int)\n",
    "\n",
    "# Store Anlegg and Miljøgifter contain some duplicated information\n",
    "val_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Average any remaining duplciates (because sometimes the same value is reported with different precision)\n",
    "val_df = val_df.groupby([\"ANLEGG_NR\", \"INP_PAR_ID\", \"YEAR\"]).mean().reset_index()\n",
    "\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop any existing values for this year\n",
    "# sql = f\"DELETE FROM resa2.rid_punktkilder_inpar_values WHERE year = {year}\"\n",
    "# res = engine.execute(sql)\n",
    "\n",
    "# # Add to RESA2.RID_PUNKTKILDER_INPAR_VALUES\n",
    "# val_df.to_sql(\n",
    "#     \"rid_punktkilder_inpar_values\",\n",
    "#     con=engine,\n",
    "#     schema=\"resa2\",\n",
    "#     if_exists=\"append\",\n",
    "#     index=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Små anlegg (small treatment works)\n",
    "\n",
    "Copy and rename the file, and rename the worksheet `sma_anlegg_{year}`. Delete rows above the header and delete unnecessary columns. The only columns required are `KOMMUNENR`, `SUM FOSFOR` and `SUM NITROGEN`, which should be renamed `KOMMUNENR`, `P_kg` and `N_kg`, respectively.\n",
    "\n",
    "This data is added directly to `RESA2.RID_KILDER_SPREDT_VALUES`. \n",
    "\n",
    "**Note:** The kommuner ID numbers in the små anlegg file should be present in \n",
    "\n",
    "    ../../../teotil2/data/core_input_data/regine_{year}.csv\n",
    "    \n",
    "Kommune IDs change from year to year, so they will usually need updating in TEOTIL - see [update_regine_kommune.ipynb](https://nbviewer.org/github/JamesSample/rid/blob/master/notebooks/update_regine_kommune.ipynb) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raw (tidied) data\n",
    "in_xlsx = f\"../../../Data/point_data_{year}/avlop_sma_anlegg_{year}_raw.xlsx\"\n",
    "sman_df = pd.read_excel(in_xlsx, sheet_name=f\"sma_anlegg_{year}\")\n",
    "\n",
    "# Drop blank rows\n",
    "sman_df.dropna(how=\"all\", inplace=True)\n",
    "\n",
    "# Kommune nr. should be a 4 char string, not a float\n",
    "fmt = lambda x: \"%04d\" % x\n",
    "sman_df[\"KOMMUNENR\"] = sman_df[\"KOMMUNENR\"].apply(fmt)\n",
    "\n",
    "# Check if any kommuner are not already in TEOTIL\n",
    "reg_csv = f\"../../../teotil2/data/core_input_data/regine_{year}.csv\"\n",
    "kmnr_df = pd.read_csv(reg_csv, sep=\";\", encoding=\"utf-8\")\n",
    "kmnr_df[\"komnr\"] = kmnr_df[\"komnr\"].apply(fmt)\n",
    "\n",
    "not_in_db = set(sman_df[\"KOMMUNENR\"].values) - set(kmnr_df[\"komnr\"].values)\n",
    "if len(not_in_db) > 0:\n",
    "    print(\n",
    "        f'\\nThe following {len(not_in_db)} kommuner are not in the TEOTIL \"regine\" file. Consider updating?:'\n",
    "    )\n",
    "    print(sman_df[sman_df[\"KOMMUNENR\"].isin(list(not_in_db))])\n",
    "\n",
    "# Get cols of interest for RID_KILDER_SPREDT_VALUES\n",
    "sman_df = sman_df[[\"KOMMUNENR\", \"P_kg\", \"N_kg\"]]\n",
    "\n",
    "# In RESA2.RID_PUNKTKILDER_INPAR_DEF, N is par_id 44 and P par_id 45\n",
    "sman_df.columns = [\"KOMM_NO\", 45, 44]\n",
    "\n",
    "# Melt to \"long\" format\n",
    "sman_df = pd.melt(\n",
    "    sman_df,\n",
    "    id_vars=\"KOMM_NO\",\n",
    "    value_vars=[45, 44],\n",
    "    var_name=\"INP_PAR_ID\",\n",
    "    value_name=\"VALUE\",\n",
    ")\n",
    "\n",
    "# Add column for year\n",
    "sman_df[\"AR\"] = year\n",
    "\n",
    "sman_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to RESA2.RID_KILDER_SPREDT_VALUES\n",
    "# sman_df.to_sql(\n",
    "#     \"rid_kilder_spredt_values\",\n",
    "#     con=engine,\n",
    "#     schema=\"resa2\",\n",
    "#     if_exists=\"append\",\n",
    "#     index=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fish farms\n",
    "\n",
    "The aquaculture data is usually encrypted and must be stored securely. Copy and rename the file, and change the worksheet name to `fiskeoppdrett_{year}`. Check that data have only been provided for **one year** and that the column names match submissions from previous years.\n",
    "\n",
    "These data must be added to two tables in RESA2:\n",
    "\n",
    " * First, the site data must be added to `RESA2.RID_KILDER_AQUAKULTUR`. Most of the sites should already be there, but occasionally new sites are added. Any new stations must be be assigned lat/lon co-ordinates and the appropriate \"regine\" catchment ID. This usually requires geocoding plus co-ordinate conversions and/or a spatial join to determine catchment IDs.\n",
    " \n",
    "    **Note:** The key ID fields in the raw data appear to be `LOKNR` and `LOKNAVN`. <br><br>\n",
    " \n",
    " * Secondly, the chemistry data for each site must be extracted and converted to \"long\" format, then added to `RESA2.RID_KILDER_AQKULT_VALUES`. Parameter IDs etc. are taken from `RESA2.RID_PUNKTKILDER_INPAR_DEF`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Basic data checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raw (tidied) data\n",
    "# Fish farms\n",
    "in_xlsx = f\"../../../Data/point_data_{year}/fiske_oppdret_{year}_raw.xlsx\"\n",
    "fish_df = pd.read_excel(in_xlsx, sheet_name=f\"fiskeoppdrett_{year}\")\n",
    "\n",
    "# Drop no data\n",
    "fish_df.dropna(how=\"all\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any sites are not already in db\n",
    "sql = \"SELECT UNIQUE(NR) FROM resa2.rid_kilder_aquakultur\"\n",
    "aqua_df = pd.read_sql_query(sql, engine)\n",
    "\n",
    "not_in_db = set(fish_df[\"LOKNR\"].values) - set(aqua_df[\"nr\"].values)\n",
    "nidb_df = fish_df[fish_df[\"LOKNR\"].isin(list(not_in_db))][\n",
    "    [\"LOKNR\", \"LOKNAVN\", \"N_DESIMALGRADER_Y\", \"O_DESIMALGRADER_X\"]\n",
    "].drop_duplicates(subset=[\"LOKNR\"])\n",
    "if len(not_in_db) > 0:\n",
    "    print(f\"nThe following {len(not_in_db)} locations are not in the database:\")\n",
    "    print(nidb_df)\n",
    "\n",
    "# Check for missing co-ords\n",
    "no_coords_df = fish_df.query(\n",
    "    \"(N_DESIMALGRADER_Y!=N_DESIMALGRADER_Y) or \"\n",
    "    \"(O_DESIMALGRADER_X!=O_DESIMALGRADER_X)\"\n",
    ")[[\"LOKNR\", \"LOKNAVN\"]].sort_values(\"LOKNR\")\n",
    "if len(no_coords_df) > 0:\n",
    "    print(\n",
    "        f\"\\nThe following {len(no_coords_df)} locations do not have co-ordinates \"\n",
    "        \"in this year's data:\"\n",
    "    )\n",
    "    print(no_coords_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Geocode fish farms and add to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to Regine catchment shapefile\n",
    "reg_shp_path = r\"../../../Data/gis/shapefiles/reg_minste_f_wgs84.shp\"\n",
    "\n",
    "# Spatial join\n",
    "if len(nidb_df) > 0:\n",
    "    loc_df = nivapy.spatial.identify_point_in_polygon(\n",
    "        nidb_df,\n",
    "        reg_shp_path,\n",
    "        \"LOKNR\",\n",
    "        \"VASSDRAGNR\",\n",
    "        \"N_DESIMALGRADER_Y\",\n",
    "        \"O_DESIMALGRADER_X\",\n",
    "    )\n",
    "\n",
    "    # Rename cols\n",
    "    loc_df.columns = [\"NR\", \"NAVN\", \"LENGDE\", \"BREDDE\", \"REGINE\"]\n",
    "\n",
    "    # Drop rows where 'REGINE' is NaN\n",
    "    no_reg = pd.isna(loc_df[\"REGINE\"])\n",
    "    if no_reg.sum() > 0:\n",
    "        no_reg_df = loc_df[no_reg]\n",
    "        print(\n",
    "            f\"The following {len(no_reg_df)} locations cannot be linked to a regine. \"\n",
    "            \"These sites will be ignored.\"\n",
    "        )\n",
    "        print(no_reg_df)\n",
    "\n",
    "        loc_df.dropna(subset=[\"REGINE\"], inplace=True)\n",
    "\n",
    "    print(f\"The following {len(loc_df)} locations will be added to the database.\")\n",
    "    print(loc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to RESA2.RID_KILDER_AQUAKULTUR\n",
    "# loc_df.to_sql(\n",
    "#     \"rid_kilder_aquakultur\", con=engine, schema=\"resa2\", if_exists=\"append\", index=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Estimate nutrient and copper inputs\n",
    "\n",
    "The methodology here is a little unclear. The following is my best guess, based on the files located here:\n",
    "\n",
    "    K:\\Avdeling\\Vass\\316_Miljøinformatikk\\Prosjekter\\RID\\2016\\Rådata\\Fiskeoppdrett\n",
    "\n",
    "Old workflow:\n",
    "\n",
    " 1. Calculate the fish biomass from the raw data. See the equation in the `Biomasse` column of the spreadsheet *JSE_TEOTIL_2015.xlsx* <br><br>\n",
    " \n",
    " 2. Split the data according to salmon (\"laks\"; species ID 71101) and trout (\"øret\"; species ID 71401), then group by location and month, summing biomass and `FORFORBRUK_KILO` columns (see Fiskeoppdrett_biomasse_2016.accdb) <br><br>\n",
    " \n",
    " 3. Calculate production. This involves combining biomass for the current month with that for the previous month. See the calculations in e.g. *N_P_ørret_2015.xlsx*. <br><br>\n",
    " \n",
    " 4. Calculate NTAP and PTAP. **NB:** I don't know what these quantities are, so I'm just duplicating the Excel calculations in the code below. The functions are therefore not very well explained <br><br>\n",
    " \n",
    " 5. Estimate copper usage at each fish farm by scaling the total annual Cu usage in proportion to P production\n",
    " \n",
    "The annual copper figure is provided by Miljødirektoratet and it is assumed that 85% of the total is lost to water. Values for each year are stored in \n",
    "\n",
    "    ../../../Data/annual_copper_useage_aquaculture.xlsx\n",
    "    \n",
    "**This file should be updated with the latest figure before running the code below**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get annual copper usgae\n",
    "cu_xlsx = r\"../../../Data/annual_copper_usage_aquaculture.xlsx\"\n",
    "cu_df = pd.read_excel(cu_xlsx, sheet_name=\"Sheet1\", index_col=0)\n",
    "tot_an_cu = cu_df.loc[year, \"tot_cu_tonnes\"]\n",
    "an_cu = 0.85 * tot_an_cu\n",
    "print(f\"The total annual copper lost to water from aquaculture is {an_cu:.1f} tonnes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate nutrient inputs from fish farns\n",
    "fish_nut = rid.estimate_fish_farm_nutrient_inputs(fish_df, year, an_cu)\n",
    "fish_nut.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to RESA2.RID_KILDER_AQKULT_VALUES\n",
    "# fish_nut.to_sql(\n",
    "#     \"rid_kilder_aqkult_values\",\n",
    "#     con=engine,\n",
    "#     schema=\"resa2\",\n",
    "#     if_exists=\"append\",\n",
    "#     index=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Land use\n",
    "\n",
    "The land use dataset is provided by NIBIO. The file usually gives errors when it is opened, but it can be saved again as a `.xlsx` without problems. \n",
    "\n",
    "Open the file and `Save as`, then rename the worksheet to `jordbruk_{year}`. Tidy the header to match previous submissions and correct any issues with Norwegian characters in the `omrade` column. \n",
    "\n",
    "The entry for Oslo (`osl1`; fylke_sone = 3_1) is usually missing from the data provided by NIBIO. This row should be added manually to the Excel file and the values should be set identical to those for område `ake2`. This works because the land areas in TEOTIL's `fysone_land_areas.csv` have been made identical for `osl1` and `ake2` (even though this is not correct), so the inputs in terms of kg/km2 are calculated as being the same for both regions, which is what is required.\n",
    " \n",
    "These data are added to the table `RESA2.RID_AGRI_INPUTS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to (tidied) Bioforsk data\n",
    "lu_xlsx = f\"../../../Data/point_data_{year}/jordbruk_{year}.xlsx\"\n",
    "lu_df = pd.read_excel(lu_xlsx)\n",
    "\n",
    "lu_df[\"year\"] = year\n",
    "\n",
    "# Order cols\n",
    "lu_df = lu_df[\n",
    "    [\n",
    "        \"omrade\",\n",
    "        \"year\",\n",
    "        \"n_diff_kg\",\n",
    "        \"n_point_kg\",\n",
    "        \"n_back_kg\",\n",
    "        \"p_diff_kg\",\n",
    "        \"p_point_kg\",\n",
    "        \"p_back_kg\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "lu_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to RESA\n",
    "# lu_df.to_sql(\n",
    "#     name=\"rid_agri_inputs\",\n",
    "#     con=engine,\n",
    "#     schema=\"resa2\",\n",
    "#     index=False,\n",
    "#     if_exists=\"append\",\n",
    "#     dtype={\"omrade\": types.VARCHAR(lu_df[\"omrade\"].str.len().max())},\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
